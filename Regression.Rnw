\documentclass{article}

\usepackage[english]{babel} 
\usepackage[latin1]{inputenc}
\usepackage{fancyhdr}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{float}
\usepackage{csquotes}
\usepackage{ragged2e}
\usepackage[final]{pdfpages}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{draftwatermark}
\graphicspath{ {c:/Users/pc/Dropbox/Congresos/} }

\usepackage[headheight=48pt]{geometry}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,242mm},
 left=20mm,
 right=20mm,
 top=25mm,
 bottom=20mm,}

\SetWatermarkText{For teaching only}
\SetWatermarkScale{4}
\SetWatermarkColor[gray]{0.95}

\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{\markright{\thesubsection\ #1}}
\renewcommand{\subsubsectionmark}[1]{\markright{\thesubsubsection\ #1}}

\fancyhf{}
\lhead{\fancyplain{}{Illustrating regression analysis \newline \rightmark}}
\rhead{\includegraphics[height=1.5cm, keepaspectratio=TRUE]{logo_ub}}
\cfoot{\fancyplain{}{Page \thepage}}


\newcolumntype{N}{@{}m{0pt}@{}}

\begin{document}

\begin{titlepage}
  \centering
  {\huge \textbf {\phantom{Research techniques}} \par}
  \vspace{2cm}
  {\huge \textbf{Regression analysis with R} \par}
  \vspace{2cm}
  {\Large Illustrations for the social sciences \par}
  \vspace{2cm}
  \vfill
{\large \underline{Rumen Manolov, David Leiva, \& Antonio Solanas} \par}
\begin{figure}[H] 
\includegraphics[height=3cm,keepaspectratio=TRUE]{cc}
\centering
\end{figure}

\end{titlepage}

\newpage
\section*{\phantom{LMA}}
\pagenumbering{gobble}
\thispagestyle{empty}
\phantom{Lyubov Moya y Anita}
\vspace{9cm}
%{\tt \centering Sommarbrisen mot kinden\\
%fick alla löven att vinka i vinden...  \\
%Och jag vet det blir mera\\
%en vacker dag blir det s\aa \\
%}

\begin{figure}[H] 
\includegraphics[keepaspectratio=TRUE,scale=0.65]{Michael_msg}
\centering
\end{figure}


\nopage

\newpage
\tableofcontents
\pagenumbering{arabic}

\vspace{2cm}

Dado el carácter no lucrativo y la finalidad exclusivamente docente y eminentemente ilustrativa de los materiales disponibles en este espacio virtual, los profesores se acogen al artículo 32 de la Ley de Propiedad Intelectual (\url{https://www.boe.es/buscar/act.php?id=BOE-A-1996-8930}) vigente respecto al uso parcial de obras ajenas como imágenes, gráficos, textos u otro material utilizado en el presente documento docente. \\[3ex]
Adicionalmente, hay que considerar que el documento sigue una Licencia Creative Commons (BY-NC-ND) que implica que es necesario reconocer la autoría de dicha obra, que no se puede utilizar el material con finalidad comercial y que, en caso de remezclar, transformar o crear una obra a partir del material aquí presente, no puede difundir el material modificado.


\newpage
\section{Aim and characteristics of the document}
\phantom{Some text}\\
\vspace{3cm}
 
The aim of the present document is to illustrate the key issues related to carrying out different types of regression analyses with social sciences data. The emphasis is put on graphical representations as a mean of aiding the interpretation of the numerical results. Moreover, we have tried to avoid mathematical and statistical expressions as much as possible, given that many statistical books already include this information. The examples provided are about with the free software \textbf{R}. For more information about this software, we refer the reader to the website \url{www.r-project.org} and to the materials developed by John Verzani (\url{http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf}) and John Fox (material at (\url{http://socserv.socsci.mcmaster.ca/jfox/Misc/Rcmdr/Getting-Started-with-the-Rcmdr.pdf}). Finally, we also recommend using \textbf{RStudio} (\url{www.rstudio.com}) when working with \textbf{R}. \\[2ex]
 
We offer the \textbf{R} code that can be used to carry out the regression analyses illustrated and to obtain the graphical representations. The aims is that the reader not only knows what analyses can be carried out, but also how s/he can carry them out. For that purpose, we have tried to offer the code using universal notation that is not specific to the current examples, for instance refering to the dependent variable as {\tt Y} and the independent as {\tt X}, to be replaced by the user's data. Moreover, the code includes comments that would be useful for the reader. We aimed to use the flexibility of \textbf{R} as much as needed and for that reason the code is, in some cases, quite complex. For a simpler and much shorter summary of the main already existing \textbf{R} functions for regression analysis, the reader can consult \url{http://www.statmethods.net/stats/regression.html}, as well as the document prepared by Oscar Torres-Reyna at \url{http://www.princeton.edu/~otorres/Regression101R.pdf}.  \\[2ex]
 
Finally, this document is supposed to serve as an additional guide to carrying out regression analysis. However, for learning about and fully understanding regression analysis, it is necessary to attend specific course and to consult the relevant literature, part of which is included in section \ref{sec:read}. We recommend consulting some of the texts suggested for gaining a deeper understanding of what is being done and why.
 
\newpage
\section{Presenting the data}

\subsection{Data for linear regression}
\label{data.linear}

The data used for Linear regression are inspired by the article by Heffernan, O'Neill, and Moss (2012).\\[2ex]

\textbf{Main data}\\[2ex]

\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
PM\_failure&smoking&age&gender&IQ&depression&self\_PM\\ \hline
35&smoker&45&male&95&21&26\\
26&smoker&18&male&105&13&29\\
40&smoker&56&female&112&7&34\\
25&smoker&26&female&100&17&14\\
29&smoker&50&female&99&16&26\\
20&ex-smoker&23&male&101&15&16\\
13&ex-smoker&29&male&120&5&14\\
28&ex-smoker&35&female&96&20&12\\
29&ex-smoker&39&female&100&16&30\\
33&ex-smoker&20&female&90&20&20\\
19&non-smoker&26&male&110&8&21\\
14&non-smoker&29&male&115&6&15\\
10&non-smoker&20&female&103&14&9\\
13&non-smoker&35&female&126&4&10\\
20&non-smoker&41&female&92&20&19\\
\hline
\end{tabular}
\end{table}

\vspace{1cm}

In the following more detailed information is provided about the data. 

\begin{itemize}
\item \textbf{PM\_failure}: Number of items forgotten (e.g., call a friend, buy batteries for the remote controller) from the initial list of 50. This corresponds to the real world task.
\item \textbf{smoking}: Smoking status, distinguishing between people who currently smoke, people who smoked previously but have quit and people who have never smoked.
\item \textbf{age}: Age of the paprticipants, measured in complete years (i.e., number of birthdays)
\item \textbf{gender}: Gender of the participants.
\item \textbf{IQ}: Intelligence quotient of the participants measured using WAIS.
\item \textbf{depression}: Score in the Beck Depression Invetory.
\item \textbf{self\_PM}: Score in the self-report questionnaire about prospective memory failure in which the participats assess their own problems in prospective memory ansering a series of questions.
\end{itemize}

The following code allows inputting the data from a previously generated tab-delimited file (e.g., after saving it as such in \textbf{Excel}\textsuperscript{\textregistered}):

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
Datos <- read.table('Reg.txt',header=TRUE,fill=TRUE)

# This function allows working with the data from the "Datos" database
# without the need to specify "Datos$" before every variable
# and/or without the need to state "data=Datos" in lm() 
attach(Datos)
@

\newpage
\subsection{Data for cross-validation}\\[2ex]
\label{data.cross}

\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
PM\_failure&smoking&age&gender&IQ&depression&self\_PM\\ \hline
33&smoker&59&male&97&20&23\\
25&smoker&38&male&100&10&31\\
38&smoker&61&female&109&11&35\\
22&smoker&36&female&103&19&15\\
28&smoker&53&female&93&16&23\\
18&ex-smoker&43&male&103&13&16\\
12&ex-smoker&49&male&114&4&12\\
28&ex-smoker&45&female&92&21&10\\
31&ex-smoker&55&female&99&13&31\\
30&ex-smoker&33&female&87&18&19\\
18&non-smoker&41&male&110&9&23\\
10&non-smoker&39&male&120&5&13\\
13&non-smoker&31&female&105&15&11\\
19&non-smoker&55&female&123&5&13\\
22&non-smoker&61&female&87&18&17\\
\hline
\end{tabular}
\end{table}

\vspace{1cm}

In the following more detailed information is provided about the data. 

\begin{itemize}
\item \textbf{PM\_failure}: Number of items forgotten (e.g., call a friend, buy batteries for the remote controller) from the initial list of 50. This corresponds to the real world task.
\item \textbf{smoking}: Smoking status, distinguishing between people who currently smoke, people who smoked previously but have quit and people who have never smoked.
\item \textbf{age}: Age of the paprticipants, measured in complete years (i.e., number of birthdays)
\item \textbf{gender}: Gender of the participants.
\item \textbf{IQ}: Intelligence quotient of the participants measured using WAIS.
\item \textbf{depression}: Score in the Beck Depression Invetory.
\item \textbf{self\_PM}: Score in the self-report questionnaire about prospective memory failure in which the participats assess their own problems in prospective memory ansering a series of questions.
\end{itemize}

The following code allows inputting the data from a previously generated tab-delimited file (e.g., after saving it as such in \textbf{Excel}\textsuperscript{\textregistered}):

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
Datos2 <- read.table('Reg_cross.txt',header=TRUE,fill=TRUE)
@

The data from cross validation will be used only in section \ref{sec:cross} and they will not be attached from the beginning.

\newpage
\subsection{Data for piecewise regression}\\[2ex]
\label{data.piecewise}

\begin{table}[H] 
\centering
\begin{tabular}{|llllll|}
\hline
Time&Score&Phase&Time1&Time2&Phase\_time2\\
1&4&0&0&-4&0\\
2&7&0&1&-3&0\\
3&5&0&2&-2&0\\
4&6&0&3&-1&0\\
5&8&1&4&0&0\\
6&10&1&5&1&1\\
7&9&1&6&2&2\\
8&7&1&7&3&3\\
9&9&1&8&4&4\\
\hline
\end{tabular}
\end{table}

In the following more detailed information is provided about the data. 
\begin{itemize}
\item \textbf{Time}: Variable representing the measurement occasion; technically it is \enquote{centered} right before the study begins. Used to model general trend not related to the intervention. 
\item \textbf{Score}: Variable representing the measurement obtained in the target behavior (i.e., the dependent variable of interest).
\item \textbf{Phase}: Dummy variable representing the condition: 0 for baseline; 1 for intervention. Used for modelling change in level in relation to the intervention.
\item \textbf{Time1}: Version of the \emph{Time} variable centered at the first baseline phase measurement occasion.
\item \textbf{Time2}: Version of the \emph{Time} variable centered at the first intervention phase measurement occasion.
\item \textbf{Phase\_time2}: Interaction term corresponding to the multiplication of the \emph{Phase} variable and the \emph{Time2} variable. Used for modelling change in slope in relation to the intervention.  
\end{itemize}

The following code loads the data. 
<<echo=TRUE, eval=TRUE>>=
Piecewise <- read.table('Piecewise.txt', header=TRUE)
@

The data for piecewise regression will be used only in section \ref{sec:piecewise} and they will not be attached from the beginning.

\newpage
\subsection{Data for multilevel modelling}\\[2ex]
\label{data.multilevel}

The data file will be split into two page in order to fit in the current document.\\[2ex]

\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Id&Moment&MoCA&Age\_in&Gender&Prev\_occup&Reside&Activ&Mcenter\\\hline
1&2005&28&70&male&physical&community&4&$-2$\\
1&2007&26&70&male&physical&community&3&$-1$\\
1&2009&26&70&male&physical&community&2&$0$\\
1&2011&24&70&male&physical&community&5&$1$\\
1&2013&20&70&male&physical&community&3&$2$\\
2&2005&17&71&male&intellectual&residence&1&$-2$\\
2&2007&16&71&male&intellectual&residence&1&$-1$\\
2&2009&18&71&male&intellectual&residence&1&$0$\\
2&2011&13&71&male&intellectual&residence&1&$1$\\
2&2013&11&71&male&intellectual&residence&1&$2$\\
3&2005&30&66&male&intellectual&community&3&$-2$\\
3&2007&30&66&male&intellectual&community&4&$-1$\\
3&2009&30&66&male&intellectual&community&5&$0$\\
3&2011&30&66&male&intellectual&community&5&$1$\\
3&2013&29&66&male&intellectual&community&5&$2$\\
4&2005&30&80&female&intellectual&community&7&$-2$\\
4&2007&30&80&female&intellectual&community&6&$-1$\\
4&2009&26&80&female&intellectual&community&7&$0$\\
4&2011&20&80&female&intellectual&community&5&$1$\\
4&2013&12&80&female&intellectual&residence&2&$2$\\
5&2005&24&68&female&intellectual&community&3&$-2$\\
5&2007&19&68&female&physical&community&3&$-1$\\
5&2009&13&68&female&physical&residence&2&$0$\\
5&2011&11&68&female&physical&residence&1&$1$\\
5&2013&7&68&female&physical&residence&1&2\\
6&2005&6&73&female&physical&residence&1&-2\\
6&2007&4&73&female&physical&residence&2&-1\\
6&2009&7&73&female&physical&residence&1&$0$\\
6&2011&3&73&female&physical&residence&2&$1$\\
6&2013&5&73&female&physical&residence&1&$2$\\
7&2005&15&65&male&physical&residence&2&$-2$\\
7&2007&12&65&male&physical&residence&2&$-1$\\
7&2009&12&65&male&physical&residence&3&$0$\\
7&2011&13&65&male&physical&residence&2&$1$\\
7&2013&9&65&male&physical&residence&2&$2$\\
8&2005&26&74&male&physical&community&6&$-2$\\
8&2007&19&74&male&physical&residence&4&$-1$\\
8&2009&18&74&male&physical&residence&4&$0$\\
8&2011&19&74&male&physical&residence&3&$1$\\
8&2013&16&74&male&physical&residence&3&$2$\\
9&2005&30&69&male&intellectual&community&6&$-2$\\
9&2007&30&69&male&intellectual&community&7&$-1$\\
9&2009&28&69&male&intellectual&community&7&$0$\\
9&2011&29&69&male&intellectual&community&6&$1$\\
9&2013&27&69&male&intellectual&community&7&$2$\\
10&2005&29&76&female&intellectual&community&7&$-2$\\
10&2007&24&76&female&intellectual&community&7&$-1$\\
10&2009&26&76&female&intellectual&community&5&$0$\\
10&2011&27&76&female&intellectual&community&4&$1$\\
10&2013&27&76&female&intellectual&community&5&$2$\\
\hline
\end{tabular}
\end{table}

\newpage
\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Id&Moment&MoCA&Age\_in&Gender&Prev\_occup&Reside&Activ&Mcenter\\\hline
11&2005&22&80&female&intellectual&community&6&$-2$\\
11&2007&23&80&female&intellectual&community&7&$-1$\\
11&2009&20&80&female&intellectual&community&6&$0$\\
11&2011&12&80&female&intellectual&residence&4&$1$\\
11&2013&11&80&female&intellectual&residence&3&$2$\\
12&2005&12&75&female&physical&residence&3&$-2$\\
12&2007&10&75&female&physical&residence&4&$-1$\\
12&2009&11&75&female&physical&residence&2&$0$\\
12&2011&9&75&female&physical&residence&3&$1$\\
12&2013&10&75&female&physical&residence&2&$2$\\
13&2005&18&75&male&physical&community&4&$-2$\\
13&2007&19&75&male&physical&community&5&$-1$\\
13&2009&20&75&male&physical&community&4&$0$\\
13&2011&18&75&male&physical&community&5&$1$\\
13&2013&16&75&male&physical&community&5&$2$\\
14&2005&28&68&male&intellectual&community&2&$-2$\\
14&2007&19&68&male&intellectual&community&2&$-1$\\
14&2009&10&68&male&intellectual&residence&3&$0$\\
14&2011&6&68&male&intellectual&residence&1&$1$\\
14&2013&4&68&male&intellectual&residence&1&$2$\\
15&2005&21&71&male&intellectual&residence&4&$-2$\\
15&2007&20&71&male&intellectual&residence&3&$-1$\\
15&2009&19&71&male&intellectual&residence&4&$0$\\
15&2011&17&71&male&intellectual&residence&4&$1$\\
15&2013&18&71&male&intellectual&residence&2&$2$\\
16&2005&23&73&female&intellectual&community&5&$-2$\\
16&2007&25&73&female&intellectual&community&6&$-1$\\
16&2009&24&73&female&intellectual&community&4&$0$\\
16&2011&22&73&female&intellectual&community&3&$1$\\
16&2013&20&73&female&intellectual&community&3&$2$\\
17&2005&26&78&female&intellectual&community&6&$-2$\\
17&2007&27&78&female&intellectual&community&5&$-1$\\
17&2009&25&78&female&intellectual&community&7&$0$\\
17&2011&16&78&female&intellectual&residence&2&$1$\\
17&2013&13&78&female&intellectual&residence&4&$2$\\
18&2005&11&69&female&physical&residence&2&$-2$\\
18&2007&10&69&female&physical&residence&3&$-1$\\
18&2009&9&69&female&physical&residence&2&$0$\\
18&2011&7&69&female&physical&residence&2&$1$\\
18&2013&7&69&female&physical&residence&1&$2$\\
19&2005&17&81&male&physical&residence&2&$-2$\\
19&2007&14&81&male&physical&residence&4&$-1$\\
19&2009&15&81&male&physical&residence&3&$0$\\
19&2011&12&81&male&physical&residence&3&$1$\\
19&2013&13&81&male&physical&residence&1&$2$\\
20&2005&19&80&female&physical&community&6&$-2$\\
20&2007&12&80&female&physical&residence&5&$-1$\\
20&2009&10&80&female&physical&residence&5&$0$\\
20&2011&8&80&female&physical&residence&4&$1$\\
20&2013&9&80&female&physical&residence&4&$2$\\
\hline
\end{tabular}
\end{table}

\newpage
In the following more detailed information is provided about the data. 

\begin{itemize}
\item \textbf{Id}: Identifier for each participant. In order to ensure anonymity, the relation between the identifier and the name is kept in a separate file that is not made public. Nevertheless, it is important to keep the information on the participans together with the signed informed consent, just in case. Note that in this study, the requisite of having at least 5 measurements and at least 50 participants (Mass \& Hox, 2005) available for performing a multilevel analysis is not met, given that the number of Level 2 units is smaller.

\item \textbf{Moment}: Years in which data have been gathered: 2005, 2007, 2009, 2011 and 2013. This variable is only included as additional information, but it is not used in the multilevel analyses - for that purpose the Mcenter variable is used.

\item \textbf{Mcenter}: represents the order of the measurement occasion for each person. Given that it is centered in the thid moment (year 2009), it takes negative values for previous years, 0 for 2009, and positive values for the following years:

\item \textbf{MoCA}: score in the Montreal Cognitive Assessment (Nasreddine et al., 2005) that measures cognitive function and allows detectingmild cogitive impairment. The score can take values ranging from 0 to 30. Higher values indicate better performance.

\item \textbf{Age\_in}: age of the participants when the study started (i.e., in 2005); measured as the number of birthdays.

\item \textbf{Gender}: male or female; it can be converted to a dummy (0-1) variable and it has to be kept in mind which the reference category is (i.e., the one denoted with 0).

\item \textbf{Prev\_occup}: describe el trabajo que ha realizado la persona en la mayor parte de su vida laboral. La realidad se ha simplificado distinguiendo entre trabajos "físicos" e "intelectuales". Al convertirla a variable ficticia Ocu_previa=0 corresponde a trabajos físicos y Ocu_previa=1 a trabajos intelectuales. Se convierte en variable ficticia para poder llevar a cabo parte de los análisis que se presentan aquí. 
\item \textbf{Resides}: variable indicating, for each measurement occasion, whether a person resides in his/her usual home (\enquote{community}) or in a \enquote{residence} for elderly; it can be converted to a dummy (0-1) variable and it has to be kept in mind which the reference category is (i.e., the one denoted with 0).

\item \textbf{Activ}: abbreviation of \enquote{social activity}, referring to the number of events in which the person participates each week (approximately or on average). This variable has not been centered, because a value of zero could be interpreted in meaningful terms (i.e., socially isolated people). 
\end{itemize}

Keep in mind that there is more than one record (i.e., more than one row in the data matrix) for each person (i.e., for each \enquote{Id}). Moreover, the data matrix is in chronological order for each individual. This kind of data matrix is called \enquote{person-period} to distinguish it from data matrices such as the ones presented in the previous pages, which are called \enquote{person-level}. \\[2ex]

\end{itemize}
\item  \textbf{Response variable}: MoCA as a measure of cognitive function. 
\item  \textbf{Level 1 predictor variables}: Mcenter (Moment centered at occasion 3: year 2009), Activ, Resides (dummy variable). These variables can vary in time and, therefore, can take different values at the intra-individual level in the different measurement occasions. 
\item	\textbf{Level 2 predictor variables}: Prev\_occup (dummy variable), Gender, Age at the beginning of the study. These variables are constant in time and, therefore, take only one value per individual (i.e., the same value for the different measurement occasions).
\item	\textbf{Random effect}: In order to assume that a predictor variable has a different effect on the response variable in different people (i.e., level 2 units), it is necessary to have evidence for such an assumption. In the situation described, it is only expected that the Moment predictor has such influence, given that one of the main characteristics of elderly people is their heterogeneity (Phillips, Ajrouch, \& Hillcoat-Nallétamby, 2010).  
\item	\textbf{Fixed effect}: The remaining predictor variables (social activity, place of residence, previous occupation and age at the beginning of the study) are treated as fixed factors in the analyses presented.
\end{itemize}


The following code allows inputting the data from a previously generated tab-delimited file (e.g., after saving it as such in \textbf{Excel}\textsuperscript{\textregistered}):

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
Datos.Multi <- read.table('Multilevel.txt',header=TRUE,fill=TRUE)
@

The data for multilevel analysis will be used only in section \ref{sec:multilevel} and they will not be attached from the beginning.

\newpage
\subsection{Data for logistic regression}\\[2ex]
\label{data.logistic}

\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Id&addiction&h.alcohol&occup.status&income&other.addictions&bar.per.week\\\hline
1&0&no&working&1700&no&1\\
2&0&no&working&1700&no&2\\
3&0&no&working&1700&no&3\\
4&0&no&working&1700&no&2\\
5&0&no&working&1300&no&2\\
6&0&no&working&1300&no&2\\
7&0&no&working&1300&no&3\\
8&0&no&working&1300&no&3\\
9&0&no&working&1100&yes&2\\
10&0&no&working&1100&yes&4\\
11&0&yes&working&900&yes&3\\
12&0&yes&unemployed&900&no&3\\
13&0&yes&unemployed&750&no&5\\
14&1&yes&working&1700&no&2\\
15&1&yes&working&1300&yes&4\\
16&1&yes&working&1100&no&3\\
17&1&yes&working&1100&no&3\\
18&1&yes&working&1100&no&1\\
19&1&yes&working&900&yes&4\\
20&1&yes&working&900&yes&4\\
21&1&yes&working&900&yes&2\\
22&1&yes&unemployed&750&yes&3\\
23&1&yes&unemployed&750&yes&5\\
24&1&yes&unemployed&750&yes&6\\
25&1&yes&unemployed&750&yes&5\\
26&1&no&unemployed&450&no&5\\
27&1&no&unemployed&450&yes&4\\
28&1&no&unemployed&450&yes&6\\
29&1&no&unemployed&450&yes&7\\
30&1&no&unemployed&450&yes&3\\
\hline
\end{tabular}
\end{table}

In the following more detailed information is provided about the data. 
\begin{itemize}
\item \textbf{Id}: Identifier for each participant. In order to ensure anonymity, the relation between the identifier and the name is kept in a separate file that is not made public. Nevertheless, it is important to keep the information on the participans together with the signed informed consent, just in case. 
\item \textbf{addiction}: Main dependent variable indicating whether each person suffers from addiction to alcohol (1) or not (0), that is, a binary variable. Coded as \enquote{working} or \enquote{unemployed}: a binary variable.
\item \textbf{occup.status}: Abbreviation of occupational status; potential predictor according to the study by Stickley et al. (2015).  
\item \textbf{income}: Amount of money, expressed in euros, that each person in the sample earns per month; potential predictor according to the study by Stickley et al. (2015).  
\item \textbf{h.alcohol}: Abbreviation of family history of addiction to alcohol: coded as present (1) or absent(0), that is, a binary variable. Potential predictor of the main dependent variable. 
\item \textbf{bar.per.week}: Number of days per week (i.e., minimum = 0 and maximum = 7) in which each individuals goes to the bar. used as dependent variable in some of the analyses. 
\end{itemize}

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
Datos.Logistic <- read.table('Logistic.txt',header=TRUE,fill=TRUE)
@

The data for logistic regression will be used only in sections \ref{sec:log.simple.cat}, \ref{sec:log.simple.quanti}, \ref{sec:log.multiple.cat}, \ref{sec:log.multiple.catquanti}, \ref{sec:log.confounding}, \ref{sec:log.proportion}, and \ref{sec:Poisson} and they will not be attached from the beginning.

\newpage
\subsection{Data for nonlinear regression}\\[2ex]
\label{data.nonlinear}

The data file will be split into two page in order to fit in the current document.\\[2ex]

\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|}
\hline
population&years\_start&years\\\hline
1&-50&1\\
37&-49&2\\
25&-48&3\\
13&-47&4\\
1&-46&5\\
1&-45&6\\
9&-44&7\\
41&-43&\\
1&-42&9\\
33&-41&10\\
26&-40&11\\
37&-39&12\\
9&-38&13\\
1&-37&14\\
27&-36&15\\
1&-35&16\\
1&-34&17\\
7&-33&18\\
31&-32&19\\
1&-31&20\\
1&-30&21\\
33&-29&22\\
20&-28&23\\
1&-27&24\\
4&-26&25\\
64&-25&26\\
42&-24&27\\
55&-23&28\\
62&-22&29\\
1&-21&30\\
103&-20&31\\
93&-19&32\\
12&-18&33\\
90&-17&34\\
128&-16&35\\
66&-15&36\\
121&-14&37\\
154&-13&38\\
197&-12&39\\
226&-11&40\\
200&-10&41\\
191&-9&42\\
220&-8&43\\
175&-7&44\\
249&-6&45\\
251&-5&46\\
296&-4&47\\
337&-3&48\\
305&-2&49\\
313&-1&50\\
\hline
\end{tabular}
\end{table}

\newpage
\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|}
\hline
population&years\_start&years\\\hline
357&0&51\\
448&1&52\\
399&2&53\\
404&3&54\\
452&4&55\\
484&5&56\\
542&6&57\\
537&7&58\\
526&8&59\\
534&9&60\\
521&10&61\\
497&11&62\\
615&12&63\\
597&13&64\\
571&14&65\\
622&15&66\\
629&16&67\\
680&17&68\\
636&18&69\\
685&19&70\\
700&20&71\\
702&21&72\\
698&22&73\\
773&23&74\\
776&24&75\\
674&25&76\\
735&26&77\\
774&27&78\\
774&28&79\\
767&29&80\\
764&30&81\\
787&31&82\\
857&32&83\\
835&33&84\\
849&34&85\\
876&35&86\\
841&36&87\\
850&37&88\\
917&38&89\\
834&39&90\\
861&40&91\\
862&41&92\\
925&42&93\\
882&43&94\\
843&44&95\\
914&45&96\\
882&46&97\\
906&47&98\\
888&48&99\\
942&49&100\\
\hline
\end{tabular}
\end{table}

\newpage
\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|}
\hline
population&years\_start&years\\\hline
889&50&101\\
942&51&102\\
954&52&103\\
905&53&104\\
939&54&105\\
997&55&106\\
978&56&107\\
945&57&108\\
950&58&109\\
911&59&110\\
910&60&111\\
963&61&112\\
989&62&113\\
985&63&114\\
981&64&115\\
950&65&116\\
999&66&117\\
988&67&118\\
975&68&119\\
946&69&120\\
937&70&121\\
967&71&122\\
1016&72&123\\
1005&73&124\\
896&74&125\\
1037&75&126\\
1037&76&127\\
944&77&128\\
1035&78&129\\
941&79&130\\
1009&80&131\\
935&81&132\\
952&82&133\\
949&83&134\\
946&84&135\\
999&85&136\\
926&86&137\\
1009&87&138\\
1022&88&139\\
1018&89&140\\
1010&90&141\\
950&91&142\\
978&92&143\\
992&93&144\\
970&94&145\\
978&95&146\\
987&96&147\\
977&97&148\\
971&98&149\\
982&99&150\\
989&100&151\\
\hline
\end{tabular}
\end{table}

\newpage
In the following more detailed information is provided about the data. 

\begin{itemize}
\item \textbf{Population}: Number of people living in an island.
\item \textbf{years\_start}: Years since an expedition of foreigners discovered the island. The year 0 represents the year of discovery, whereas negative years represent the time in which only the local people lived on the island 
\item \textbf{years}: Years since the island was first inhabited by local people. Note that the 1 one represents the first year when any person started living on the island - the value of 0 was avoided due to mathematical reasons related to the potential need to transform data, as described in the corresponding section. 
\end{itemize}

\vspace{1cm}

The following code is used to load the data

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
Datos.Nonlinear <- read.table('Gompertz.txt',header=TRUE,fill=TRUE)
@

The data for nonlinear regression will be used only in section \ref{sec:nonlinear} and they will not be attached from the beginning.


\newpage
\section{Correlation, coefficient of determination, and regression coefficients}
\label{sec:correl.reg}
\subsection{A comparison}

In the present section we illustrate the different kinds of information provided by three closely related statistical indices and analyses. The strength of association between the variables is approximately the same, as quantified by the coefficient of determination, but the direction of the relation is opposite: IQ and prospective memory failure are negatively related, whereas age and propsective memory failure are positively related, as shown by the sign of the correlation coefficient and the slope coefficient from the regression analysis. Note that regression analysis quantifies not only the direction and strength of association, but it also models the functional form of the relation, as a straight line with a certain intercept and slope. The intercept represents the value of dependent variable (prospective memory failure) when the predictor (IQ in one of the analyses and age in the other) is equal to 0. The slope represents the change (increase or decrease) of the dependent variable per one point increase in the predictor. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=6,fig.width=6>>=
# Set the space for a space with 4 plots: 2 x 2
par(mfrow=c(2,2))

# Carry out a simple linear regression 
# IQ as predictor and PM_failure as dependent variable
regcor1 <- lm(PM_failure~IQ)
plot(PM_failure~IQ)
abline(regcor1)
arrows(100,(regcor1$coef[1]+regcor1$coef[2]*100),
       110,(regcor1$coef[1]+regcor1$coef[2]*110),
       col="red",code=2,lwd=3)
arrows(min(IQ)-0.5,(regcor1$coef[1]+regcor1$coef[2]*100),
       min(IQ)-0.5,(regcor1$coef[1]+regcor1$coef[2]*110),
       col="red",code=2,lwd=3,length=0.1)
title(main=paste("Slope x10: Change in PM failure for +10 IQ pts"),cex.main=0.8)
plot(IQ,PM_failure,xlim=c(0,max(IQ)),
     ylim=c(min(min(PM_failure),regcor1$coefficients[1]),
            max(max(PM_failure),regcor1$coefficients[1])))
abline(regcor1)
points(0,regcor1$coefficients[1],pch=16,cex=2,col="red")
title("Intercept in red")
paste("Correlation",round(cor(PM_failure, IQ),2))
paste("R-squared",round(summary(regcor1)$r.squared,2))
paste("Regression intercept", round(regcor1$coef[1],2), 
      "and slope" , round(regcor1$coef[2],2))  


regcor2 <- lm(self_PM~age)
plot(self_PM~age)
abline(regcor2)
arrows(30,(regcor2$coef[1]+regcor2$coef[2]*30),
       40,(regcor2$coef[1]+regcor2$coef[2]*40),
       col="red",code=2,lwd=3)
arrows(min(age)-0.5,(regcor2$coef[1]+regcor2$coef[2]*30),
       min(age)-0.5,(regcor2$coef[1]+regcor2$coef[2]*40),
       col="red",code=2,lwd=3,length=0.1)
title(main=paste("Slope x10: Change in self PM for +10 years"),cex.main=0.8)
plot(age,self_PM,xlim=c(0,max(age)),
     ylim=c(min(min(self_PM),regcor1$coefficients[1]),
            max(max(self_PM),regcor1$coefficients[1])))
abline(regcor2)
points(0,regcor2$coefficients[1],pch=16,cex=2,col="red")
title("Intercept in red")
summary(regcor2)$r.squared
paste("Correlation",round(cor(self_PM, age),2))
paste("R-squared",round(summary(regcor2)$r.squared,2))
paste("Regression intercept", round(regcor2$coef[1],2), 
      "and slope" , round(regcor2$coef[2],2)) 
@


\newpage
\subsection{R-squared illustrated}

R-squared is the proportion of variability explained by the predictor(s) in relation to the total amount of variabiliaty. Equivalently, it can be understood as the complementary of the proportion of residual variability to the total variability. Total variability is the sum of squared deviations between each actually observed value and the overall mean. The explained variability is the sum of squared deviations between each predicted (fitted) value and the overall mean. The residual (error, unexplained) variability is the sum of squared deviations between each predicted value and each corresponding the actually obtained value.


<<echo=TRUE, eval=TRUE,warning=FALSE>>=
par(mfrow=c(2,2))

plot(age,self_PM)
title("Total variability in the data")
abline(h=mean(self_PM))
means <- rep(mean(self_PM),length(self_PM))
for (i in 1:length(age))
  arrows(age[i], self_PM[i],age[i],means[i],length=0.1)

plot(age,self_PM)
title("Explained variability")
abline(regcor2)
abline(h=mean(self_PM))
means <- rep(mean(self_PM),length(self_PM))
for (i in 1:length(age))
  arrows(age[i], regcor2$fitted[i],age[i],means[i],length=0.1,col="green")

plot(age,self_PM)
title("Residual variability: Unexplained")
abline(regcor2)
for (i in 1:length(age))
  arrows(age[i], self_PM[i],age[i],regcor2$fitted[i],
         length=0.1,col="red")

plot(age,self_PM)
title("Explaied and residual variability")
abline(h=mean(self_PM))
abline(regcor2)
means <- rep(mean(self_PM),length(self_PM))
for (i in 1:length(age))
{
  arrows(age[i], regcor2$fitted[i],age[i],means[i],
         length=0.1,col="green");
  arrows(age[i], self_PM[i],age[i],regcor2$fitted[i],
         length=0.1,col="red")
}
@

\newpage
\subsection{Regression analysis using standardized variables}

Standardizing entails subtracting the mean from all values and dividing by the standard deviation. When standardized variablas are used in the regression analysis the intercept becomes unnecessary, as both standardized variables have a mean of zero and the slope coefficient is equivalent to the correlation coefficiet (i.e., standardized slope coefficient, sometimes referred to as \enquote{beta}).

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
stdIQ <- (IQ - mean(IQ))/sd(IQ)
stdPM_failure <- (PM_failure-mean(PM_failure))/sd(PM_failure)
stdself_PM <- (self_PM-mean(self_PM))/sd(self_PM)
stdage <- (age-mean(age))/sd(age)

regstd1 <- lm(stdPM_failure~stdIQ)
paste("Regression intercept", round(regstd1$coef[1],2), 
      "and slope" , round(regstd1$coef[2],2)) 
paste("Correlation",round(cor(stdPM_failure, stdIQ),2))

regstd2 <- lm(stdself_PM~stdage)
paste("Regression intercept", round(regstd2$coef[1],2), 
      "and slope" , round(regstd2$coef[2],2)) 
paste("Correlation",round(cor(stdself_PM, stdage),2))

par(mfrow=c(2,2))

plot(IQ,PM_failure)
title("Original variables")
abline(regcor1)

plot(stdIQ,stdPM_failure)
title("Standardized variables")
abline(regstd1)
points(0,regstd1$coefficients[1],pch=16,cex=2,col="red")

plot(age,self_PM)
title("Original variables")
abline(regcor2)

plot(stdage,stdself_PM)
title("Standardized variables")
abline(regstd2)
points(0,regstd2$coefficients[1],pch=16,cex=2,col="red")

@

\newpage
\subsection{Regression analysis using a centered predictor}

Centering entails subtracting the mean from all values of the predictor variable. It changes the interpretation of the intercept. It was previously stated that when IQ is used as a predictor, the intercept is the value of prospective memory failure when IQ is equal to zero. However, given that IQ is not likely to be equal to zero this value is meaningless - it is only used for predicting/modelling the relation between the two variables for the logical range of IQ values. Centering the predictor changes the interpretation of the intercept to refer to the expected value of the dependent variable for individuals with an IQ equal to the average IQ of the sample. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Centring (i.e., subtracting the mean for) the predictors that stepwise chose
c.IQ <- IQ - mean(IQ)

# Regression with centered predictors
regcent <- lm(PM_failure ~ c.IQ, data=Datos)

# The intercept, without centring, represents the predicted value
# for a participant with average IQ and average score in the depression scale
paste("Prospective memory failure value expected for the average IQ in the sample:", 
      round(regcent$coefficients[1],2)) 

# The intercept, without centring, represents the predicted value
# for a participant with IQ = 0 (impossible) 
paste("Prospective memory failure value expected for people with IQ=0:",
      round(regcor1$coefficients[1],2)) 

# The slope coefficient remains unchanged
paste("Decrease in PM failure for each addition IQ point: centered IQ:", 
      round(regcent$coefficients[2],2))
paste("versus not centered IQ:",
      round(regcor1$coefficients[2],2))
@

Centering will be discussed once again in section \ref{sec:multilevel.center} in the context of multilevel models. 

\newpage
\section{Simple linear regression}

\subsection{Using a quantitative predictor}
\label{sec:simple.linear.quanti}

Simple linear regression involves a single numerical dependent variable and a single independent variable, which is also usually numerical. This most common use is illustrate here, showing first the main results that are obtained, dealing with model validation in the second place and, finally, with diagnostics useful for identifying values that have great influence on the results (regression coefficients, R-squared) obtained. 

\subsubsection{Main results}

The main results of a simple regression analysis are: (a) whether the regression model provides better prediction of the dependent variable than its overall mean: information shown in the ANOVA table; (b) how large is the R-squared (proportion of variability explained) and whether this values is statistically significantly different from zero; (c) the point and interval estimates of the regression coefficients: intercept and slope coefficient for the predictor. Note that in simple linear regression the \emph{p} value from the ANOVA table coincides with the \emph{p} value of R-squared and the \emph{p} value of the slope coefficient for the predictor. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
X <- self_PM
Y <- PM_failure

reg.num <- lm(Y ~ X)
summary(reg.num)

# 95% confidence interval about regression line: red
# Prediction of individual values (95% confidence): blue
plot(X,Y,pch=20)
abline(reg.num,col="green",lty=2)
p_conf1 <- predict(reg.num,interval="confidence")
p_pred1 <- predict(reg.num,interval="prediction")
lines(sort(X),p_pred1[,2][order(X)],col="blue",lty=2)
lines(sort(X),p_pred1[,3][order(X)],col="blue",lty=2)
lines(sort(X),p_conf1[,2][order(X)],col="red",lty=2)
lines(sort(X),p_conf1[,3][order(X)],col="red",lty=2)
@

The graphical representation shown above includes both the point estimates (fitted data in green), but also the interval estimates of the dependent variable when predicting the average Y for a group of individuals with the same X value (in red) or for each individual with a given X value (in blue). The widening red prediction intervals illustrate the fact that predictions become unreliable if they refer to X values distant from the mean and even more unreliable for X values out of the range of observed values.

\newpage
\subsubsection{Model validation}
\label{subsec:linearvalid}

In order to have greater confidence that the \emph{p} values of the regression analysis output are valid, it is necessary to verify the assumptions related to the residuals. Note that, in contrast to other parametric tests such as \emoh{t}-test and analysis of variance (ANOVA), in regression analysis the assumptions are checked after the main results are obtained or even interpreted, given that these assumptions refer to the residuals (what is not explained by the model once the analysis has been carried out) and not to the data.

Normality can tested statistically and also assessed visually. We here focus on the Shapiro-Wilk test, as it has been shown to be more powerful than the Kolmogorov-Smirno ad Anderson-Daling tests. A result that is not statistically significant implies not rejecting the ull hypothesis of normality and that the assumption is met. Visual inspection includes the QQ plot, for detecting deviations from normality (i.e., the diagonal line).  

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
#Validating the model: Normality of residuals
shapiro.test(reg.num$residuals)
qqnorm(reg.num$residuals)
qqline(reg.num$residuals)
@

Another visual option is to inspect the histogram with a superimposed normal curve line to check the similarity between the distribution of the residual and a normal model for data with the same mean and standard deviation. 

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
hist(reg.num$residuals,prob=TRUE,xlab="Residuals")
curve(dnorm(x, mean=mean(reg.num$residuals),sd=sd(reg.num$residuals)),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")
@

Homogeneity of the variance of the residual can also be tested statistically. Once again not rejecting the null hypothesis of homoskedasticity would imply meeting the assumption of regression analysis. Note that using the Breusch-Pagan test requires having installed and loaded the {\tt lmtest} package.

<<echo=TRUE, eval=FALSE,warning=FALSE>>=
# Install the lmtest package only once
instal.packages("lmtest")
@

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Load the necessary package everytime the test is needed
require(lmtest)
#Validating the model: Homogeneity of variance of residuals
bptest(reg.num)
@

The visual evaluation of the homogeneity of variance can be done via a scatterplot of the residuals against the fitted values. The desired data pattern is actually having no clear data pattern or a cloud in which the amount of variability is approximately the same for the smaller and the larger fitted values.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=6>>=
#Validating the model: Plot fitted vs. residuals
par(mfrow=c(1,2))
plot(reg.num$fitted, reg.num$residuals,xlab="Fitted",ylab="Residual")
title(main="Pattern in the residuals?")
lines(sort(reg.num$fitted),
      loess(reg.num$residuals~reg.num$fitted, 
            span=0.8, degree=1)$fitted[order(reg.num$fitted)],col="red")
stdres <- (reg.num$residuals-mean(reg.num$residuals))/sd(reg.num$residuals)
plot(reg.num$fitted, stdres,xlab="Fitted",ylab="Standardized residual")
abline(h=c(-1,1),lty="dashed")
abline(h=c(-2,2),lty="dashed",col="orange")
abline(h=c(-3,3),lty="dashed",col="red")
abline(h=0)
@

\newpage
\subsubsection{Diagnostics}

Apart from validating the model, it is also possible to explore whether there are some values of the variables included in the regression model that have influence on the results obtained (i.e., estimates of the regression coefficients and R-squared for the model). First, the studentized eliminated residual can be used to check whether there are outlying values of the dependent variable - values which are not well accounted for by the model. Values that are greater than 2 or smaller than -2 could be considered indicative of outlying Y values, although a statistical test can also be performed, with or without a Bonferroni correction of the \emph{p} value, due to the fact that the same data are used for repetitive testing.\\[2ex]

Second, the leverage or hat values can be used to detect values of the predictor that are far away from the average. Several rules have been proposed for detecting outlying X values: $2\bar{h}$ or $3\bar{h}$, where $\bar{h}=\frac{p}{n}$, with $p$ denoting the number of parameters (predictors plus 1 intercept) and $n$ denoting the number of measurements, usually sample size.  
\\[2ex]

Finally, Cook's distance can be used to identify influential values, which are not only far away from the remaining values in a univariate sense but are also discrepant from the general pattern of the data (i.e., bivariate outliers). Different cut-off points for detecting influential values have been suggested for Cook's D, such as $D > 1$ or, more commonly, $D > \frac{4}{n}$ or $D > \frac{4}{n-p}$. 

<<echo=TRUE,eval=TRUE,warning=FALSE>>=
# Load the car package necessary for performing the outlierTest
# The car package is usually part of R and needs not be installed separately
require(car)

cookds <- cooks.distance(reg.num)
cook <- sqrt(cookds)
hats <- hatvalues(reg.num)
rstud <- rstudent(reg.num)

# Create bubble plot:
# Abscissa: leverage (of X values)
# Ordinate: eliminated studentized residuals (of Y values)
# Bubble size: Cook's distance: influential, if discrepant
plot (hats, rstud, type='n',
      ylab="Studentized eliminated residual", xlab="hat = leverage")
points(hats, rstud,cex=10*cook/max(cook))

abline(h=c(-2, 0, 2), lty=2)
abline(v=2*length(reg.num$coef)/length(Y), lty=2,col="orange")
title(main=paste("D cut-off =",round(4/(length(Y)),3), 
                 ". Red crosses mark values beyond: influential"))

highest <- 0
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  if (cookds[i] > highest) 
   {highest <- cookds[i] 
    value <- i }
}
valuesd <- rep(0,counterd)
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  valuesd[counterd] <- i}
points(hats[valuesd],rstud[valuesd],col="red",pch=3,cex=2)

# Leverage
counterh <- 0
for (i in 1: length(X))
if (hats[i] >= 3*length(reg.num$coef)/length(X))
  counterh <- counterh + 1
valuesh <- rep(0,counterh)
counterh <- 0
for (i in 1: length(X))
if (hats[i] >= 3*length(reg.num$coef)/length(reg.num))
  {counterh <- counterh + 1;
   valuesh[counterh] <- i}

# Outlying Y
highest_r <- outlierTest(reg.num,labels=row.names(Datos))[[1]]
for (i in 1: length(X))
if (rstud[i] == highest_r) value_r <- i

yaxis <- rstud[value_r]
xaxis <- hats[value_r]
arrows(xaxis+0.075,yaxis,xaxis+0.002,yaxis,length = 0.5,col="blue")
text(xaxis+0.075,yaxis,"Outlying Y",col="blue")

influ <- c(X[valuesd],Y[valuesd])
dim(influ) <- c(length(valuesd),2)
colnames(influ) <- c("X","Y")
@

The bubble plot presented above contains information about the largest residuals (ordenate; Y-axis), the values with highest leverage (abscissa; X-axis) and the values that are most influential (largest bubblest). In the Y-axis, vertical lines are included marking the critical values for indentifying statistically large residuals. In the X-axis, vertical lines are included, representing the $2\times \frac{p}{n}$ and $3\times \frac{p}{n}$ criteria. Regarding the size of the bubbles, we have identified with red crosses, the values for which $D > \frac{4}{n}$. This latter criterion can be chaged by the interested reader.

\newpage
An already existing \textbf{R} function for influence plots

<<echo=TRUE,eval=TRUE,warning=FALSE>>=
#Alternatively using the car package
influencePlot(reg.num)
@

\newpage 
Plot and test for large residuals (dependent variable)

<<echo=TRUE,eval=TRUE,warning=FALSE>>=
# Outlying values
outlierTest(reg.num)
qqPlot(reg.num,labels=row.names(Datos),simulate=T,id.n=1)
@

\newpage 
Identify the values that were pointed oput by the diagnostics 

<<echo=TRUE,eval=TRUE,warning=FALSE>>=
plot(Y~X, cex=2,pch=20)
xs <- length(influ)/2
for (i in 1:xs)
  points(influ[i],influ[xs+i],pch=3,col="red",cex=4)
points(X[value_r],Y[value_r],pch=0,col="blue",cex=4)
title(main= "Red cross = influential & high leverage. Blue square = largest residual")
@

After inspecting all thre graphs,the \enquote{special} data point(s) is/are identified. I the current example, there is one data point that is both influential and a high residual. The location of this value suggests that it probably lift the regression line upwards,or at least its lower part (i.e., shifting the intercept upwards). Moreover, the large residual indicates that the presence of this value leads to lower R-squared. 

\newpage
\subsection{Using a categorical predictor: Between groups design}
\label{sec:simple.linear.cat}

Regression analysis can also be applied when the predictor is a categorical variable, providing identical results (\emph{p} value and R-squared or eta-squared) as analysis of variance (ANOVA). Technically, it is necessary to convert a categorical variable with \emph{k} categories into $k-1$ dummy variables scored 0-1, where 1 represents each of the categories and one serves as a reference category (scored 0 in all dummy variables). In \textbf{R}, it is not necessary for the user to create the dummies, but it is important to consider which the reference category is (usually the one that is alphabetically first).

\subsubsection{Main results}

We first present the results of regression analysis, after defining the dummy variables. Note that for a categorical variable with \emph{k} categories, $k-1$ dummy variables need to be created. In that sense, we would actually be carrying out a multiple regression when there are more than two categories or groups being compared. Nevertheless, multicollinearity will not be an issue as long we do not interpret the \emph{p} values for dummy variables, a point which is further stressed below. Considering that one of the categories is represented by all dummy variables (i.e., all predictors) being scored with a 0, the intercept of the regression equation is always meaningul: it refers to the predicted value of the dependent variable for the reference category.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
# The user creates the dummies
# Dummy #1: 1 assigned to smokers, 0 to everyone else 
smoker <- rep(0,length(PM_failure))
for (i in 1:length(PM_failure))
  if (smoking[i]=="smoker") smoker[i] <- 1
# Make universally applicable
D1 <- smoker

# Dummy #2: 2 assigned to smokers, 0 to everyone else 
# Therefore, non-smoker is the reference
exsmoker  <- rep(0,length(PM_failure))
for (i in 1:length(PM_failure))
  if (smoking[i]=="ex-smoker") exsmoker[i] <- 1
# Make universally applicable
D2 <- exsmoker

# Carry out the (multiple) regression analysis with the two dummies
reg.cat <- lm(Y ~ D1 + D2)
# Obtain the information about the regressiona analysis
summary(reg.cat)
@

We also illustrate how this analysis can be done without such definitions. 

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
#Make the code universally applicable
X <- smoking
Y <- PM_failure

# Let R create the dummies: 
# The reference category is defined by the software
# Same information about the model, but not about the dummy predictos
reg.catR <- lm(Y ~ X)
summary(reg.catR)
@

With the following code we show that the results of ANOVA are equivalent to the ones obtained via regression analysis.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
# This makes the following code generally applicable
X <- smoking
Y <- PM_failure

# ANOVA assuming homoscedasticity
anova0 <-aov(Y ~ X)
summary(anova0)

# The eta-squared values is equal to the regression model's R-squared
totalSS <- sum((Y-mean(Y))^2)
residualSS <- sum((anova0$residuals)^2)
eta_sq <- 1 - residualSS/totalSS
eta_sq
@

The difference between the categories/groups can be represented graphically using a boxplot or a plot of means, with the latter being present in \textbf{R-Commander}, which should be installed once. It should be kept in mind that the values predicted by the regression equation are actually the group means. 

<<echo=TRUE, eval=FALSE>>=
# Install, if necessary
install.packages("Rcmdr")
@

Note that we have included the individual data points on both plots in order to make them more informative. The bars in the plot of means refer to standard deviations, but they can also be defined to refer to the standard errors.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=5,fig.width=6>>=
# Load the R-Commander package necessary for the plot
par(mfrow=c(1,2))
boxplot(Y~X)
for (i in 1:length(table(X)))
  points(jitter(rep(i,length(Y[X==names(table(X))[i]])),factor=0.1),
         as.numeric(Y[X==names(table(X))[i]]),col="blue",pch="*",cex=1.5)

require(Rcmdr)
plotMeans(Y,X,error.bars="sd",ylim=c(min(Y),max(Y)))
for (i in 1:length(table(X)))
  points(jitter(rep(i,length(Y[X==names(table(X))[i]])),factor=0.1),
         as.numeric(Y[X==names(table(X))[i]]),col="blue",pch="*",cex=1.5)
@

Now we show that the \emph{p} values of the dummy variables are not equivalent to carrying out \emph{t}-tests using these dummy variables, as the latter do not control for the other dummy. Moreover, the regression analysis \emph{p} values are also ot equivalent to post hoc tests between pairs of groups.   

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
# The regression slope coefficients for the the dummy variables
# are not the same thing as separate t test to the reference category
t.test(Y ~ D1)
t.test(Y ~ D2)

# The regression slope coefficients for the the dummy variables
# are not the same as post-hoc test
TukeyHSD(anova0)
@

Considering these last examples, it has to be noted that it is usually more meaningful to interpret the R-squared for all (here both) dummy variables that together represent the categorical variable and the statistical significance of this R-squared than the \emph{p} values of each dummy separately, as their meaning is somewhat obscure (the difference between one category and the others, after controlling for the difference between each other category and the remaining categories). For the same reason, all dummies representing a categorical variable need to be entered together in the same step in multiple regression. 

\newpage
\subsubsection{Model validation: Regression analysis}

The assumptions are the same ones as for a simple regression analysis with a quantitative predictor, but the shape of the residuals is different, given that there are only as many predicted (fitted) values as categories.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=4>>=
#Validating the model: Normality of residuals
qqnorm(reg.cat$residuals)
qqline(reg.cat$residuals)
shapiro.test(reg.cat$residuals)

#Validating the model: Homogeneity of variance of residuals
# Load the necessary package
require(lmtest)
bptest(reg.cat)
@

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=6>>=
#Validating the model: Plot fitted vs. residuals
par(mfrow=c(1,2))
plot(reg.cat$fitted, reg.cat$residuals,xlab="Fitted",ylab="Residual")
title(main="Pattern in the residuals?")
lines(sort(reg.cat$fitted),
      loess(reg.cat$residuals~reg.cat$fitted, 
            span=0.8, degree=1)$fitted[order(reg.cat$fitted)],col="red")
stdres <- (reg.cat$residuals-mean(reg.cat$residuals))/sd(reg.cat$residuals)
plot(reg.cat$fitted, stdres,xlab="Fitted",ylab="Standardized residual")
abline(h=c(-1,1),lty="dashed")
abline(h=c(-2,2),lty="dashed",col="orange")
abline(h=c(-3,3),lty="dashed",col="red")
abline(h=0)
@

\newpage
\subsubsection{Model validation: Analysis of variance}

We include this section here to make clear that the assumptions of ANOVA are very similar to the ones of regression analysis, but referring to the data instead of the residuals. We also illustrate the use of the version of the ANOVA not requiring homoskedasticity (achieved by Welch's correction of the degrees of freedom, which make the test more conservative). 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# ANOVA requires normality
tapply(Y, X, shapiro.test)     

# ANOVA requires homogeneity of variance
# Load the necessary package
require(car)
leveneTest(Y ~ X)

# ANOVA not assuming homoscedasticity
oneway.test(Y ~ X, var.equal = FALSE)
@

\newpage
\subsubsection{Diagnostics}

The diagnostics performed here are the same as for regression with a quantitative predictor, but the bubble chart has a different shape given that all predictors are dummies with the same values (0 and 1) and thus the hat values cannot be very different.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
cookds <- cooks.distance(reg.cat)
cook <- sqrt(cookds)
hats <- hatvalues(reg.cat)
rstud <- rstudent(reg.cat)

# Create bubble plot:
# Abscissa: leverage (of X values)
# Ordinate: eliminated studentized residuals (of Y values)
# Bubble size: Cook's distance: influential, if discrepant
plot (hats, rstud, type='n',
      ylab="Studentized eliminated residual", xlab="hat = leverage")
points(hats, rstud,cex=10*cook/max(cook))

abline(h=c(-2, 0, 2), lty=2)
abline(v=2*length(reg.cat$coef)/length(Y), lty=2,col="orange")
title(main=paste("D cut-off =",round(4/(length(Y)),3), 
                 ". Red crosses mark values beyond: influential"))

highest <- 0
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  if (cookds[i] > highest) 
   {highest <- cookds[i] 
    value <- i }
}
valuesd <- rep(0,counterd)
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  valuesd[counterd] <- i}
points(hats[valuesd],rstud[valuesd],col="red",pch=3,cex=2)
@

\newpage
An already existing \textbf{R} function for the influence plot

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
#Alternatively
require(car)
influencePlot(reg.cat)
@

\newpage
Plot and test for large residuals (dependent variable)

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Outlying values
outlierTest(reg.cat)
qqPlot(reg.cat,labels=row.names(Datos),simulate=T,id.n=1)
@

\newpage
\subsection{Robust regression}

Wilcox (2012) discusses the following features of ordinary least squares regression analysis:
\begin{itemize}
\item High Type I error rates: heteroscedastic although normal error
\item High Type II error rates (overestimation of standard errors): heteroscedastic although normal error or homoscedastic error with a heavy tailed distribution
\item Methods for testing the null hypothesis of homoscedasticity  do not control Type I error rates and also do not have sufficient statistical power. 
\end{itemize}

In the current section, we offer R code for some of the alternatives (e.g., skipped regression, deepest regression line) for robust regression reviewed by Wilcox (2012) and we refer the interested reader to his book, which also describes \textbf{R} code created by the author himself for a variety of robust procedures, not only regression analysis. Currently, the webpage in which the code by Wilcox (2012) is available is \url{http://dornsife.usc.edu/labs/rwilcox/software/}, from which a text file (\enquote{Rallfun-v30.txt}) with all the code can be downloaded and loaded in \textbf{R}, after locating it via

<<echo=TRUE, eval=FALSE>>=
# Look for the .txt file in the folder that contains it
source(file.choose())
@

\subsubsection{Theil-Sen estimator}

Correlation and ordinary least squares regression are both sensitive to the presence of outlying values, which is why diagnostics is also part of regression analysis, as illustrated later on. Therefore, alternative ways exist for estimating the intercept and the slope of the regression line. One of these robust alternatives is presented here: the Theil-Sen estimator, which estimates the slope as the median among all lines that connect pairs of measurements, whereas intercept is the median difference between the actual measurements and the values predicted according to the slope and the measurement occasion. The Theil-Sen estimator is part of several ropbust regression procedures discussed by Wilcox (2012): in one of these procedures, this estimator is used after removing outliers (i.e., skipped regression). 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5.3, fig.width=6>>=
# Make the code universally applicable
X <- IQ
Y <- PM_failure

# Represent the data with the OLS trend
plot(X,Y)
title(main="Black line: OLS. Red dashes: Theil-Sen")
abline(lm(Y ~ X))

# Generate the vector that will contain as many values as pairs of points exist
values <- length(X)
slopes <- rep(0,((values*(values-1))/2))

# Useful for copunting the number of comparisons
counter <- 0

# For all pairs of values
for (i in 1:values)
  for (j in 1:values)
    if (i != j) # Not comparing Y values for the same X
      {
        counter <- counter + 1 
        # Difference between values divided by distance in X
        slopes [counter] <- (Y[j]-Y[i])/(X[j]-X[i]) 
      }
# The Theil-Sen estimate is the median of all slopes
slope.estimate <- median(slopes)
slope.estimate

# Generate the vector that will contain as many values as points exist
differ <- rep(0,values)
# For all values
for (i in 1:values) 
  # Compute the difference between the value and the estimate according to slope and X
  differ[i] <- Y[i] - slope.estimate*X[i] 
# The estimate of the intercept is the median of all differences
intercept.estimate <- median(differ)
intercept.estimate

# Add the Theil-Sen slope line to the graph
abline(a=intercept.estimate,b=slope.estimate,lty="dashed",col="red")
@

The Theil-Sen estimator is also available in {\tt mblm} package, used as shown below:

<<echo=TRUE, eval=FALSE>>=
# Install package only once
install.packages("mblm")

# Load package everytime that the function is to be used
require(mblm)

# Call the function for obtaining the Theil-Sen regression estimates
mblm(Y~X,repeated=FALSE)
@

\newpage
\subsubsection{Winsorized regression}

Another option discussed by Wilcox (2012) is winsorized regression, which involves first winsorizing the data (i.e., replacing the most extreme 10\% or 20\% of the ordered data, at both extreme, with the adjacent values) and afterwards carrying out ordinary least squares regression. According to the evidence provided by Wilcox (2012), winsorized regression estimates have smaller standard errors than ordinary least squares regression estimates for heteroscedastic or heavy-tailed distributions of the residuals, but not when X is heavy-tailed and the residuals are normally distributed and homoscedastic. The code that can be used for implementing winsorized regression is as follows:


<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=8, fig.width=5.5>>=
# Make the code universally applicable
X <- IQ
Y <- PM_failure

# Proportion of values to winsorize
gamma <- 0.1
# Number of values to winsorize
k <- ceiling(gamma*length(X))

# Winsorizing
xsorted <- sort(X)
ysorted <- sort(Y)
original <- 1:length(X)

xdata <- 1:(2*length(X))
dim(xdata) <- c(length(X),2)
xdata[,1] <- X
xdata[,2] <- original
xsort <- xdata[order(X),]

ydata <- 1:(2*length(X))
dim(ydata) <- c(length(X),2)
ydata[,1] <- Y
ydata[,2] <- original
ysort <- ydata[order(Y),]

xw <- rep(0,length(X))
yw <- rep(0,length(Y))
for (i in 1:length(X))
{
  if (i <= k+1) 
    {xw[i] <- xsort[k+1,1]
     yw[i] <- ysort[k+1,1]}  
  if ( (i > k+1) && (i < length(X)-k) ) 
    {xw[i] <- xsort[i,1]
     yw[i] <- ysort[i,1]}
  if (i >= length(X)-k) 
    {xw[i] <- xsort[length(X)-k,1]
     yw[i] <- ysort[length(X)-k,1]}
}

# Putting the variables in their original order
yw.unsort <- rep(NA,length(Y))
yw.unsort <- yw[order(ysort[,2])]

xw.unsort <- rep(NA,length(X))
xw.unsort <- xw[order(xsort[,2])]

# Detecting the changes that need to be marked in the plot
# (i.e., the data to winsorize)
# Total number of data points to winsorize for each variable
changes <- 2*k

# Positions for X
position.x <- rep(NA,changes)
counter <- 0
for (i in 1:length(X))
  if (X[i] != xw.unsort[i])
  {
    counter <- counter + 1
    position.x[counter] <- i 
  }

# Positions for Y
position.y <- rep(NA,changes)
counter <- 0
for (i in 1:length(Y))
  if (Y[i] != yw.unsort[i])
  {
    counter <- counter + 1
    position.y[counter] <- i 
  }
@

First, a graphical comparison is oferred, between ordinary least squares regression and Winsorized regression. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=7, fig.width=6>>=
# Representing the results graphically
par(mfrow=c(2,1))

plot(X,Y,xlab="original X",ylab="original Y")
abline(lm(Y~X))
title(main="OLS regression with original data")
title(sub=list(paste("X-data to winsorize in red; Y-data to winsorize in blue"),
               col="orange"))
points(X[position.x],Y[position.x],pch=19,col="red")
points(X[position.y],Y[position.y],pch=19,col="blue")

plot(xw.unsort,yw.unsort,ylim=c(min(Y),max(Y)),xlim=c(min(X),max(X)),
     xlab="winsorized X",ylab="winsorized Y")
abline(lm(yw.unsort~xw.unsort))
title(main="OLS regression with winsorized data")
title(sub=list(paste("Gamma value =",gamma),col="green"))
points(xw.unsort[position.x],yw.unsort[position.x],pch=19,col="green")
points(xw.unsort[position.y],yw.unsort[position.y],pch=19,col="green")
@

\newpage
The numerical results are easily obtained:

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=7, fig.width=5.5>>=
# Numerical comparison
summary(lm(Y~X))
summary(lm(yw.unsort~xw.unsort))
@

It can be seen that, in this case, winsorizing the data leads to better fit of the regression line to the data, that is, to higher R-squared. 

\newpage
\subsubsection{Inference about robust regression parameters: Omnibus test}

In Chapter 11, Wilcox (2012) discusses a robust test for testing null hypothesis about the population values of the regression coefficients. The test is based on taking bootstrap samples and computing the distance between the bootstrap estimates and the null hypothesis values. The test is applicable to any of the procedures for robust regression described in Chapter 10 by Wilcox (2012), including the Theil-Sen estimator discussed here and Winsorized regression discussed in the next subsection. \\[2ex]

We here illustrate our own code for taking bootstrap samples (i.e., sampling rows with replacement from the 15 x 2 matrix containing the 15 values for the two variables: independent and depents) and how the graph with the boostrap estimates and the null hypothesis values can be constructed. On this graph, the Theil-Sen estimates are presented with a green filled triangle, the null hypotheses values (on the one hand, $\beta_0=0 and \beta_1=0$ and, on the other hand, $\beta_0=80$ and $\beta_1=-1$) are represented with red empty squares, and the averages of the bootstrap estimates are represented with a blue filled circle.\\[2ex]

Finally, we illustrate how the code by Wilcox can be used, specifically, the {\tt regtest} function, in which:
\begin{itemize}
\item X is the independent variable (or a vector of independent variables in multiple regression)
\item Y is the dependent variable
\item {\tt regfun=} is used for specifying the robust regression function ({\tt tsreg} is equivalent to {\tt mblm} for applying the Theil-Sen estimator)
\item {\tt nboot=} specifies the number of bootstrap samples
\item {\tt alpha=} specifies the nominal significance level
\item {\tt grp=c()} specifies the regression coefficients for which a statistical significance test is desired (0 refers to the intercept and 1 refers to the slope coeficient for the first predictor or the only predictor in simple linear regression)
\item {\tt nullvec=c()} specifies the null hypothesis values for the regression coefficients, in the same order as specified in {\tt grp=c()} and also the same number of coefficients is obviously required.
\end{itemize}

The main output of the {\tt regtest} function is the {\tt \$ p.value} which indicates whether the null hypothesis values for the parameters can be rejected as unlikely. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=4.5, fig.width=6>>=
# Make the code universally applicable
X <- IQ
Y <- PM_failure
alldat <- cbind(X,Y)

# Create the objects needed for bootstrap
bsamples <- 500
b.int <- rep(NA,bsamples)
b.slo <- rep(NA,bsamples)

# Load the package required for the Theil-Sen estimator
require(mblm)

# Take bootstrap samples and save the regression coefficients estimates
for (i in 1:bsamples)
{
  bsam <- alldat[sample(length(X), length(X),replace=T),]
  xb <- bsam[,1]
  yb <- bsam[,2]
  b.int[i] <- mblm(yb~xb,repeated=FALSE)$coef[[1]]
  b.slo[i] <- mblm(yb~xb,repeated=FALSE)$coef[[2]]
}

# Specify the Theil-Sen estimates for the actual data
int.actual <- mblm(Y~X,repeated=FALSE)$coef[[1]]
slo.actual <- mblm(Y~X,repeated=FALSE)$coef[[2]]  

# Specify some interesting null hypothesis values
int.hypoth <- 80
slo.hypoth <- -1    

# Prepare the space for the graphical representations
par(mfrow=c(1,3))

# Graphical representation of our bootstrap samples
plot(b.int,b.slo,pch="*")
# Plot the average of the bootstrap estimates with a blue filled circle
points(mean(b.int),mean(b.slo),pch=16,col="blue",cex=3)
# Plot Theil-Sen estimates with a green filled triangle
points(int.actual,slo.actual,pch=17,col="green",cex=3)
# Plot the potentially relevant null hypotheses values with red empty squares
points(int.hypoth,slo.hypoth,pch=0,col="red",cex=1.5)
# Plot the usual null hypotheses values of zero with red empty squares
points(0,0,pch=0,col="red",cex=1.5)
abline(h=quantile(b.slo,probs=c(0.025,0.975))[1],col="green") 
abline(h=quantile(b.slo,probs=c(0.025,0.975))[2],col="green") 

# Locate the .txt file by Wilcox containing all the R code he created
source('Rallfun-v30.txt')

# In this case the "regtest" function is used for a robust test
# on the regression coeffcients
# Applicable to any robust regression technique; default: Theil-Sen

# Testing the null hypothesis that beta0 = beta1 = 0
# Numerical and graphical results are included
regtest(X,Y,regfun=tsreg,nboot=500,alpha=0.05,grp=c(0,1),nullvec=c(0,0))

# Testing the null hypothesis that beta0 = 80 and beta1 = -1
# Numerical and graphical results are included
regtest(X,Y,regfun=tsreg,nboot=500,alpha=0.05,grp=c(0,1),nullvec=c(80,-1))
@

\newpage
\subsubsection{Inference about robust regression parameters: Confidence intervals}
In the first panel of the plot presented in the previous page we represented, via horizontal green lines, the limits of the 95\% confidence interval for the slope coefficient. This confidence interval was obtained using the percentile bootstrap method, as suggested by Wilcox (2012). In contrast, with some other robust methods, for inference regarding the coefficients of robust regression, Wilcox (2012) recommends using the percentile bootstrap method without any modification. In that sense, it is first necessary to obtain a specific number \emph{B} of bootstrap samples. The confidence interval with a degree of confidence equal to $1-\alpha$ is represented by quantiles $Q_{\alpha/2}$ and $Q_{1-(\alpha/2)}$ of the boostrap distribution of estimates. In what follows we repeat the code for bootstrap and we also show that the results obtained via the {\tt regci} function developed by Wilcox (2012) are practically identical, with any differences being due to sampling variability, as 1000 bootstrap samples are generated two times (once with our code and once with Wilcox's code). Note also that the standard error of the coefficients is estimated as the standard deviation of the bootstrap estimates.In order to be able to use this part of the code separately, we once again include the line for locating Wilcox's code.


<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
# Make the code universally applicable
X <- IQ
Y <- PM_failure
alldat <- cbind(X,Y)

# Create the objects needed for bootstrap
bsamples <- 1000
b.int <- rep(NA,bsamples)
b.slo <- rep(NA,bsamples)

# Load the package required for the Theil-Sen estimator
require(mblm)

# Take bootstrap samples and save the regression coefficients estimates
for (i in 1:bsamples)
{
  bsam <- alldat[sample(length(X), length(X),replace=T),]
  xb <- bsam[,1]
  yb <- bsam[,2]
  b.int[i] <- mblm(yb~xb,repeated=FALSE)$coef[[1]]
  b.slo[i] <- mblm(yb~xb,repeated=FALSE)$coef[[2]]
}

# Specify alpha, the complementary to the degree of confidence
alpha <- 0.05

# Confidence interval for the intercept
round(quantile(b.int,probs=c(alpha/2,(1-alpha/2))),2)

# Confidence interval for the slope coefficient
round(quantile(b.slo,probs=c(alpha/2,(1-alpha/2))),2)

# Standard error for the intercept: SD of the bootstrap distribution
sd(b.int)
# Standard error for the slope coefficient: SD of the bootstrap distribution
sd(b.slo)

# Locate the .txt file by Wilcox containing all the R code he created
source('Rallfun-v30.txt')

# Confidence intervals as obtained via Wilcox's function
regci(X,Y,xout=FALSE,regfun=tsreg,nboot=bsamples,alpha=alpha)
@

\newpage
\section{Multiple linear regression}

\subsection{Model specification: Hierarchical or sequential}
\label{sec:multiple.linear.hierarchical}

\subsubsection{Use and main results}

Hierarchical regression is used when the analysts has a solid reason for including some variable(s) in the model prior to other(s). This way of introducing the predictors in the model means that for the first one R-squared is available, whereas for the second predictor the change in R-square, $\Delta R^2$, (semi-partial R-squared) is available: the unique contribution of the second predictor beyond what the first one has explained from the depedent variable. This change in R-squared can be obtained simply as the subtraction of the R-squared of the model in the first step from the R-squared of the model in the second step. Moreover, the statistical significance of the change in R-squared can be obtained using the {\tt anova} function. \\[2ex]

Another use of hierarchical regression is for statistical control, that is, introducing extraneous (potential confounding) variables in the initial step(s) of the analysis in order to control for their iinfluence prior to exploring the additional effect of the main predictor(s). The same purpose is achieved via analysis of covariance discussed in section \ref{sec:ancova}.\\[2ex]

Finally, before we proceed with the example of analyses, hierarchical or sequential regression should not be confounded with hierarchical linear models or multilevel models, as commented on in section \ref{sec:multilevel}.\\[2ex]

The main results are presented below:

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Carry out simple linear regression with a quantitative predictor
# represeting a variable that has to be controlled for
reg1 <- lm(PM_failure ~ age)
cat("Step 1: R-squared", "\n")
summary(reg1)$r.squared

# Carry out multiple linear regression with a second quantitative predictor
# represeting another variable that has to be controlled for
reg2 <- lm(PM_failure ~ age + IQ)
cat("Step 2: Change in R-squared", "\n")
summary(reg2)$r.squared - summary(reg1)$r.squared

# Carry out multiple linear regression with the main (categorical) predictor
# in the third step
reg3 <- lm(PM_failure ~ age + IQ + smoker + exsmoker)
cat("Step 3: Change in R-squared", "\n")
summary(reg3)$r.squared - summary(reg2)$r.squared
cat("Step 3: Statistical significance of the change in R-squared", "\n")
anova(reg3,reg2)$"Pr(>F)"[2]

# Carry out multiple linear regression with another quantitative predictor
# although it has no conceptual meaning in this step
reg4 <- lm(PM_failure ~ age + IQ + smoker + exsmoker + depression)
cat("Step 4: Change in R-squared", "\n")
summary(reg4)$r.squared - summary(reg3)$r.squared
cat("Step 4: Statistical significance of the change in R-squared", "\n")
anova(reg4,reg3)$"Pr(>F)"[2]
@

\subsubsection{Model validation}

Apart from the assumptions about the residuals, which are the same as for simple linear regression (see section \ref{subsec:linearvalid}), there is an additional aspect that needs to be verified: whether the predictors are related among themselves, called \enquote{multicollinearity}. The issue with multicollinearity is that a large R-squared can be obtained (if one or several of the predictors are useful) but few or none of the slope coefficients may be statistically significant. This is due to the fact tha multicollinearity implies that several predictors account for the same part of the variability of the dependent variable, that is, their unique contribution (as seen in the last step of the regression model, or in the only step of a simultaneous method for entering the predictors in the regression analysis) is very small. \\[2ex]
I the following, we first explore for multicollinearity by exploring the correlation of pairs of potential predictors. Usually a correlation greater than 0.8 is considered to be problematic. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Explore the bivariate correlations between potential predictors
cor(IQ, self_PM)
cor(depression, self_PM)
cor(IQ, age)
cor(IQ, depression)
cor(age, depression) 
@

Checking multicollinearity visually. In the following we present a scatterplot using the code from \url{http://www.r-bloggers.com/scatter-plot-matrices-in-r/}.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
#Scatterplot function: should be included before calling it

panel.cor <- function(x, y, digits = 2, cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  # correlation coefficient
  r <- cor(x, y)
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.6, txt)

  # p-value calculation
  p <- cor.test(x, y)$p.value
  txt2 <- format(c(p, 0.123456789), digits = digits)[1]
  txt2 <- paste("p= ", txt2, sep = "")
  if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
  text(0.5, 0.4, txt2)
}

# Saving the potential predictors in a data frame
predictors <- data.frame(smoker,exsmoker,IQ,age,depression)
# Calling the function for the scatterplot
pairs(predictors, upper.panel = panel.cor)
@

Further checks of multicollinearity can be performed using the variance inflation factor (VIF), which quantifies the number of times the standard error of the estimate of the regression coefficient increases due to the relation between the predictors. Increasing the standard error entails increasing the \emph{p} value and thus makes less likely that the estimate is statistically significant. Mathematically, $VIF=\frac{1}{{\tt Tolerance}}$, where Tolerance is defined as $T = 1 - R^2$, referring to the (multiple) R-squared value of the predictor with all other predictors, that is, if each predictor is used as a dependent variable to check how much of its variability the remaining predictors account for. In the following code, we use both the {\tt vif()} function already implemented in \textbf{R} and the calculation of VIF via R-squared and Tolerance, in order to make clearer what is being done. Finally, note that when there are two predictors the Tolerance for both of them (and therefore also VIF) is the same.\\[2ex]

Regarding VIF values that indicate multicollinearity, several values have been suggested: 2, 5, or 10. In any case, the larger VIF, the larger the multicollinearity. Analogously, the closer the Tolerance to zero, the larger the multicollinearity (given that it would stem from a larger R-squared between the predictors).    

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Using functions: Models 2, 3 and 4
require(car)
vif(reg2)
vif(reg3)
vif(reg4)

# Additional code: Model 2
cor(age, IQ)
tolerance <- 1 - cor(age, IQ)^2
tolerance
VIF <- 1/tolerance
VIF

# Additional code: Model 4: VIF for depression (analogously for remaining predictors)
# Always excluding the dependent variable
reg.mc_dep <- lm(depression ~ IQ + smoker + exsmoker + age)
tolerance_dep <- 1 - summary(reg.mc_dep)$r.squared
tolerance_dep
VIF_dep <- 1/(1 - summary(reg.mc_dep)$r.squared)
VIF_dep
@

\newpage
\subsection{Model specification: Stepwise}
\label{sec:multiple.linear.stepwise}

The stepwise method for choosing predictors is data-driven, in contrast with the theory-driven hierarchical (sequential) regression. It has been suggested that it is necessary to have at least 40 measurements per predictor variable tested to be included or not in the model, in order to ensure adequate Type I and Type II error rates. In that sense, the current example does not illustrate a recommendable practice, as there are only 15 individuals in the sample.

\subsubsection{Selecting predictors}

It is recommended to build an initial model with all potetial predictors. The stepwise selection is then carried out in a backward/forward fashion. This means that the primary aim is to eliminate all the unnecessary predictors from the model and retain only the useful ones, according to the statistical criterion. Due to the characteristics of the stepwise method, it is possible that a predictor is excluded from the model at a given step and is re-introduced in a later step, if that is justified statistically. \\[2ex]

The following code can be used in \textbf{R} for carrying out a stepwise regression analysis. In the output we see the differet steps in the process and we obtain detailed information about the final result. 


<<echo=TRUE, eval=TRUE,warning=FALSE>>=
reg4 <- lm(PM_failure ~ IQ + depression + self_PM)
summary(reg5 <- step(reg4, direction='both'))
@

Another function for stepwise regression analysis is available from the \textbf{MASS} package.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Alternative 1
require(MASS)
stepAIC(reg4, direction="both")$anova
@

Another function is part of the \textbf{R-Commander} package. Here, it is possible to use BIC (Bayesian Information Criterion) instead of AIC (Akaike Information Criterion), although the output still used the label \enquote{AIC}. BIC is more conservative than AIC and thus is more likely to exclude unnecessary predictors from the model. It may be necessary to install \textbf{R-Commander} first.

<<echo=TRUE, eval=FALSE>>=
install.packages("Rcmdr")
@

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Alternative 2
require(Rcmdr)
stepwise(reg4, direction='backward/forward', criterion='BIC')
@

The results obtained with the previously presented functions need not agree with the results provided by a statistical package such a \textbf{IBM SPSS}\textsuperscript{\textregistered}, given that in these \textbf{R} functions AIC or BIC are used for selecting predictors, whereas in \textbf{IBM SPSS}\textsuperscript{\textregistered} an \emph{F} test is used. For that reason, to avoid cofusions and to ensure transparency, it is necessary to report which statistical package was used for carrying out the analysis and what criterion was used for selecting the predictors to be present in the model. 

\newpage
\subsubsection{Cross validation}
\label{sec:cross}

Given that stepwise regression analysis is sometimes criticized for being too data-driven and leading to solutions that are specific to the data set actually used and not representing any other data, cross validation has been suggested as a way of exploring whether the model fits well other data as well. In the current illustration we are using a sample of older individuals, as it was one of the limitations of the research by Heffernan et al. (2012). The sample in which the model was estimated is usually called a \enquote{training sample}, whereas the new one is a \enquote{testing sample}. \\[2ex]

When performing the cross validation, an R-squared value can be computed. The total variability is, as always, the sum of squared differences between each value of the dependent variable and its mean. The residual variability is the sum of squared differences between each value of the dependent variable and each predicted value for the same values of the predictors. The explained variability is the complementary of the proportion of residual to total variability. Note that in this case the regression line fitted is not straight, given that it is a function of more than one predictor. Regarding the judgement of whether the R-squared value obtained is large enough to indicate that the stepwise model is also valid for the new data, it is difficult to provide definitive rule of thumb. However, the guidelines reviewed by Kotrlik, Williams and Jabor (2011) can be useful: for instance, Cohen's benchmarks for multiplie regression suggest the values .0196, .1300 and .2600 small, medium, and large, respectively. 

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4,fig.width=6>>=
# Obtain the predicted values using the regression coefficients estimates
# from the previously identified model via stepwise
# and the predictor variables values from the new dataset
pred <- reg5$coefficients[1] + reg5$coefficients[2]*Datos2$depression +
        reg5$coefficients[3]*Datos2$self_PM

# Represent fit graphically
par(mfrow=c(1,2))
# New data: predictor "depression"
plot(Datos2$depression,Datos2$PM_failure)
# Fit of the stepwise-generated regression equation to the new data
lines(sort(Datos2$depression),pred[order(Datos2$depression)])
# New data: predictor "self_PM"
plot(Datos2$self_PM,Datos2$PM_failure)
# Fit of the stepwise-generated regression equation to the new data
lines(sort(Datos2$self_PM),pred[order(Datos2$self_PM)])

# Obtain residual variance: Sum of squared error
sum((pred-Datos2$PM_failure)^2)
# Obtain total variance
sum((Datos2$PM_failure-mean(Datos2$PM_failure))^2)
# Obtain fit, R-squared
1 - sum((pred-Datos2$PM_failure)^2)/sum((Datos2$PM_failure-mean(Datos2$PM_failure))^2)
# Mean square error
sum((pred-Datos2$PM_failure)^2)/length(PM_failure)
@

Cross-validation can be carried out in several different ways. Using two different samples, as illustrated here is only one of the options. Actually, it is possible to split repetitively the sample into two portions and estimate and test the model repetitively, as well. The fit or the complementary prediction error are obtained as the average of the ones obtaied for each split. This option is called \emph{k}-fold cross validation, with the simplest case being a two-fold cross validation (or holdout method): (1) the sample randomly split into two halves; (2) Half 1 is used as a training sample and Half 2 as a testing sample; (3) The mean square error (MSE: average of the squared residuals) is obtained;  (4) Half 2 is used as a training sample and Half 3 as a testing sample; (5) The MSE is obtained; (6) The average of the two MSEs is obtained. Note that with a sample as small as the current one (15 idividuals) there are two major issues. First, the odd number of participants makes a 50\% split impossible. The code we present suggests that the first training sample be of 7 individuals and the first testing sample of 8. Second, the random sampling of individuals to belong to one of the samples may have a large effect on the estimates of the regression coefficients. In that sense, it is possible obtain reasonably small MSEs for some bipartitions of the sample and larger MSE values for other bipartitions. 

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=6, fig.width=6>>=
# Define the dependent and predictor variables so that the coode 
# can be useful beyond the current data
Y <- PM_failure
X1 <- depression
X2 <- self_PM

# Define the size of oe of the samples
if (length(Y)%%2==0) sizehalf <- length(Y)/2 else sizehalf <- (length(Y)-1)/2 
# Identify which individuals will belong to the samples first used for trainig
ids <- 1:length(Y)
s1 <- sample(ids, size=sizehalf, replace=FALSE)
Y.s1 <- Y[s1]
X1.s1 <- X1[s1]
X2.s1 <- X2[s1]
# Assign the individuals to the sample first used for testing
Y.s2 <- Y[-s1]
X1.s2 <- X1[-s1]
X2.s2 <- X2[-s1]

# First training
coeffs <- lm(Y.s1 ~ X1.s1 + X2.s1)$coefficients
# First testing
predicted <- coeffs[[1]] + coeffs[[2]]*X1.s2 + coeffs[[3]]*X2.s2
MSE.1 <- sum((Y.s2 - predicted)^2)/length(Y.s2)

# Represent the fit the training model to the testing sample
par(mfrow=c(2,2))
plot(X1.s2,Y.s2,ylim=c(min(Y,predicted),max(Y,predicted)),
     xlab="Predictor 1 from sample 2", ylab="DV values from sample 2",
     main="Fit: reg.equation from sample 1")
lines(sort(X1.s2),predicted[order(X1.s2)])
plot(X2.s2,Y.s2,,ylim=c(min(Y,predicted),max(Y,predicted)),
     xlab="Predictor 2 from sample 2", ylab="DV values from sample 2",
     main="Fit: reg.equation from sample 1")
lines(sort(X2.s2),predicted[order(X2.s2)])

# Second training
coeffs <- lm(Y.s2 ~ X1.s2 + X2.s2)$coefficients
# Second testing
predicted <- coeffs[[1]] + coeffs[[2]]*X1.s1 + coeffs[[3]]*X2.s1
MSE.2 <- sum((Y.s1 - predicted)^2)/length(Y.s1)

# Represent the fit the training model to the testing sample
plot(X1.s1,Y.s1,ylim=c(min(Y,predicted),max(Y,predicted)),
     xlab="Predictor 1 from sample 1", ylab="DV values from sample 1",
     main="Fit: reg.equation from sample 2")
lines(sort(X1.s1),predicted[order(X1.s1)])
plot(X2.s1,Y.s1,,ylim=c(min(Y,predicted),max(Y,predicted)),
     xlab="Predictor 2 from sample 1", ylab="DV values from sample 1",
     main="Fit: reg.equation from sample 2")
lines(sort(X2.s1),predicted[order(X2.s1)])

#Average MSE
(MSE.1+MSE.2)/2
@

Another form of cross-validation entails fitting as many models as values are in the sample, and for each of the models leave one of the individuals out of the dataset used for estimating the model. Afterwards, the value for that individual is predicted from the estimated regression equation and the residual is computed. The MSE can then be obtained as a measure of prediction error. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Create the vector where the residual will be kept
resid <- rep(0,length(Y))

# Cary out as many regression analysis (with the oridinal data)
# as individual are in the sample, excluding one person at a time
for (i in 1:length(resid))
{
  coeffs <- lm(Y[-i] ~ X1[-i] + X2[-i])$coefficients
  predicted <- coeffs[[1]] + coeffs[[2]]*X1[i] + coeffs[[3]]*X2[i]
  resid[i] <- Y[i] - predicted
}

# Sum of squared error
sum(resid^2)
# Mean square error
sum(resid^2)/length(Y)
@

Finally, cross validation can also be performed using bootstrap procedures (see Efron \& Tibshirani, 1997)

\newpage
\subsection{Mediation analysis}
\label{sec:multiple.mediation}

Mediation analysis is one of the possible ways of dealing with situations in which there are three variables involved. Mediation refers to the situation in which the effect of a predictor {\tt X} on a response variable {\tt Y} takes place via another variable {\tt Z}. Several requirements need to be met in order to identify mediation: (1) the predictor {\tt X} occurs before the potential mediator {\tt Z}, which occurs before the reponse variable {\tt Y}: this ensures that mediation is possible chronologically; (2) in an initial simple regression analysis, there must a (statistically significant) effect of {\tt X} on {\tt Y}: an effect which be subjected to analysis to explore whether it is mediated or not; (3) in another simple regression analysis, there must a (statistically significant) effect of {\tt X} on {\tt Z}: {\tt X} is expected to be the cause of the changes in {\tt Z}, if {\tt Z} is to be a mediator (i.e., to be justified to talk about a causal chain); (4) in a multiple regression analysis with {\tt X} and {\tt Z} as predictors, there must a (statistically significant) effect of {\tt Z} on {\tt Y}: {\tt Z} is expected to be the cause of the changes in {\tt Y}, if {\tt Z} is to be a mediator; (5) in the same multiple regression analysis with X and {\tt Z} as predictors, the \enquote{direct effect} of {\tt X} on {\tt Y} should be diminished (i.e., the \emph{p} value should be increased and, ideally, be greater than the conventional .05) in order to identify \enquote{complete mediation}, if this effect remains statistically significant, the term \enquote{partial mediation} is used. \\[2ex]

Two other situations involving three variables are: (a) confusion: a third variable {\tt W} affects (i.e., is a predictor of) both {\tt X} and {\tt Y}; (b) covariance: a third variable {\tt W} is another predictor of {\tt Y}, just as {\tt X} is a predictor of it.\\[2ex]

The code presented here is adapted from Wright and London (2009), see \url{https://uk.sagepub.com/en-gb/eur/modern-regression-techniques-using-r/book233198#resources}. The \emph{p} values associated with the regression coefficients are presented in green if they agree with the criteria mentioned above. In contrast, if statistical significance values do not agree to the requirements for a mediation effect, the \emph{p} value(s) is/are in presented red.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
mediator <- function(x,y,m, ...){
# Put in experimental variable x, outcome variable y, then mediator m.
# Some of this is written so that it works in a mediator
# function for more complex problems.

# Initial simple linear regression analysis
reg0 <- lm(y~x)
# Another simple linear regression analysis
reg1 <- lm(m~x)
# Multiple linear regression analysis
reg2 <- lm(y~m+x)
# Save the regression slope coefficients' estimates
# and their associated p values
c <- summary(reg0)$coefficients[2,1]
csig <- summary(reg0)$coefficients["x","Pr(>|t|)"]
sc <- summary(reg0)$coefficients[2,2]
a <- summary(reg1)$coefficients[2,1]
asig <- summary(reg1)$coefficients["x","Pr(>|t|)"]
sa <- summary(reg1)$coefficients[2,2]
b <- summary(reg2)$coefficients[2,1]
bsig <- summary(reg2)$coefficients["m","Pr(>|t|)"]
sb <- summary(reg2)$coefficients[2,2]
cp <- summary(reg2)$coefficients[3,1]
cpsig <- summary(reg2)$coefficients["x","Pr(>|t|)"]
scp <- summary(reg2)$coefficients[3,2]
# Represent grapicahhy the results of the mediation analysis
plot(c(0,100),c(0,110),col="white",ann=F,tck=0,col.axis="white")
# Draw boxes
rect(c(10,10,70,70,40),c(10,50,10,50,80),c(30,30,90,90,60),c(30,70,30,70,100))
# Draw boxes
arrows(c(30,30,20,60),c(20,60,70,90),c(70,70,40,80),c(20,60,90,70),length=.15)
# Represent the codes for the variables involved
text(c(20,20,80,80,50),c(20,60,20,60,90),c("X","X","Y","Y","M"),cex=2)
# Represent the estimated regression coefficients 
# and their p values in colors
text(30,80,paste(format(a,digits=2,nsmall=2)),pos=2,cex=1.3)
if (asig <= 0.05) 
  text(31,70,paste(format(asig,digits=2,nsmall=2)),pos=3,cex=0.8,col="green")
if (asig > 0.05) 
  text(31,70,paste(format(asig,digits=2,nsmall=2)),pos=3,cex=0.8,col="red")
text(70,80,paste(format(b,digits=2,nsmall=2)),pos=4,cex=1.3)
if (bsig <= 0.05) 
  text(71,70,paste(format(bsig,digits=2,nsmall=2)),pos=3,cex=0.8,col="green")
if (bsig > 0.05) 
  text(71,70,paste(format(bsig,digits=2,nsmall=2)),pos=3,cex=0.8,col="red")
text(50,20,paste(format(c,digits=2,nsmall=2)),pos=3,cex=1.3)
if (csig <= 0.05) 
  text(50,13,paste(format(csig,digits=2,nsmall=2)),pos=3,cex=0.8,col="green")
if (csig > 0.05) 
  text(50,13,paste(format(csig,digits=2,nsmall=2)),pos=3,cex=0.8,col="red")
text(50,60,paste(format(cp,digits=2,nsmall=2)),pos=3,cex=1.3)
if (cpsig > 0.05) 
  text(50,53,paste(format(cpsig,digits=2,nsmall=2)),pos=3,cex=0.8,col="green")
if (cpsig <= 0.05) 
  text(50,53,paste(format(cpsig,digits=2,nsmall=2)),pos=3,cex=0.8,col="red")
}

mediator(smoker,PM_failure,self_PM)
@

A term similar to \enquote{mediation} is \enquote{moderation}. A moderator variable is not necessary for the effect of {\tt X} on {\tt Y}, but it modifies the strength of association between {\tt X} and {\tt Y}. Moderator analysis is common in meta-analytical intergrations of several studies, when the variability in effect sizes (quantifying the magnitude of effect of {\tt X} [e.g., a psychological intervention] on {\tt Y} [e.g., a score in an inventory on depression] or their strength of association) is accounted for via the inclusion of moderator variables (e.g., the age of the participants, the duration of the therapy, the country in which the study took place). Such a moderator analysis can also be performed via regression analysis. \\[2ex]

\newpage
The previously presented code and reasoning is based on the causal steps approach of Baron and Kenny (1996), although it is not the only approach available for testing mediation or the presents of an intervening variable: MacKinnon, Lockwood, Hoffman, West, and Sheets (2002) test the Type I error rates and statistical power of several approaches. One of the procedures that performs best (and better than the approach by Baron and Kelly) and is also straighforward to apply entails obtaining the Z-versions of the regression coefficients for Model 2 (using {\tt X} to predict {\tt M}) and Model 3 (using {\tt M} to predict {\tt Y} when {\tt X} is also in the model) by dividing the estimates by their standard deviations (i.e., standard errors). The assumption made is that the regression coefficients are normally distributed. The statistical significance of the mediation effect is tested via a comparison to tabulated critical values. For instance, for the commonly used .05 nominal level, the critical value is 2.18. The following code implements this test for mediation, whereas the interested reader is referred to MacKinnon et al. (2002) for more specific information about the performance of all approaches tested by these authors.  

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
smoker <- rep(0,length(PM_failure))
for (i in 1:length(PM_failure))
  if (smoking[i]=="smoker") smoker[i] <- 1

x <- smoker
y <- PM_failure
m <- self_PM

# Initial simple linear regression analysis
reg0 <- lm(y~x)
# Another simple linear regression analysis
reg1 <- lm(m~x)
# Multiple linear regression analysis
reg2 <- lm(y~m+x)

# Obtain the Z corresponding to the effect of X on M 
# (divide coefficient by its SE)
z.alfa <- summary(reg1)$coefficients[2,1]/summary(reg1)$coefficients[2,2]
paste("Z for the effect of X on M = ",round(z.alfa,2))
# Obtain the Z corresponding to the effect of M on Y
# (divide coefficient by its SE)
z.beta <- summary(reg2)$coefficients[2,1]/summary(reg2)$coefficients[2,2]
paste("Z for the effect of M on Y, with X in the model = ",round(z.beta,2))

# Obtain the product of the Z
p.z <- z.alfa*z.beta
paste("Product of Zs = ",round(p.z,2))
# Compare the product to the critical value reported by MacKinnon et a.
if (p.z >= 2.18) print("Mediation is statistically significant") else 
  print("Mediation is not statistically significant")
@

\newpage
\subsection{Multiple regression with interaction: Single-case designs}
\label{sec:single-case}

The present section shows an application to single-case experimental designs, which entail gathering data longitudinally: that it, there are repeated measures of the same unit (an individual or a group) taking place in different conditions. These conditions are also called phases; A is used to denote a baseline phase (before the intervention has been introduced or after it has been withdrawn) and B denotes the phase in which the intervention is active. The data used in the present section correspond to the two phases (A and B) of a four-phase (ABAB) design study conducted by Dolezal el al. (2007). 

\subsubsection{Presenting the modelling options}

There are two main aspects of interest when modelling single-case data. First, the measurements can be modelled as a function of type, like in pure longitudinal designs in which the evolution of a unit is studied without any intervention by the researcher. Time is modelled as a variable taking value from 1 to the number of measurements, with increases of 1. In that sense, it can be explored whether the measurements increase or decrease with time (i.e., show an upward or a downward trend) and whether this change is linear, quadratic, cubic. This latter aspect is also dealt with when discussing \nameref{sec:polynomial}. Second, the measurements can be modelled as a function of the condition in which they have been produced. This aspects refers to the changes in the target behavior as a function of the intervention. Typically, a dummy variable is used, coding baseline phases with 0 and intervention phases with 1. It is possible to model whether the data change according to the phase even after accounting for any trends not related to the intervention: several procedures have been suggested for controlling for trend prior to studying the effect of the intervention and later in the document we present \nameref{sec:piecewise}. \\[2ex]

Apart from trend and the effect of the intervention, it is possible to construct an interaction term representing the effect of the intervention on the time trend. This effect is called \enquote{change in slope}, expecting that (a) a deteriorating trend is stopped or converted into an improving trend; or (b) an improving trend becomes even more pronounced with the introduction of the intervention. The interaction term is just the multiplication between the Time variable and the dummy variable representing the phase, although a slight modification has been proposed by Huitema and McKean (2000) to improve the interpretability of the interaction term: see the difference between the {\tt tp} and {\tt tph} variables specified in the following box. \\[2ex]

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Enter the data:
# Outcome
words <- c(200,217,220,230,255,295) 
# Variable modelling time
tiempo <- c(1:6) 
# Dummy variable for the condition (phase)
phase <- c(0,0,0,1,1,1) 
# Interaction term representing slope change
tp <- tiempo*phase 
# Alternative specification of the interaction term
tph <- (tiempo - (length(words[phase==0])+1))*phase

#Plot 
par(mfrow=c(2,2))

plot(tiempo,words,main="General trend only (i.e., no change)",xlab="Measurement time",pch=19)
abline(lm(words~tiempo),col="blue")
abline(v=(max(tiempo[phase==0]))+0.5,lty="dashed")

plot(tiempo,words,main="Change in level",xlab="Measurement time",pch=19)
lines(tiempo,fitted(lm(words~phase)),col="blue")
abline(v=(max(tiempo[phase==0]))+0.5,lty="dashed")

plot(tiempo,words,main="Trend and change in level",xlab="Measurement time",pch=19)
lines(tiempo,fitted(lm(words~tiempo+phase)),col="blue")
abline(v=(max(tiempo[phase==0]))+0.5,lty="dashed")

plot(tiempo,words,main="Trend and changes in level and slope",xlab="Measurement time",pch=19)
lines(tiempo,fitted(lm(words~tiempo+phase+tp)),col="blue")
abline(v=(max(tiempo[phase==0]))+0.5,lty="dashed")

# First step: regression with a single predictor
reg.i1 <- lm(words~tiempo)

# Second step: regression with the two predictors
reg.i2 <- lm(words~tiempo+phase)
# Obtain the change in R-squared 
summary(reg.i2) $r.squared - summary(reg.i1) $r.squared
# Obtain the significance of the change in R-squared
anova(reg.i1,reg.i2)

# Third step: regression with the three predictors
reg.i3 <- lm(words~tiempo+phase+tph)
# Obtain the change in R-squared 
summary(reg.i3)$r.squared - summary(reg.i2) $r.squared
# Obtain the significance of the change in R-squared
anova(reg.i2,reg.i3)

# Another option for checking the significance of each added predictor separately
add1(reg.i1,.~. + phase, test="F")[6]
add1(reg.i2,.~. + tph, test="F")[6]
@

Model validation: in single-case designs data are gathered longitudinally and, therefore, it is crucial to check the assumption of independence of the residuals via the Durbin-Watson test. This assumption is specific to longitudinal designs, which is the reason why it was not tested previously. Just like with the assumptions of normality and heteroskedasticity, meeting the assumption requires not rejecting the null hypothesis, which here postulates the independence of the residual (i.e., the serial dependence, also known as autocorrelation is not statistically significant). Actually, some proposals for adapting regression analysis to single-case designs (e.g., Swaminathan, Rogers, Horner, Sugai, \& Smolkowski, 2014) data include explicit modelling of autocorrelation. Moreover, autocorrelation has been suggested to be modelled when performing multilevel analysis. 


<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=3,fig.width=3>>=
# Normality QQ (quantiles) plot
qqnorm(reg.i3$residuals)
qqline(reg.i3$residuals)

# Normality (Shapiro-Wilk) test
shapiro.test(reg.i3$residuals)
@

For performing the Durbin-Watson test, the \textbf{lmtest} packages is necessary. If not previously available, it has to be installed, before using it. \textbf{lmtest} is also necessary for the Breusch-Pagan test of homogeneity of variance.

<<echo=TRUE, eval=FALSE,warning=FALSE>>=
# Install the lmtest package only once
instal.packages("lmtest")
@

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=3,fig.width=3>>=
# Testing Independence requires loading the lmtest package
require(lmtest)
dwtest(reg.i3)

# Homogeneity of the residual (also requires lmtest)
bptest(reg.i3)
@

Visual inspection of the residual can be performed in addition to the statistical tests to explore whether their results might be affected by some specific features of the data. Moreover, homogeneous variances does not imply the absence of pattern in the residual.  

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=3.5,fig.width=5>>=
#Validating the model: Plot fitted vs. residuals
par(mfrow=c(1,2))
plot(reg.i3$fitted, reg.i3$residuals,xlab="Fitted",ylab="Residual")
title(main="Pattern in the residuals?")
lines(sort(reg.i3$fitted),
      loess(reg.i3$residuals~reg.i3$fitted, 
            span=0.8, degree=1)$fitted[order(reg.i3$fitted)],col="red")
stdres <- (reg.i3$residuals-mean(reg.i3$residuals))/sd(reg.i3$residuals)
plot(reg.i3$fitted, stdres,xlab="Fitted",ylab="Standardized residual")
abline(h=c(-1,1),lty="dashed")
abline(h=c(-2,2),lty="dashed",col="orange")
abline(h=c(-3,3),lty="dashed",col="red")
abline(h=0)
@

\newpage
\subsubsection{Piecewise regression}
\label{sec:piecewise}

Several proposals have been made for applying regression analysis to single-case data (e.g., Allison \& Gorman, 1993; Center, Skiba, \& Casey, 1985-1986; Gorscuh, 1983; Swaminathan et al., 2014) and we here focus on piecewise regression (Center et al., 1985-1986), as it is the most straightforward procedure for providing descriptive measures of the changes produced in the target behavior when introducing an intervention and due to the fact that it is used as a basis for using \nameref{sec:multilevel}. Moeyaert, Ugille, Ferron, Beretvas, and Van Den Noortgate (2014) offer a detailed list of how to specify piecewise/multilevel models for different single-case designs. \\[2ex]

Note that Intercept of the model will refer to the first baseline phase measurement, given that the variable used for representing trend is {\tt Time1} (centered at the first baseline phase measurement occasion), rather than {\tt Time} (centered right before the start of the experiment). The immediate change in level refers to the situation in which the {\tt Time2} variable takes the value of zero, that is in the first intervention phase measurement occasion. Thus, it can be seen that in order to improve interpretability the original {\tt Time} variable is not used. \\[2ex]

The results are expressed both in raw terms and in standardized terms. For this type of regression analysis, it was suggested that the standardization takes place by dividing the raw values of estimate of change by the square root ot the mean square error. 

The code to implement Piecewise regression is as follows:

<<echo=TRUE,eval=TRUE,tidy=FALSE,fig.height=4,fig.width=5, warning=FALSE>>=

# Create necessary objects
nsize <- nrow(Piecewise)
n_a <- 0
for (i in 1:nsize) 
  if (Piecewise$Phase[i]==0) n_a <- n_a + 1
n_b <- nsize - n_a
phaseA <- Piecewise$Score[1:n_a]
phaseB <- Piecewise$Score[(n_a+1):nsize]
time_A <- 1:n_a
time_B <- 1:n_b

# Unstandardized
# Piecewise regression
reg <- lm(Score ~ Time1 + Phase + Phase_time2, Piecewise)
summary(reg)
res <-  sqrt(sum((reg$residual-mean(reg$residual))^2)/df.residual(reg))



# Compute unstandardized effect sizes
Change_Level <- reg$coefficients[[3]]
Change_Slope <- reg$coefficients[[4]]


# Compute standardized effect sizes
Change_Level_s  = reg$coefficients[[3]]/res;
Change_Slope_s = reg$coefficients[[4]]/res;

# Standardize the results
baseline.scores_std <- rep(0,n_a)
intervention.scores_std <- rep(0,n_b)
for (i in 1:n_a) 
  baseline.scores_std[i] <- Piecewise$Score[i]/res
for (i in 1:n_b) 
  intervention.scores_std[i] <- Piecewise$Score[n_a+i]/res
scores_std <- rep(0,nsize)
for (i in 1:nsize) 
  scores_std[i] <- Piecewise$Score[i]/res
Piecewise_std <- cbind(Piecewise,scores_std)

# Plot data
indep <- 1:nsize
proj <- reg$coefficients[[1]]+reg$coefficients[[2]]*
+ Piecewise$Time1[n_a+1]

plot(indep,Piecewise$Score, xlim=c(indep[1],indep[nsize]), 
     xlab="Time1", ylab="Score", font.lab=2, xaxt="n")
axis(side=1, at=seq(1,nsize,1),labels=Piecewise$Time1, font=2)
lines(indep[1:n_a],Piecewise$Score[1:n_a],lty=2)
lines(indep[(n_a+1):length(Piecewise$Score)],
      Piecewise$Score[(n_a+1):length(Piecewise$Score)],lty=2)
abline (v=(n_a+0.5))
points(indep, Piecewise$Score, pch=24, bg="black")
title(main="Piecewise regression: Unstandardized effects")
lines(indep[1:(n_a+1)],c(reg$fitted[1:n_a],proj),col="red")
lines(indep[(n_a+1):length(Piecewise$Score)],
      reg$fitted[(n_a+1):length(Piecewise$Score)],col="red")
text(1,reg$coefficients[[1]],paste(round(reg$coefficients[[1]],
                                         digits=2)),col="blue")
if (n_a%%2 == 0) middle <- n_a/2
if (n_a%%2 == 1) middle <- (n_a+1)/2
lines(indep[middle:(middle+1)],rep(reg$fitted[middle],2),
      col="darkgreen")
lines(rep(indep[middle+1],2),reg$fitted[middle:(middle+1)],
      col="darkgreen",lwd=4)
text((middle+1.5),(reg$fitted[middle]+0.5*(reg$coefficients[[2]])),
     paste(round(reg$coefficients[[2]],digits=2)),col="darkgreen")

if (n_b%%2 == 0) middleB <- n_a + n_b/2
if (n_b%%2 == 1) middleB <- n_a + (n_b+1)/2
lines(indep[middleB:(middleB+1)],rep(reg$fitted[middleB],2),
      col="darkgreen")
lines(rep(indep[middleB+1],2),reg$fitted[middleB:(middleB+1)],
      col="darkgreen",lwd=4)
lines(indep[middleB:(middleB+1)],
      c(reg$fitted[middleB],(reg$fitted[middleB]+(reg$coefficients[[2]]))),
      col="green")
lines(rep(indep[middleB+1],2),
      c((reg$fitted[middleB]+ (reg$coefficients[[2]])),reg$fitted[middleB]),
      col="green",lwd=4)
text((middleB+1.5),
     (reg$fitted[middleB]+(reg$coefficients[[2]])),
     paste(round(reg$coefficients[[4]],digits=2)),col="red")
text((middleB+1.5),
     (reg$fitted[middleB]+0.5*(reg$coefficients[[4]])),
     paste(round((reg$coefficients[[4]]+reg$coefficients[[2]]),
                 digits=2)),col="darkgreen")

lines(rep(indep[n_a+1],2),c(proj,reg$fitted[n_a+1]),col="blue",lwd=4)
text((n_a+1.5),(proj+0.5*(reg$fitted[n_a+1]-proj)),
     paste(round(reg$coefficients[[3]],digits=2)),col="blue")

# Print results
paste("Piecewise unstandardized immediate treatment effect", round(Change_Level,2))
paste("Piecewise unstandardized change in slope", round(Change_Slope,2))

# Print results
paste("Piecewise standardized immediate treatment effect", round(Change_Level_s,2))
paste("Piecewise standardized change in slope", round(Change_Slope_s,2))
@

Finally, we do not present the results of model validation here, given that the main output of piecewise regression are the descriptive values depicted on the resulting plot. In that sense, the statistical significance of those values is not of interest and, therefore, normality, homogeneity of variance, and independence are not as criticial.

\newpage
\section{Polynomial regression}

Polynomial regression refers to linear regression models applied for modelling nonlinear relations between the variables. For some data patterns it is possible to model a curved relation linearly. Specifically, in some cases, it is possible to add the squared version of the predictor variable ($X^2$) to the predictor variable ($X^1$) itself in order to model a single curve or bend. Analogously, it is possible to add a version of the predictor variable elevated to the third power ($X^3$) in order to model a relation between X and Y that presents two curves or bends. Actually, a polynomial of order \emph{k} (i.e., $X^1 + X^2 + ... + X^k$) can be used to model as well as possible a relation with $k-1$ bends. However, such a model would not necessarily be conceptually meaningful, nor parsimonious. It has to be highlighted that in some cases, when the relation presents visually a single bend, it is possible that a quadratic model ($X + X^2$) does not provide adequate fit, but a cubic model ($X + X^2 + X^3$) does, although the second bend is beyond the range of observed values. It is also possible that such a nonlinear relation with a single bend cannot be modelled via polynomial regression or any other linear model.

\subsection{Linear model}
\label{sec:polynomial}

In this first section, we represent the scatterplot and compare a simple linear regression with a quadratic and a cubic model. Note that the way of entering the predictors is identical to hierarchical regression. This allows obtaining information about how good, in terms of R-squared, the linear fit is, how much does quadratic and cubic models improve that fit and whether the improvements are statistically signficant. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Define objects so that the code can be generally useful
Y <- PM_failure
X <- age

# Graphical representation
scatterplot(Y~X, reg.line=lm, smooth=TRUE, spread=FALSE, 
            boxplots=FALSE, span=0.8, cex=2,pch=20)

# Variables necessary for polynomial regression
X2 <- X^2
X3 <- X^3

# Carry out the regression analyses
reg.p1 <- lm(Y~X)
reg.p2 <- lm(Y~X+X2)
reg.p3 <- lm(Y~X+X2+X3)

summary(reg.p1)$r.squared
cat("Quadratic vs. Linear: Change in R-squared", "\n")
summary(reg.p2)$r.squared - summary(reg.p1)$r.squared
cat("Quadratic vs. Linear: Significance of the change in R-squared", "\n")
anova(reg.p2,reg.p1)$"Pr(>F)"[2]
cat("Cubic vs. Quadratic: Change in R-squared", "\n")
summary(reg.p3)$r.squared - summary(reg.p2)$r.squared
cat("Cubic vs. Quadratic: Significance of the change in R-squared", "\n")
anova(reg.p3,reg.p2)$"Pr(>F)"[2]
@

In the following we present the diagnostics plots in order to identify outlying and influential values. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
cookds <- cooks.distance(reg.p2)
cook <- sqrt(cookds)
hats <- hatvalues(reg.p2)
rstud <- rstudent(reg.p2)

plot (hats, rstud, type='n',
      ylab="Studentized eliminated residual", xlab="hat = leverage")
points(hats, rstud,cex=10*cook/max(cook))
abline(h=c(-2, 0, 2), lty=2)
abline(v=2*length(reg.p2$coef)/length(Y), lty=2,col="orange")
text(2*length(reg.p2$coef)/length(Y),max(rstud)+0.1,
     "2 parameters / values",col="orange",cex=0.6)
abline(v=3*length(reg.p2$coef)/length(Y), lty=2,col="red")
text(3*length(reg.p2$coef)/length(Y),max(rstud)+0.1,
     "3 parameters / values",col="red",cex=0.6)
dlimit <- 4/(length(Y))
title(main=paste("D cut-off =",round(dlimit,3), 
                 ". Red crosses mark values beyond: influential"))

# Cook's D: influential
highest <- 0
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  if (cookds[i] > highest) 
   {highest <- cookds[i] 
    value <- i }
}
valuesd <- rep(0,counterd)
counterd <- 0
for (i in 1: length(Y))
if (cookds[i] >= 4/(length(Y)))
{ counterd <- counterd + 1;
  valuesd[counterd] <- i}
points(hats[valuesd],rstud[valuesd],col="red",pch=3,cex=2)
influ <- c(Y[valuesd],age[valuesd])
dim(influ) <- c(length(valuesd),2)
colnames(influ) <- c("Y","X")

# Leverage
counterh <- 0
for (i in 1: length(Y))
if (hats[i] >= 3*length(reg.p2$coef)/length(Y))
  counterh <- counterh + 1
valuesh <- rep(0,counterh)
counterh <- 0
for (i in 1: length(Y))
if (hats[i] >= 3*length(reg.p2$coef)/length(Y))
  {counterh <- counterh + 1;
   valuesh[counterh] <- i}

# Outlying Y
highest_r <- outlierTest(reg.p2)[[1]]
for (i in 1: length(X))
if (rstud[i] == highest_r) value_r <- i
yaxis <- rstud[value_r]
xaxis <- hats[value_r]
arrows(xaxis+0.075,yaxis,xaxis+0.002,yaxis,length = 0.5,col="blue")
text(xaxis+0.075,yaxis,"Outlying Y",col="blue")

@

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print info
cat("Outlying Y test","\n")
outlierTest(reg.p2)
cat("Influential data points","\n")
influ
cat("High-leverage X data points","\n")
X[valuesh]
@

Next, we mark, on the original scatterplot, the values singled out by the diagnostic statistics in order to identfy them better and to think about the influence they might have on the data.  

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
plot(Y~X, cex=2,pch=20)
xs <- length(influ)/2
for (i in 1:xs)
  points(influ[[i,2]],influ[[i,1]],pch=3,col="red",cex=4)
points(X[value_r],Y[value_r],pch=0,col="blue",cex=4)
title(main= "Cross = influential. Square = largest residual. Triange = high leverage")
for (j in 1:length(valuesh))
  points(X[valuesh[j]],Y[valuesh[j]],pch=2,col="orange",cex=4)
@

Now we re-run the regression analyses based on the linear and the quadratic models without the influential data point(s) identified via Cook's distance (D). We present the improvement of the fit of the models to the data in absence of these influential data points. Nevertheless, we do not recommend following such a practices indiscriminately: it is not (professionally and statistically) ethical to remove problematic values only to increase R-squared and report better results. On the contrary, any univariate or bivariate outlier has be assessed not only statistically (boxplots, diagnostics) but also conceptually and according to the knowledge of the individual that produced this value. In some cases, it might be justified to check the results of the analysis without some value(s), but only aftter first reporting the results with all the data.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Perform the analysis without the influential values
newX <- rep(0,length(X)-length(valuesd))
newY <- rep(0,length(Y)-length(valuesd))
j <- 0
for (i in 1:length(X))
  if(!i %in% valuesd) 
    {j <- j + 1;
     newX[j] <- X[i];
     newY[j] <- Y[i]
  }
reg.p1new <- lm(newY ~ newX)
paste("Improvement in linear fit", round(summary(reg.p1new)$r.squared,2), 
      "vs.", round(summary(reg.p1)$r.squared,2))

newX2 <- newX^2
reg.p2new <- lm(newY ~ newX + newX2)
paste("Improvement in quadratic fit", round(summary(reg.p2new)$r.squared,2), 
      "vs.", round(summary(reg.p2)$r.squared,2))
@

Finally, we represent the scatterplot without the influential data points.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=  
scatterplot(newY ~ newX, reg.line=lm, smooth=TRUE, spread=FALSE, 
            boxplots=FALSE, span=0.8, cex=2,pch=20)
            @

\newpage
\subsection{Polynomial regression as part of using B-splines}
\label{sec:bsplines}

Another way of using linear regression analysis for specifying models that might fit well to data presenting a nonlinear pattern is to use B-splines. This method involves fitting separately several regression lines (usually, straight or quadratic), each of which is fitted to a different part of the data. Where to split the scatterplot (i.e., where to place the \enquote{knots} tying the different regression lines) is a decision that can be made by the researcher according to a substantive criterion (e.g., existance of cut-off points in the predictor variable). Another option is to leave the knot to be at the middle / median point of the X variable, as is specified by default. \\[2ex]

We have included splines in the present section, given that they are based on polynomial regression; specifically, several polynomial regressions, each performed on a different portion of the data. In order to apply the splines method in \textbf{R}, a specific package is necessary. 

<<echo=TRUE, eval=FALSE,warning=FALSE>>=
# May be necessary to install the package "splines" (once)
install.packages("splines")
@

With the following code we compare the fit of four models: simple linear regression (i.e., first-order polynomial with zero knots, as only one regression analysis is applied to all the data points), a B-splines model with one knot and two linear models connected, a quadratic polynomial model (i.e., second-order and with zero knots), and B-splines model with one knot and two quadratic models connected. Note that for the latter two models, it is not necessary that the user defines where the knots will be placed.   

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Load the package
require(splines)

# To make the following code universally applicable for any X and Y:
Y <- PM_failure
X <- age

# Define the parameters of the B-splines
# degree of the polynomial (1:linear, 2:quadratic) 
poly <- c(1,1,2,2) 
# Given that the number of (automatically chosen) knots = df-poly
# the degrees of freedom are specified according to the number of knots desired
# and the order of the polynomial for the model
knots.desired <- c(0,1,0,1)
degfree <- poly+knots.desired

# Construct table for pseudo R-squared values
R2.table <- 1:(3*length(poly))
dim(R2.table) <- c(length(poly),3)
colnames(R2.table) <- c("Knots", "Polynomial", "R-squared")
# Enter the column with the number of knots
R2.table[,1] <- knots.desired
R2.table[,2] <- poly

# Create the basis for the plot
if (length(poly)%%2==0) rows <- length(poly)/2 else rows <- (length(poly)+1)/2
par(mfrow=c(rows,2))
j <- order(X)

# Vector containing p values comparing models
better.prev <- 1:length(poly)
better.prev[1] <- NA

# Carry out the regression analyses and plot the fitted line
for (i in 1:length(poly))
{
  # Perform regression analysis
  reg.spline <- lm(Y ~ bs(X,degree=poly[i],df=degfree[i]))
  # Represent the data points
  plot(X,Y,pch = 19)
  # Print plot title with the characteristics of the model
  title(main=paste("Polynomial=",poly[i],"Knots=",R2.table[i,1]),cex.main=0.8)
  # Draw the regression line
  lines(X[j], fitted(reg.spline)[j],col="red",lwd=3)
  # Save the R-squared value in the tamble
  R2.table[i,3] <- round(summary(reg.spline)$r.squared,2)
  # Compare models statistically
  if (i > 1) reg.prev <- lm(Y ~ bs(X,degree=poly[i-1],df=degfree[i-1]))
  if (i > 1) better.prev[i] <- anova(reg.spline,reg.prev)$"Pr(>F)"[2]
}
@

In the following we present the summary information about the fit of the four models, as well as a
comparison between each model and previous one. Note that these comparisons are possible only when the models have a different number of degrees of freedom. Except for the simple linear regression model, the other three models show very similar fit. The choice between them should be based not only on statistical criteria, but also on substative ones: is there a theoretical explanation or expectation about the type of relation between the variables?

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print pseudo R-squared values and result of comparisons
R2.table
paste("p values comparing model with previous" , round(better.prev,2))
@

Although it is not recommended in regression in general (due to the loss of precision; i.e., wider confidence intervals) and in the case of picking a model only on the basis of the data at hand (due to the dependency on these data), it is possible to predict values of the dependent variable for values of the predictor taht were not actually observed in the sample. In the following, we illustrate a prediction for individuals, who are up to \emph{k} years older than the oldest individual of the actual sample. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
k <- 10
reg.spline <- lm(Y ~ bs(X,degree=poly[4],df=degfree[4]))

# How to predict
newX <-  seq(max(X)+1,max(X)+k,by=1)
B <- data.frame(X=newX)
newY <- predict(reg.spline,newdata=B)

# Plot actual and prediceted
plot(X,Y,pch = 19,xlim=c(min(X),(max(X)+k)),ylim=c(min(Y,newY),c(max(Y,newY))))
title(main=paste("Polynomial=",poly[4],"Knots=",R2.table[4,1]),cex.main=0.8)
lines(X[j],fitted(reg.spline)[j])
lines(newX,newY,lwd=2,col="red")
@

\newpage
\subsection{Polynomial regression as part of Local regression (LOESS)}
\label{sec:loess}

Another option for modelling nonlinear relations without the need to specify mathematically a nonlinear model is provided by local regression or, \enquote{locally weighted polynomial regression} (LOESS). Actually, with this alternative, it is not necessary to specify a priori any particular model at all. In that sense, the result is purely data driven and may not easily be interpretable in theoretical terms.\\[2ex] 

The main characteristics of LOESS are the following (see Jacoby, 2000, for more details):
\begin{itemize}
\item several polynomial regressions are fitted iteratively to different fractions of the data
\item the user usually chooses between a linear or a quadratic regression being fit to every fraction of data
\item the amount of data included in each fraction is also determined by the user (smaller fractions lead to greater sensitivity and potentially better fit, whereas larger fractions lead to more smoothed lines and less importance assigned to values far away from the general pattern); it is possible to compare the fit of models based on different fractions of data
\item most data points are used in more than regression analysis as they belong to more than one fraction, unlike the fit provided using B-splines, where every data point only belongs to the fraction of data before (in terms of the predictor) the knot or after the knot
\item the data points that are at the lower and upper extreme of each fraction are assigned smaller weights and they have less influence on the results of the regression analysis fitted to this fraction
\item robust estimation can be used to give less weight to observations with high residuals in order to control for outliers, but it may lead to nonnormal residuals (which would invalidate the inferential information: \emph{p} values and confidence intervals)
\item LOESS can also be used to explore whether there is any pattern in the residuals
\item just like with B-splines (and also with hierarchical regression and multilevel models) it is possible to compare nested models statistically in order to explore whether a more complex model (e.g., using quadratic vs. linear local regressions) is justified 
\item The model cannot easily be expressed as a regression equation
\item Given that the emphasis is put on the fit of the model to the data, rather than on conceptual meaningfulness, LOESS is most easily used with a single predictor and becomes less visually clear with several predictors
\end{itemize}


<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# To make the following code universally applicable for any X and Y:
Y <- PM_failure
X <- age

# Define the parameters of the LOESS
# "span" represents the fraction of data used in each local regression
alpha <- c(0.60,0.60,0.80,0.80) 
# "lambda" represents the order of the polynomial 
# (e.g., whether each local regression is linear or quadratic)
lambda <- c(1,2,1,2) 

# Construct table for pseudo R-squared values
R2.table <- 1:(3*length(lambda))
dim(R2.table) <- c(length(lambda),3)
colnames(R2.table) <- c("Alpha (span)", "Lambda (polynomial)", "R-squared")
# Enter the information about the fraction of data used
R2.table[,1] <- alpha
# Enter the information about the polynomial
for (i in 1:length(lambda))
  if (lambda[i] == 1) R2.table[i,2] <- "1 (linear)" else R2.table[i,2] <- "2 (quadratic)" 

# Create the basis for the plot
if (length(lambda)%%2==0) rows <- length(lambda)/2 else rows <- (length(lambda)+1)/2
par(mfrow=c(rows,2))
j <- order(X)

# Vector containing p values comparing models
better.prev <- 1:length(lambda)
better.prev[1] <- NA

# Carry out the LOESS's and plot the fitted line
for (i in 1:length(lambda))
{
  # Carry out the LOESS
  reg.loess <- loess(Y~X, span=alpha[i],degree=lambda[i])
  # Compute the total sum of squares
  ss.total<- sum(scale(Y, center=TRUE, scale=FALSE)^2)
  # Compute the residual sum of squares
  ss.resid<- sum(residuals(reg.loess)^2)
  # Compute the pseudo R-squared value
  R2.table[i,3] <- round((1-ss.resid/ss.total),2)
  # Plot the data points
  plot(Y~X, pch=19,cex=1)
  # Print the plot title with the information about the regression model
  title(main=paste("alpha=",alpha[i],"polynomial=", R2.table[i,2]),cex.main=0.8)
  # Represent the regression line
  lines(X[j],reg.loess$fitted[j],col="red",lwd=3)
  # Compare the regression model with the previous one
  if (i > 1) 
  {
    reg.prev <- loess(Y~X, span=alpha[i-1],degree=lambda[i-1])
    better.prev[i] <- anova(reg.loess,reg.prev)$"Pr(>F)"[2]
  }  
}
@

In the following we present the summary information about the fit of the four models, as well as a
comparison between each model and previous one. Note that in this cases the first model is only compared to the second one and the fourth to the third one, whereas the second model is compared to both the first and the third, and the third model is comapred both to the second and the third. All four models provide very similar fit and are not statistically significantly different from each other. In that sense, for these data, it does not seem to be critical whether the fraction of data used is 60\% or 80\% and whether each local regression is linear or quadratic. Given that the subjectivity of the choice of a fraction especially is a problem, the current results are reassuring. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print pseudo R-squared values and the result of comparing models
R2.table
paste("p values comparing model with previous" , round(better.prev,2))
@

\newpage
\section{Analysis of covariance via hierarchical regression}
\label{sec:ancova}

Analysis of covariance (ANCOVA) is way of controlling statistically for extraneous (potentially confounding) variables, referred to as\enquote{covariables}. In that sense, ANCOVA represent a strategy that is somewhat inferior to controlling for extraneous variables through the design (e.g., eliminating or maintaining constant certain variables, matching individuals from different groups on a given variable, creating blocks of individuals in all groups how are similar in a relevant variable, balancing the groups to be compared regarding a variable, counterbalancing the order of presentation of different conditions in repeated measures designs or assigning individuals at random to different conditions). However statistical control as performed by ANCOVA or hierarchical regression is preferable tojust comparing groups on a potentially confounding variable and using (incorrectly) a result that is not statistically significant as a proof of equivalence (see Tryon, 2001, for a formal statistical test of equivalence). In ANCOVA or hierarchical regression an extraneous variable can be controlled for regardless of whether its effect on the dependent variable is statistically significant. This is important, given that statistical significance is not only related to the size of this effect, but also to sample size and it is widely known to small samples can lead to insufficient statistical power (while large samples can lead to labelling as statistically significant effects that are too small to be practically relevant).

\subsection{Carrying out the main analysis}

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=6, fig.width=6>>=
# To make the following code universally applicable for any X and Y:
X <- smoking
Y <- PM_failure

# Create a dummy variable for "gender" so that point-biserial correlation can be computed
gender.d <- rep(0,length(Y))
for (i in 1:length(Y))
  if(gender[i] == "female") gender.d[i] <- 1

# Check which potential covariables are linearly related to the dependent variable
potential <- 4
cors <- data.frame ("Correlation"=1:potential,p.value=1:potential)
rownames(cors) <- c("age", "IQ", "depression", "gender")
cors[1,] <- c(cor.test(age,Y)$estimate[[1]],cor.test(age,Y)$p.value)
cors[2,] <- c(cor.test(IQ,Y)$estimate[[1]],cor.test(IQ,Y)$p.value)
cors[3,] <- c(cor.test(depression,Y)$estimate[[1]],cor.test(depression,Y)$p.value)
cors[4,] <- c(cor.test(gender.d,Y)$estimate[[1]],cor.test(gender.d,Y)$p.value)
cors

# Print variable with sig. relation to Y
rownames(cors)[cors[,2]<= 0.05]

# Use as a covariable the variable previously identified
Co <- age

# Step1: Model with covariable only
reg.a1 <- lm(Y ~ Co)
# R-squared for the covariable
summary(reg.a1)$r.squared

# Step2: ANCOVA - Model with covariable and main independent variable
reg.a2 <- lm(Y ~ Co + X)
#Model R-squared (eta-squared)
summary(reg.a2)$r.squared
# Semi-partial R-squared (eta-squared) for the main independent variable
# Unique contribution
(summary(reg.a2)$r.squared - summary(reg.a1)$r.squared)
# Partial R-squared (eta-squared) for the main independent variable
(summary(reg.a2)$r.squared - summary(reg.a1)$r.squared)/(1-summary(reg.a1)$r.squared)
# Statistical significance of the effect of the main independent variable
anova(reg.a1,reg.a2)$"Pr(>F)"[2]

# Model with main independent variable only:
# necessary only for illustrating whole the partial R-squared for the covariable can be obtained
reg.a3 <- lm(Y ~ X)
# Partial R-squared (eta-squared) for the covariable
(summary(reg.a2)$r.squared - summary(reg.a3)$r.squared)/(1-summary(reg.a3)$r.squared)

# Obtain the predicted values
predicting <- lm(Y~Co+X)$fitted.values

# Plot the relation between covariable and dependent variable
plot(Co,Y,xlab="Covariable",ylab="Dependent variable")
title(main=list("Lines for each group defined by the independent variable",cex=0.6))
colors <- c("red","green","blue","orange","black")
# Add separate but parallel regression lines for each group
# defined by the categorical independent variable
for (i in 1:length(table(X)))
{ 
  # Add different colors to the data points
  points(Co[X==names(table(X))[i]],Y[X==names(table(X))[i]],col=colors[i],pch=20);
  # Add different lines with different colors
  lines(Co[X==names(table(X))[i]],predicting[X==names(table(X))[i]],col=colors[i])
}
@

<<echo=FALSE, eval=TRUE,warning=FALSE>>=
p <- anova(reg.a1,reg.a2)$"Pr(>F)"[2]
partial.eta <- (summary(reg.a2)$r.squared - summary(reg.a1)$r.squared)/(1-summary(reg.a1)$r.squared)
total.eta <- summary(reg.a2)$r.squared
@

In the following image we provide the output of an ANCOVA carried out using \textbf{IBM SPSS}\textsuperscript{\textregistered}. It can be seen that the \emph{p} value for the main predictor, once the effect of the covariable has been controlled for is the same: \Sexpr{p}. Moreover, the partial eta-squared ($\eta_{p}^{2}$) is also the same: \Sexpr{partial.eta}. In that sense, the results of ANCOVA and hierarchical regression are equivalent. Finally, note that \textbf{IBM SPSS}\textsuperscript{\textregistered} also provides the R-squared for the whole model (i.e., the amount of variability of dependent variable accounted for by the covariable and the main predictor) and it is equal to \Sexpr{total.eta}. 

\begin{figure}[H]
\includegraphics[scale=.85]{spss_ancova1}
\centering
\end{figure}

\newpage
\subsection{Checking the assumptions of ANCOVA}

ANCOVA has all the assumptions of analysis of variance, plus several additional ones, due to the fact that another kind of variable is involved in the analysis: the covariable. One of the commonly checked assumptions specific to ANCOVA, but not  to ANOVA is the homogeneity of regression slopes. This assumption states that the linear relation between covariable and the dependent variable is the same for all groups defined by the categories of the main predictor. Another way of understanding this assumption is that there is no (statistically significant) interaction between the covariable and the independent variable in their effects on the dependent variable. This assumption can be checked via hierarchical regression, introducing the interaction of the covariable and the main predictor in a step after the main effects of each of these variables has been included. As is the case with most assumptions, a nonsignificant result would be interpreted as the assumption being met.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Check assumptions: homogeneity of slopes
reg.a4 <- lm(Y ~ Co * X)
# Statistical significance of the interaction (if n.s. --> slopes are homogeneous)
anova(reg.a2,reg.a4)$"Pr(>F)"[2]
# Partial R-squared (eta-squared) for the interaction
(summary(reg.a4)$r.squared - summary(reg.a2)$r.squared)/(1-summary(reg.a2)$r.squared)
@

The numerical verification of the assumption can be complemented by a visual representation of the regression lines for {\tt Y} as function of {\tt Co}, fitted sepaetely for each group defined by {\tt X}. Note that this is not the same as the graph provided previously in which the same regression equation was used for all categories. Homogeneous slopes should be parallel. In contrast, an interaction is detected in the same way as in two-way ANOVA: with lines crossing or at least approaching each other for the range of values actually observed. In the current case, not all the lines are perfectly linear, but the amount of approximation between the lines (and the number of data points) is not sufficient for reaching statistical significance. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=4, fig.width=4>>=
# Represent the separate regression slopes
plot(Co,Y)
title(main=list("Separate regression lines fitted to each group",cex=0.6))
colors <- c("red","green","blue","orange","pink","brown")
for (i in 1:length(table(X)))
{
  lines(sort(Co[X==names(table(X))[i]]),
        lm(Y[X==names(table(X))[i]]~Co[X==names(table(X))[i]])$fitted[order(Co[X==names(table(X))[i]])],
        col=colors[i])
  points(Co[X==names(table(X))[i]],Y[X==names(table(X))[i]],col=colors[i],pch=20)
}
@

<<echo=FALSE, eval=TRUE,warning=FALSE>>=
p.interact <- anova(reg.a2,reg.a4)$"Pr(>F)"[2]
@

In \textbf{IBM SPSS}\textsuperscript{\textregistered} this assumption can also be checked with ANCOVA iteself, specifying a model that includes not only the main effects of {\tt X} and {\tt Co}, but also their interaction. After this verification is performed and if the interaction is not statistically significant, the actual ANCOVA results are ob tained only for the main effects, as shown in the previous \textbf{IBM SPSS}\textsuperscript{\textregistered} output. The result obtained is identical to the one shown with hierarchical regression in \textbf{R}:  \Sexp{p.interact}.

\begin{figure}[H]
\includegraphics[scale=.85]{spss_ancova2}
\centering
\end{figure}

\newpage
Normality is an assumption also present in ANOVA. In this case it refers to all quantitative variables: the dependent variable and the covariable. Just like in ANOVA, normality has to be checked for each group (technically, population) defined by the categories of the independent variable. Once again, a result that is not statistically significant would imply that the assumption is met. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=7, fig.width=5>>=
# Normality of covariable and DV for each level of the IV
tapply(Co, X, shapiro.test)   
tapply(Y, X, shapiro.test)   

# Represents Y and Co via histograms with superimposed normal curve
par(mfrow=c(length(table(X)),2))
for (i in 1:length(table(X)))
{
  hist(Co[X==names(table(X))[i]],prob=TRUE,xlab="Residuals", 
       main=paste("Covariable distribution for X=",names(table(X))[i]))
  curve(dnorm(x, mean=mean(Co[X==names(table(X))[i]]),sd=sd(Co[X==names(table(X))[i]])),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")
  
  hist(Y[X==names(table(X))[i]],prob=TRUE,xlab="Residuals",
       main=paste("Y distribution for X=",names(table(X))[i]))
  curve(dnorm(x, mean=mean(Y[X==names(table(X))[i]]),sd=sd(Y[X==names(table(X))[i]])),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")
}
@

\newpage
Given that we have implemented ANCOVA via hierarchical regression, the homoskedasticity of the residuals is also necessary. We explore it both visually and statistically with the Breusch-Pagan test. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=4, fig.width=4>>=
# Graphical representation
plot(reg.a2$fitted, reg.a2$residuals,xlab="Fitted",ylab="Residual")
title(main="Pattern in the residuals?")
# Represent a LOESS line 
lines(sort(reg.a2$fitted),
      loess(reg.a2$residuals~reg.a2$fitted, 
            span=0.8, degree=1)$fitted[order(reg.a2$fitted)],col="red")

# Homogeneity of variance of the residuals: test
require(lmtest)
bptest(reg.a2)
@

\newpage
\subsection{ANCOVA: Adjusting for group means}

ANCOVA can be understood not only as hierarchical regression in which the covariable is entered first and the main predictor next, but also as a way of adjusting the means of the dependent variable for each group defined by the independent variable. This adjustment is performed according to the difference in the mean levels of the covariable and according to the slope coefficient of the regression between the covariable (as predictor) and the dependent variable. With the plot we represent the idea that without taking the covariable into account the groups defined by {\tt X} are compared in terms of their {\tt Y} for different values of {\tt Co}, but after adjusting the means, the groups are compared at a common value of {\tt Co}. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Original means for smoker, ex, and non, respectively
tapply(Y,X,mean)

# Adjusted means  for smoker, ex, and non, respectively
tabla <- 1:(length(table(X)))
dim(tabla) <- c(1,length(table(X)))
colnames(tabla) <- names(table(X))
for (i in 1:length(table(X)))
  tabla[i] <- round(mean(Y[X==names(table(X))[i]]) - 
                      reg.a1$coefficients[[2]]*(mean(Co[X==names(table(X))[i]])-mean(Co)),2)

# Plot Covariable vs. Dependent variable, with colors defined by independent variable
plot(Co,Y)
for (i in 1:length(table(X)))
{ 
  points(Co[X==names(table(X))[i]],
         Y[X==names(table(X))[i]],col=colors[i],pch=20);
  lines(Co[X==names(table(X))[i]],
        predicting[X==names(table(X))[i]],col=colors[i]);
  points(mean(Co[X==names(table(X))[i]]),
         mean(Y[X==names(table(X))[i]]),pch=8,cex=2,col=colors[i]);
  points(mean(Co),tabla[i],pch=23)
  }
xval <- rep(mean(Co),length(tabla))
lines(xval,tabla,lty="dashed")
points(xval,tabla,pch=23)
title(main="Colored stars = original means at mean of the covariable per group. 
      \n Black diamonds = adjusted means at the overall mean of the covariable.",
      cex.main=0.8)
@

\newpage
\subsection{Eta-squared and R-squared and their partial and semi-partial versions}

In current section we are represeting graphically and numerically the amount of variaility of the dependent variable that is: (a) explained separately by the independent variable (R-squared or eta-squared for {\tt X}) or by the covariable (R-squared for {\tt Y}); (b) explained by the independent variable as a unique contribution after the covariable is already included in the model (semi-partial R-squared) or, which is less logical, explained by the covariable as a unique contribution after the independent variable is already included in the model (semi-partial R-squared); (c) explained by the independent variable as a unique contribution after the covariable is already included in the model and expressed as a proportion of the variability not explained by the covariable (partial R-squared) or, which is less logical, explained by the covariable as a unique contribution after the independent variable is already included in the model and expressed as a proportion of the variability not explained by the independent variable (partial R-squared).\\[2ex]

In the Venn diagram only the dependent variable areas explained and unexplained are represented, considering that the covariable is entered first in the model (accounting for the blue area) and representing the unique contribution of the independent variable (semi-partial R-squared) as the green area. Note that the colored areas are not proportional to the R-squared values. For representing the Venn diagram it is necessary to install the \textbf{VennDiagram} package for \textbf{R}. 

<<echo=TRUE, eval=FALSE,warning=FALSE>>=
install.packages("VennDiagram")
@

Visual representation:

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=6, fig.width=6>>=
area1 <- 1
area2 <- 1
area3 <- 1

# Define the amount of intersection (variability shared)
n12 <-round(summary(reg.a3)$r.squared*1,2)
n23 <- round(summary(reg.a1)$r.squared*1,2)
n123 <- n23 - round((summary(reg.a2)$r.squared-summary(reg.a3)$r.squared)*1,2)
reg.a5 <- lm(Co~X)
maximal <- min((1 - n12),(1 - n23))
minimal <- n123
n13 <- mean(c(minimal,maximal)) 
# Altern: n13 <- round((summary(reg.a5)$r.squared*(var(Co)/var(Y)))*1,2)

# Define the titles and graphical parameters
categ <- c("Independent","Dependent","Covariable")
colors <- c("green","red","blue")
filling <- c("green","red","blue")
transp <- c(1,0,1)
lcolors <- c("green","white","black","blue","orange","white","blue")

# load the R package
require(VennDiagram)
# Draw the Venn Diagram
draw.triple.venn(area1, area2, area3, n12, n23, n13, n123, 
                 category=categ,col=colors,sigdigits=2,
                 print.mode="raw",fil=filling,
                 alpha=transp,label.col=lcolors)
@

\newpage
Numerical representation of R-squared, semi-partial R-squared (i.e., unique contribution of each predictor of {\tt Y}) and partial R-squared (i.e., unique contribution of each predictor of {\tt Y} as a proportion of the {\tt Y} variability not explained by the remaining predictors). 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
#Create table with R-squared values
R2.table <- 1:6
dim(R2.table) <- c(2,3)
colnames(R2.table) <- c("R-squared", "semi-partial", "partial")
rownames(R2.table) <- c("Covariable", "Independent variable")

R2.table[1,1] <- round(summary(lm(Y~Co))$r.squared,2)
R2.table[1,2] <- round(summary(lm(Y~Co+X))$r.squared-summary(lm(Y~X))$r.squared,2)
R2.table[1,3] <- round(R2.table[1,2]/(1-summary(lm(Y~X))$r.squared),2)
R2.table[2,1] <- round(summary(lm(Y~X))$r.squared,2)
R2.table[2,2] <- round(summary(lm(Y~Co+X))$r.squared-summary(lm(Y~Co))$r.squared,2)
R2.table[2,3] <- round(R2.table[2,2]/(1-summary(lm(Y~Co))$r.squared),2)
R2.table
@



\newpage
\section{Multilevel models}
\label{sec:multilevel}

Multilevel models (and more specifically hierarchical linear models) allow dealing with data that present a hierarchical (nested) structure, in order to taking nesting into account and to model the influence of factors at different levels. Another reason for applying multilevel models is to obtain more efficient estimates of the parameters and also to obtain accurate standard error estimates so that Type I error rates are controlled. In the following example, we deal with a longitudinal study in which there are several measurements available for each individual: these measurements represent the Level 1 data. These data are nested into the individuals, who represent the Level 2 data. Therefore, the target behavior (here cognitive function) can vary both at the intra-individual level (changes within each person with time) and at the inter-individual or between-subjects level (differences between the individuals in terms of their initial level or in their evolution). Before we focus on the application of multilevel models for modelling growth curves in studies about aging, it is necessary to make clear some distinctions. First, in the current example the emphasis is put on longitudinal studies that have time as main expalanatory factor, although the characteristics of the individuals are also taken into account. In that sense, such a design is different from single-case designs for which multilevel models are also applicable (e.g., Ferron, Bell, Hess, Rendina-Gobioff, & Hibbard, 2009; Van Den Noortgate & Onghena, 2003). In single-case designs, whose analysis with regression-based models is discussed in section \ref{sec:single-case}, measurements are also taken repeatedly, but from an experimental perspective, given that the researcher manipulates deliberately the conditions (Barlow, Nock, & Hersen, 2009). Second, the analysis of longitudinal data is also possible not only using regression models or, more specifically, multilevel models (Shaw \$ Liang, 2012) but also from a perspective that does not take nesting into account (Newsom, 2012). Third, latent growth curve modelling is different from the analytical option presented here, as structure equation modelling is usually employed (see Rovine \& Liu, 2012 for a theoretical introduction and Finkel, Reynolds, McArdle, Gatz, \& Pedersen, 2003 and Zahodne et al., 2012 for applications).\\[2ex]

When applying a multilevel model it is possible to model the change in the variable of interest (here cognitive function) in all the participants in the sample considered as a whole, that is, on average. It is also possible to explore and quantify the amount of heterogeneity among the individuals. In that sense, every individual can have his/her growth curve. Using multilevel analysis, it is possible to account for the intra-individual or within-subject changes focusing on factors that are related to each person and change in time. In what follows, we will refer to these factors and \enquote{time-variant} or \enquote{Level 1} factors. These factors include where the person live, the marital status (susceptible to changes in any longidutinal study and specifically when working with elderly participants) or the amount of social activity (Hedecker \& Gibbons, 2006). Therefore, we will not only be describing how each person evolves in time, but we will also quantify what proportion of the changes in time are related to changes in other factors.Multilevel models also allow accounting for the inter-individual variability via factors that do not change in time (hereinafter, \enquote{time-constant} predictors), such as gender, main occupation previous to the retirement or education level (although it is possible that elderly people attend courses, it is less likely that they pursue achieven formal education degrees). \\[2ex]  

Note that the distinction between the two types of factors can be fuzzy and is related to the specific individuals participating in the study and their particular evolution. For instance, when studying aging processes, the place of residence (in the community or in special housing) can be a time-constant factor distinguishing between individuals (and used to explain differences between them in the target behavior) or can be a time-variant predictor, if the participants in the sample change their place of residence during the study. 

\subsection{Modelling options}

Formally, it is possible to specify different types of models according to the type of predictors they contain. The simplest model is the unconditional (or null) model, in which only the grand mean of all measurements is estimated. For instance, at Level 1: $Y_{ij}=\beta_{0j}+\epsilon_{ij}$, where \emph{i} denotes the measurement occasion, \emph{j} denotes the individual and $\beta_{0j}$ is the average of the dependent variable \emph{Y} for individual \emph{j} across all measurement occasions, and $\epsilon_{ij}$ is the intra-individual variability for each person. It is possible to decompose $\beta_{0j}$ into a fixed part ($\gamma_{00}$ representing the average for all individuals across all measurement occasions) and a random part ($u_{0j}$ representing the inter-individual variability in the average or intercept). This decomposition takes place in the Level 2 expression representing the variability about the grand mean: $\beta_{0j} = \gamma_{00} + u_{0j}$. Therefore, the null model decomposes the variability about the grand mean ($\gamma_{00}$) into inter-individual (Level 2: $u_{0j}$) and intra-individual (Level 1: $\epsilon_{ij}$).  \\[2ex]

In a following step, a model including time is specified at Level 1 as: $Y_{ij}=\beta_{0j}+\beta_{1j}Time_{ij}+\epsilon_{ij}$. If a random coefficients model is considered appropriate, the beta parameters can be decomposed in the Level 2 expressions: $\beta_{0j} = \gamma_{00} + u_{0j}$ and $\beta_{1j} = \gamma_{10} + u_{1j}$, where $\gamma_{1j}$ is the average slope for all participants and $u_{1j}$ is the inter-individual variability in slopes. \\[2ex]

Multilevel analysis entails carrying out the Level 1 regression for each of the individuals (i.e., Level 2 units) separately. However, the analysis actually combines the expressions at the different levels in a single model: $Y_{ij}=\gamma_{0j}+\gamma_{1j}Time_{ij}+(u_{0j} + u_{1j}Time_{ij} + \epsilon_{ij})$. In this expression $\gamma_{0j}$ refers to the intercept or average value of \emph{Y} for all individuals when $Time=0$, whereas $\gamma_{1j}$ denotes the average estimate of the evolution of \emph{Y} with time. Both of these terms represent the fixed part of the expression, whereas the terms withing parentheses represent the random part: $u_{0j}$ is the inter-individual (Level 2) variability in the intercept, $u_{1j}$ is the inter-individual (Level 2) variability in the slope (relation between $Time$ and $Y$), and $\epsilon_{ij}$ is the intra-individual (Level 1) variability not explained by the linear relation between $Time$ and $Y$.\\[2ex]

A subsequent step in the analysis can try to account for the variability in the intercept using time-constant (Level 2) predictors, just as the occupation previous to retirement. This is one of the options tested later in this section and it can be expressed as in the Level 2 equations as: $\beta_{0j} = \gamma_{00} + \gamma_{01}Occupation_{j} + u_{0j}$  and $\beta_{1j} = \gamma_{10} + \gamma_{11}Occupation_{j} + u_{1j}$ leading to the following combined model:  $Y_{ij}=\gamma_{00}+\gamma_{10}Time_{ij} + \gamma_{01}Occupation_{j} + \gamma_{11}Time_{ij}Occupation_{j} + (u_{0j} + u_{1j}Time_{ij} + \epsilon_{ij})$. It can be seen that only fixed terms, but not random terms, were added to the model: the effect of \enquote{Occupation} and the interaction between \emph{Time} and \enquote{Occupation}, which represents are cross-levels intervation. The introduction of \enquote{Occupation} is expected to be useful for explaining inter-individual variability in the intercept ($u_{0j}$), whereas the interaction term can be used to explain the variability in the slopes ($u_{1j}$).\\[2ex]

The model including only $Time$ as predictor can also be extended with a time-variant (Level 1) factor, such as \enquote{Social activity}. Such a model, tested later in the section, has the following aspect at Level 1: $Y_{ij}=\beta_{0j}+\beta_{1j}Time_{ij} + \beta_{2j}Activity_{ij} + \epsilon_{ij})$. If onyl $Time$ is treated as a random effect, the combined model will be: $Y_{ij}=\gamma_{00}+\gamma_{10}Time_{ij} + \gamma_{20}Activity_{ij} + (u_{0j} + u_{1j}Time_{ij} +  \epsilon_{ij})$.

\subsection{Interpreting the variance}

The interpreation of the variance changes according to the model. For the unconditional model, inter-individual variability represents the difference between the average for each individual and the grand mean, whereas intra-individual variability is the difference between each value obtained at the different measurement occasions and the average for the individual. In the model including $Time$ as a random effect at Level 1, a quantification of the inter-individual difference in the growth lines is also available. In contrast to the null model, the inter-individual variability about the intercept is now the variability about the estimated average of $Y$ when $Time=0$. The residual intra-individual variability would in this case be the part of intra-individual variation not explained by $Time$.\\[2ex]

Moreover, with the inclusion of Level 2 predictors such as \enquote{Occupation}, the inter-individual variability can also be considered residual, given that it is the variability not explained by this predictor. Analogously, introducing Level 1 predictors (e.g., \enquote{Social activity}) aims to explain: (a) the intra-individual variability in the measurements taken at different moments
and not already explained (linearly) by $Time$ and (b) the variability in the slopes. 

\subsection{Number of predictors}

When choosing the number of factors to retain in the final model, it is commong to compare the predictive capacity (or the fit) of the models including and exlucing a factor. It is therefore important to choose a model that only contains factors whose contribution is not trivial (in substantive terms), while also being a model whose fit is not (statistically) significantly worse that the fit od models containing more factors. It has to be kept in mind that an increase number of factors in the model requires a larger number of units at the different levels of analysis in order to ensure that the models would converge and results can actually be obtained. Moreover, introducing predictors entails the need to estimate coefficients, which leads to losing degrees of freedom and, thus, the statistical power for finding statistically significant effects is reduced. 

\newpage
\subsection{The importance of centering}
\label{sec:multilevel.center}

The interpretation of the results obtained using different multilevel models for longitudinal data analysis is affected by the way in which the $Time$ variable is coded (Biesanz, Deeb-Sossa, Papadakis, Bollen, \& Curran, 2004). Shaw and Liang (2012) discuss three common ways of coding: (a) assigning 0 to the first measurement occasion, usaully treated as a baseline; (b) centring with respect to the average of the $Time$ variable, so that 0 refers to the middle point in the time interval covered by the study; and (c) assigning 0 before or after an important event taking place, for instance, an intervention. In any case, in order to decide where to place the value of 0 in the $Time$ variable does not need to reflect only conventional practices, because it should respond to the moments in time to which the main research questions refer. To reduce the effect of how $Time$ is coded, Biesanz et al. (2004) suggest plotting the data and examining the effects that are most relevant for the research. Graphs are especially relevant when interactions between the factors take place.\\[2ex]

Regarding the predictors included in the model, it is common to center these variables with respect to their mean across all individuals, especially when interval scale variables (lacking a natural 0) are used (e.g., McCoach, O'Connell, Reis, \& Levitt, 2006). What is achieved by the linear transformation (subtracting the mean from each value) is to make the interpretation easier. For instance, the intercept is interpreted as the expected value of the response variable when all predictors are at their average values. Moreover, the the variances of the intercept and the slopes represent the expected variance in relation to the \enquote{average participant} (Hox, 2002). In contrast, if the predictors are not centered, these estimates would be more difficult to interpret, as they would refer to individuals with all predictors being equal to 0 (e.g., zero income, zero social contact or getting a 0 in an IQ test).\\[2ex]

Regarding the shape of the growth curves, it has to be highlighted that linear multilevel models do not refer only to linear relations between time and the outcome of interest. In that sense, as it was already discussed in section \ref{sec:polynomial}, a polynomial model can model a quadratic relation: $Y_{ij}= \beta_{0j} +\beta_{1j}Time_{ij} + \beta_{2j}Time_{ij}^{2} + \epsilon_{ij}$. In this model, $\beta_{1j}$ would quantify the instantaneous rate of change when $Time=0$, whereas $\beta_{2j}$ would refer to the acceleration of the growth and thus positive values of $\beta_{2j}$ are associated with an increasing change in $Y$ for each measurement occasion, whereas a negative value would indicate changes that become smaller (i.e., deceleration of the growth). Another linear model represeting a nonlinear relation would be the inverse or reciprocal model (Solanas \& Puyuelo, 1995) in which there is an asymptote in the growth, defined as the intercept and a quantification of the deceleration of the growth (the slope coefficient), using a predictor $1/Time$.

\subsection{Getting to know the data at hand}

After all the theoretical explanations provided, it it time to perform an initial exploration of possible predictors of the variability in MoCA, focussing on time, time-constant predictors, and time-variant predictors. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=6, fig.width=6>>=
attach(Datos.Multi)

par(mfrow=c(2,2))

plot(Moment,MoCA, pch=1, xaxt="n")
abline(lm(MoCA~Moment))
title(main="Effect of time?")
axis(side=1,as.numeric(names(table(Moment))))

plot(Moment,MoCA, pch=" ", xaxt="n")
axis(side=1,as.numeric(names(table(Moment))))
title(main="Effect of place of residence?")
points(Moment[Resides=="community"],MoCA[Resides=="community"],pch=0, col="darkgreen")
points(Moment[Resides=="residence"],MoCA[Resides=="residence"],pch=17, col="red")
meanR <- rep(mean(MoCA[Resides=="residence"]),length(table(Moment)))
meanC <- rep(mean(MoCA[Resides=="community"]),length(table(Moment)))
lines(as.numeric(names(table(Moment))),meanR,col="red")
lines(as.numeric(names(table(Moment))),meanC,col="darkgreen")

plot(Moment,MoCA, pch=" ", xaxt="n")
axis(side=1,as.numeric(names(table(Moment))))
title(main="Effect of previous occupation?")
points(Moment[Prev_occup=="physical"],MoCA[Prev_occup=="physical"],pch=0, col="orange")
points(Moment[Prev_occup=="intellectual"],MoCA[Prev_occup=="intellectual"],pch=17, col="pink")
meanF <- rep(mean(MoCA[Prev_occup=="physical"]),length(table(Moment)))
meanI <- rep(mean(MoCA[Prev_occup=="intellectual"]),length(table(Moment)))
lines(as.numeric(names(table(Moment))),meanF,col="orange")
lines(as.numeric(names(table(Moment))),meanI,col="pink")

plot(Moment,MoCA, pch=" ", xaxt="n")
axis(side=1,as.numeric(names(table(Moment))))
title(main="Effect of gender?")
points(Moment[Gender=="male"],MoCA[Gender=="male"],pch=0, col="grey")
points(Moment[Gender=="female"],MoCA[Gender=="female"],pch=17, col="black")
meanH <- rep(mean(MoCA[Gender=="male"]),length(table(Moment)))
meanM <- rep(mean(MoCA[Gender=="female"]),length(table(Moment)))
lines(as.numeric(names(table(Moment))),meanH,col="grey")
lines(as.numeric(names(table(Moment))),meanM,col="black")
@

The plots presented suggest that there is a general deterioration (i.e., a decrease) of the cognitive function with time. Moreover, there are clear differences between the individuals according to the place of residence, with higher MoCA scores observed for people living in the community (green empty squares) than for people living in assisted housing (red filled triangles). In addition, there are also differences according to the previous occupation, with people who were involved in intellectual jobs (pink filled triangles) showing better congitive function that people with manual jobs (orange empty triangles). Finally, the differences for gender are not as clear. Thefore, there are apparently two potentially useful predictors of the MoCA scores (place of residence and previous occupation), apart from time passing. We do not yet know, however, if these predictors accopunt for unique variability or they overlap (i.e., are collinear). \\[2ex]

In what follows we continue our visual exploration of other potentially useful predictors. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=6, fig.width=6>>=
par(mfrow=c(2,2))

plot(Moment,MoCA, pch=" ", xaxt="n")
title(main="Effect of activity?")
axis(side=1,as.numeric(names(table(Moment))))
for (i in 1:length(as.numeric(names(table(Activ)))))
  points(Moment[Activ==i],MoCA[Activ==i],pch=20,cex=(1+0.225*i))

plot(Activ,MoCA, pch=1)
abline(lm(MoCA~Activ))
title(main="Effect of activity?")
axis(side=1,as.numeric(names(table(Activ))))

plot(Moment,MoCA, pch=" ", xaxt="n")
title(main="Effect of age?")
axis(side=1,as.numeric(names(table(Moment))))
start_age <- min(Age_in) - 1 
for (i in 1:length(as.numeric(names(table(Age_in)))))
  points(Moment[Age_in==start_age+i],MoCA[Age_in==start_age+i],pch=20,cex=(1+0.125*i))

plot(Age_in,MoCA, pch=1, xaxt="n")
abline(lm(MoCA~Age_in))
title(main="Effect of age?")
axis(side=1,as.numeric(names(table(Age_in))))
@

The plot of social activity versus cognitive functions suggests a positive relation between the two variables. Although not as clear, the same can be inferred from the top left panel including the same two variables and also time: the larger circles (representing more frequent social activity) are more common for the higher MoCA values. Regarding age, the bottom right scatterplot indicates that this is probably not a useful predictor. The same can be inferred from the bottom left graph, in which the size of the circle (larger for older individuals) does not seem to be related to the higher or lower MoCA scores. \\[2ex]

In order to carry out multilevel analyses in \textbf{R} the \textbf{nlme} package can be used, first installing it and then loading it. The manual by Paul Bliese (2013) is useful for learning how to use this package.

<<echo=TRUE, eval=FALSE, warning=FALSE>>=
install.packages("nlme")
@

<<echo=TRUE, eval=TRUE, warning=FALSE>>=
require(nlme)
@

\newpage
\subsection{Time-constant predictors}

The main pieces of information are: 
\begin{itemize}
\item the estimates of the fixed effects obtained via the {\tt summary()} function applied to the object containing the model - these estimates are expressed in the same measurement units as the outcome variable (here scores in MoCA); 
\item the estimates of the variances, i.e., the  random effects obtained via the via the {\tt VarCorr()} function applied to the object containing the model; \item the statistical significance of the fixed coefficients obtained via the {\tt summary()} function applied to the object containing the model; 
\item the confidence intervals of the fixed and random effects obtained via the {\tt intervals()} function applied to the object containing the model; 
\item the estimates of the intercept and the trend or growth (i.e., the random effects) for each individual obtained via the via the {\tt coefficients()} function applied to the object containing the model - in the code presented here we used these individual estimates for the graphical representations.
\end{itemize}

Apart from these pieces of information used for each model specified, there are also other relevant quantifications when comparing models among themselves:
\begin{itemize}
\item the reduction of Level 1 (intra-individual) residual variablity by adding either time-constant (Level 2) or time-variant (Level 1) predictors, usually expressed as a relative term with respect to the intra-individual residual variability in absence of the predictor; 
\item the reduction of the Level 2 (inter-individual) variability in the effects by adding time-constant (Level 2) predictors, usually expressed as a relative term with respect to the Level 2 variability in the effects in absence of the predictor; 
\item when the models share the same fixed effects but vary in the number of effects allowed to vary across individuals (Level 2 units), it is also possible to compare the models statistically in order to obtain further evidence regarding the reduction of unexplained variability.
\end{itemize}

In what follows, we specify different models, starting from the null model and adding static predictors (whose values do not change in time). In the null model, an indication of the need to apply multilevel analysis is given by the value of the intraclass correlation (ICC), which expresses the proportion of between-cases variability with respect to the whole (between-cases and within-cases) variability. The greater the ICC value, the more justified the application of multilevel models. In the results for the different models tested, we suggest that the reader pays attention to:
\begin{itemize}
\item the predictors added in each model
\item the relative reduction in residual Level 1 variability
\item the relative reduction in Level 2 variability of intercepts
\item the relative reduction in Level 2 variability of growths (i.e., time trends)
\end{itemize}

<<echo=TRUE, eval=TRUE, warning=FALSE, fig.height=7, fig.width=6>>=
par(mfrow=c(3,2))

#Graphical representation of the MoCA scores in time
# (presented at the end of the numerical output for the different models)
plot(Moment,MoCA, pch=1, xaxt="n")
title(main="Data: MoCA scores obtained")
axis(side=1,as.numeric(names(table(Moment))))

#Null model: Random intercept
Model.0 <- lme(MoCA~1,
               random=~1|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.0)

#Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.0)

# Proportion of within-case (unexplained) variability
Model.0$sigma^2/(Model.0$sigma^2+as.numeric(VarCorr(Model.0)[1,1]))

# Proportion of between-cases variability in intercept
# This values is necessarily complementary to the previous one
as.numeric(VarCorr(Model.0)[1,1])/(Model.0$sigma^2+as.numeric(VarCorr(Model.0)[1,1]))

#Graphical representation of the model 
# (presented at the end of the numerical output for the different models)
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Null model with random intercept")
rand.int <- rep(0,length(table(Id)))
for (i in 1:length(table(Id)))
  rand.int[i] <- coefficients(Model.0)$"(Intercept)"[i] 
for (i in 1:length(table(Id)))
  lines(as.numeric(names(table(Mcenter))),rep(rand.int[i],length(table(Moment))),col="blue")
mean.null <- rep(Model.0$coef[1]$fixed[[1]],length(table(Moment)))
lines(as.numeric(names(table(Mcenter))),mean.null,lwd=3)

###################################################
#Model with with random intercept and time as a fixed factor
# Level 2: Is there variability in the effect of time 
# to be accounted for by dynamic within-case predictors? 
Model.1 <- lme(MoCA~1+Mcenter,
               random=~1|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.1)

#Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.1)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.1)

# Comparison to null model
# Relative reduction of within-case (unexplained) variability
# The larger the reduction, the better the new model
(Model.0$sigma^2 - Model.1$sigma^2)/Model.0$sigma^2

# Proportion of between-cases variability in intercept
as.numeric(VarCorr(Model.1)[1,1])/sum(as.numeric(VarCorr(Model.1)[,1]))

#Graphical representation of the model
# (presented at the end of the numerical output for the different models)
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Time as a fixed factor. Random intercept")
x <- as.numeric(names(table(Mcenter)))
rand.int <- rep(0,length(table(Id)))
rand.slope <- rep(0,length(table(Id)))
for (i in 1:length(table(Id)))
{
  rand.int[i] <- coefficients(Model.1)$"(Intercept)"[i]
  rand.slope[i] <- coefficients(Model.1)$"Mcenter"[i]
}
# Represent lines for each individual: random intercepts, same slope
for (i in 1:length(table(Id)))
  curve(rand.int[i]+rand.slope[i]*x,add=T,col="blue")
# Represent the fixed effect estimates of intercept and slope
curve(Model.1$coef[1]$fixed[[1]]+Model.1$coef[1]$fixed[[2]]*x,add=T,col="black",lwd=3)

###################################################
#Model with with random intercept and time as a fixed factor
# Level 2: Between-cases (static) predictor: Previous occupation
Model.2 <- lme(MoCA~1+Mcenter+Prev_occup,
               random=~1|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.2)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.2)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.2)

# Relative reduction of within-case (unexplained) variability
# as compared to the previous model (random intercept)
# The larger the reduction, the better the new model
(Model.1$sigma^2 - Model.2$sigma^2)/Model.1$sigma^2

# Main aim: Relative reduction of in the intercept variance
(as.numeric(VarCorr(Model.1)[1,1])-as.numeric(VarCorr(Model.2)[1,1]))/as.numeric(VarCorr(Model.1)[1,1])

#Graphical representation of the model
# (presented at the end of the numerical output for the different models)
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Time - fixed. Random intercept. \nOccupation: type of line")
# Represent the random intercept and fixed slope
# with different lines according to the type of previous occupation
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual"],
        fitted(Model.2)[Id==i & Prev_occup=="intellectual"],lty=5,col="grey")
  lines(Mcenter[Id==i & Prev_occup=="physical"],
        fitted(Model.2)[Id==i & Prev_occup=="physical"],lty=3)
}


###################################################
#Model with with random intercept and time as a fixed factor
# Level 2: Between-cases (static) predictor: Previous occupation 
# Level 2: Between-cases (static) predictor: Age in the beginning of the study
Model.3 <- lme(MoCA~1+Mcenter+Prev_occup+Age_in,
               random=~1|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.3)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.3)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.3)

# Comparison to Model 2
# Relative reduction of within-case (unexplained) variability
# The larger the reduction, the better the new model
(Model.2$sigma^2 - Model.3$sigma^2)/Model.2$sigma^2

# Comparison to Model 2
# Main aim: Relative reduction of in the intercept variance
# The larger the reduction, the better the predictor in accounting for variability
(as.numeric(VarCorr(Model.2)[1,1])-as.numeric(VarCorr(Model.3)[1,1]))/
  as.numeric(VarCorr(Model.2)[1,1])

#Graphical representation of the model
# (presented at the end of the numerical output for the different models)
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Time - fixed. Random intercept. \nOccupation: type of line. Dot size=age")
# Represent the random intercept and fixed slope
# Represent previous occupation by type of line: intellectual=dashes, manual=dots
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual"],
        fitted(Model.2)[Id==i & Prev_occup=="intellectual"],lty=5,col="grey")
  lines(Mcenter[Id==i & Prev_occup=="physical"],
        fitted(Model.2)[Id==i & Prev_occup=="physical"],lty=3)
}

# Represent the different ages by the size of the dot/circle
start_age <- min(Age_in) - 1 
for (i in 1:length(as.numeric(names(table(Age_in)))))
  points(Mcenter[Age_in==start_age+i],MoCA[Age_in==start_age+i],pch=20,cex=(1+0.1*i))

###################################################
#Model with with random intercept and time as a fixed factor
# Level 2: Between-cases (static) predictors: Previous occupation and Gender
Model.4 <- lme(MoCA~1+Mcenter+Prev_occup+Gender,
               random=~1|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.4)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.4)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.4)

# Comparison to Model 2 
# (Model 3 is not used as a reference, given that age was not a useful predictor)
# Relative reduction of within-case (unexplained) variability
# The larger the reduction, the better the new model
(Model.2$sigma^2 - Model.4$sigma^2)/Model.2$sigma^2

# Comparison to Model 2
# (Model 3 is not used as a reference, given that age was not a useful predictor)
# Main aim: Relative reduction of in the intercept variance
# The larger the reduction, the better the predictor in accounting for variability
(as.numeric(VarCorr(Model.2)[1,1])-as.numeric(VarCorr(Model.4)[1,1]))/
  as.numeric(VarCorr(Model.2)[1,1])

#Graphical representation of the model
# (presented at the end of the numerical output for the different models)
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Time - fixed. Random intercept. \n Occupation: type of line. Gender=colors")
# Represent the different intercepts and the common slope
# Represent gender by color: male=blue, female=red
# Represent previous occupation by type of line: intellectual=dashes, manual=dots
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual" & Gender=="male"],
        fitted(Model.2)[Id==i & Prev_occup=="intellectual" & Gender=="male"],
        lty=5,col="blue")
    lines(Mcenter[Id==i & Prev_occup=="physical" & Gender=="male"],
        fitted(Model.2)[Id==i & Prev_occup=="physical" & Gender=="male"],
        lty=3,col="blue")
  lines(Mcenter[Id==i & Prev_occup=="intellectual" & Gender=="female"],
        fitted(Model.2)[Id==i & Prev_occup=="intellectual" & Gender=="female"],
        lty=5,col="red")
  lines(Mcenter[Id==i & Prev_occup=="physical" & Gender=="female"],
        fitted(Model.2)[Id==i & Prev_occup=="physical" & Gender=="female"],
        lty=3,col="red")
}
@

\newpage
\subsection{Time-variant predictors}

In this section we continue exploring the variability in the MoCA scores. We will first model the growth (actually a reduction indicating a deterioration) as a random efect, that is, we will allow different rates of deterioration for the different participants in the sample. We will compare this model to the model in which only the initial point (i.e., the intercept) is allowed to vary randomly among individuals. \\[2ex]

Afterwards, we incorporate in the model the time-constant predictor that was shown to be useful (previous occupation; but not age or gender) and, in addition to it, we explore whether time-variant predictors (social activities and place of residence) can also be useful for accounting for the variability in the slopes. 

<<echo=TRUE, eval=TRUE, fig.height=6, fig.width=6,warning=FALSE>>=
par(mfrow=c(2,2))

###################################################
#Model with with random intercept and time as a random factor
# Level 1: Is there variability in the effect of time 
# to be accounted for by dynamic within-case predictors? 
Model.5 <- lme(MoCA~1+Mcenter,
               random=~1+Mcenter|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.5)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.5)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.5)

# Comparison to Model 1: random intercept, but fixed slope
# Relative reduction of within-case (unexplained) variability
(Model.1$sigma^2 - Model.5$sigma^2)/Model.1$sigma^2

# Proportion of between-cases variability in intercept
as.numeric(VarCorr(Model.5)[1,1])/sum(as.numeric(VarCorr(Model.5)[,1]))

# Proportion of between-cases variability in trend line
as.numeric(VarCorr(Model.5)[2,1])/sum(as.numeric(VarCorr(Model.5)[,1]))

# Statistical comparison between the models 1 and 5
# This comparison is meaningful, as the same fixed effects are incorporating
# We are testing whether allowing for variation in slope is useful
anova(Model.1,Model.5) 

#Graphical representation of the model
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Random intercept and trend",cex.main=0.8)
rand.int <- rep(0,length(table(Id)))
rand.slope <- rep(0,length(table(Id)))
for (i in 1:length(table(Id)))
{
  rand.int[i] <- coefficients(Model.5)$"(Intercept)"[i]
  rand.slope[i] <- coefficients(Model.5)$"Mcenter"[i]
}  
# Represent separate intercepts and slopes
for (i in 1:length(table(Id)))
  curve(rand.int[i]+rand.slope[i]*x,add=T,col="blue")
# Represent the average estimate of intercept and slope
curve(Model.5$coef[1]$fixed[[1]]+Model.5$coef[1]$fixed[[2]]*x,add=T,col="black",lwd=3)

###################################################
# Model with with random intercept and time as a random factor
# Level 2: Between-cases (static) predictor: Previous occupation
Model.2and5 <- lme(MoCA~1+Mcenter+Prev_occup,
                   random=~1+Mcenter|Id,
                   control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.2and5)

#Graphical representation of the model
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Random intercept&trend. \nOccupation:line type.",
      cex.main=0.8)
# Represent separate intercepts and slopes
# Represent previous occupation by type of line: intellectual=dashes, manual=dots
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual"],
        fitted(Model.2and5)[Id==i & Prev_occup=="intellectual"],lty=5)
  lines(Mcenter[Id==i & Prev_occup=="physical"],
        fitted(Model.2and5)[Id==i & Prev_occup=="physical"],lty=3)
}

###################################################
#Model with with random intercept and time as a random factor
# Level 2: Between-cases (static) predictor: Previous occupation
# Level 1: Within-cases (dynamic) predictor: Weekly activities

Model.6 <- lme(MoCA~1+Mcenter+Prev_occup+Activ,
               random=~1+Mcenter|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.6)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.6)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.6)

# Comparison to Model 5 (including only time as a predictor)
# Main aim: Relative reduction of within-case (unexplained) variability
(Model.5$sigma^2 - Model.6$sigma^2)/Model.5$sigma^2

# Comparison to Model 2and5 (including only time and previous occupation as predictors)
# Main aim: Relative reduction of within-case (unexplained) variability
(Model.2and5$sigma^2 - Model.6$sigma^2)/Model.2and5$sigma^2

# Comparison to Model 2and5 (including only time as a predictor)
# Main aim: Relative reduction of between-cases unexplained variability in trend lines
(as.numeric(VarCorr(Model.2and5)[2,1])-as.numeric(VarCorr(Model.6)[2,1]))/
  as.numeric(VarCorr(Model.2and5)[2,1])

# Comparison to Model 2and5 (including only time and previous occupation as predictors)
# Relative reduction of between-cases unexplained variability in intercept
# (Given that no static predictors are added a large reduction is not expected)
(as.numeric(VarCorr(Model.2and5)[1,1])-as.numeric(VarCorr(Model.6)[1,1]))/
  as.numeric(VarCorr(Model.2and5)[1,1])

#Graphical representation of the model
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Random intercept&trend. Occupation:line type. \nDots size=activity",
      cex.main=0.8)
# Represent separate intercepts and slopes
# Represent previous occupation by type of line: intellectual=dashes, manual=dots
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual"],
        fitted(Model.6)[Id==i & Prev_occup=="intellectual"],lty=5,col="grey")
  lines(Mcenter[Id==i & Prev_occup=="physical"],
        fitted(Model.6)[Id==i & Prev_occup=="physical"],lty=3)
}
# Represent the activity by the size of the dot/circle
for (i in 1:length(as.numeric(names(table(Activ)))))
  points(Mcenter[Activ==i],MoCA[Activ==i],pch=20,cex=(1+0.2*i))

###################################################
#Model with with random intercept and time as a random factor
# Level 2: Between-cases (static) predictor: Previous occupation
# Level 1: Within-cases (dynamic) predictors: Weekly activities and place of residence
Model.7 <- lme(MoCA~1+Mcenter+Prev_occup+Activ+Resides,
               random=~1+Mcenter|Id,
               control=list(opt="optim"))

# Obtain summary information about the model
summary(Model.7)

# Obtain confidence intervals for the estimates of fixed and random effects
intervals(Model.7)

# Obtain variance components: summary() provides standard deviations
VarCorr(Model.7)

# Comparison to Model 6
# Relative reduction of within-case (unexplained) variability
(Model.6$sigma^2 - Model.7$sigma^2)/Model.6$sigma^2

# Comparison to Model 6
# Main aim: Relative reduction of between-cases unexplained variability in trend lines
(as.numeric(VarCorr(Model.6)[2,1])-as.numeric(VarCorr(Model.7)[2,1]))/
  as.numeric(VarCorr(Model.6)[2,1])

# Comparison to Model 6
# Relative reduction of intercept variance
(as.numeric(VarCorr(Model.6)[1,1])-as.numeric(VarCorr(Model.7)[1,1]))/
  as.numeric(VarCorr(Model.6)[1,1])

#Graphical representation of the model
plot(Mcenter,MoCA, pch=" ")
axis(side=1,as.numeric(names(table(Mcenter))))
title(main="Random intercept&trend. Occupation:line type. \nDots size=activity. Color=residence",
      cex.main=0.8)
# Represent separate intercepts and slopes
# Represent previous occupation by type of line: intellectual=dashes, manual=dots
for (i in 1:length(Id))
{  
  lines(Mcenter[Id==i & Prev_occup=="intellectual"],
        fitted(Model.7)[Id==i & Prev_occup=="intellectual"],lty=5,col="grey")
  lines(Mcenter[Id==i & Prev_occup=="physical"],
        fitted(Model.7)[Id==i & Prev_occup=="physical"],lty=3)
}
# Represent the activity by the size of the dot/circle
# Represent the place of residence by the color of the dot/circle: community=green, residence=red
for (i in 1:length(as.numeric(names(table(Activ)))))
{
  points(Mcenter[Activ==i & Resides=="community"],MoCA[Activ==i & Resides=="community"],
         pch=20,cex=(1+0.2*i),col="green")
  points(Mcenter[Activ==i & Resides=="residence"],MoCA[Activ==i & Resides=="residence"],
         pch=20,cex=(1+0.2*i),col="red")
}
@

It is also possible to model the heterogeneity of variance in the MoCA scores according the the type of previous occupation. 

<<echo=TRUE, eval=TRUE>>=
# Explore possible heterogeneity of variance
tapply(MoCA,Prev_occup,var)

# Additional aspect modelled: heterogeneous variance in different type of previous activity
Model.7b <- lme(MoCA~1+Mcenter+Prev_occup+Activ+Resides,
               random=~1+Mcenter|Id,
               weights = varIdent(form = ~1 | Prev_occup),
               control=list(opt="optim"))

# Compare statistically the two models
# (i.e., decide whether it was necessary to model this aspect of the data)
anova(Model.7,Model.7b)
@

\newpage
\subsection{Obtaining information about random effects}

The estimates per individual of the intercept and slope and the average estimates of the effect of the time-constant predictor (previous occupation) and the time-variant predictors (social activity and place of reseidence) can be obtained as follows:

<<echo=TRUE, eval=TRUE>>=
# Obtain estimates
coefficients(Model.7)
@

These random effects can also be represented graphically as deviations from the fixed effects estimates:

<<echo=TRUE, eval=TRUE, fig.height=5, fig.width=5,warning=FALSE>>=
# Plot of random effects for Model 7
plot(ranef(Model.7))
@

Regarding the degree of association between the random effects, the information was already provided in part of the output:
<<echo=TRUE, eval=TRUE>>=
# Obtain estimates
VarCorr(Model.7)
@

In the following we also represent graphically the degree of association between the random effects

<<echo=TRUE, eval=TRUE, fig.height=5, fig.width=6,warning=FALSE>>=
par(mfrow=c(1,2))

# Explore visually the association between intercept and slope
intercept.random <- coefficients(Model.7)$"(Intercept)"
slope.random <- coefficients(Model.7)$Mcenter

plot(slope.random~intercept.random)
abline(lm(slope.random~intercept.random))

# Explore visually the association between the random effects (deviations from average)
dev.intercept <- intercept.random - fixed.effects(Model.7)[[1]]
dev.slope <- slope.random - fixed.effects(Model.7)[[2]]
plot(dev.slope~dev.intercept)
abline(lm(dev.slope~dev.intercept))
@

\newpage
\subsection{Validating the model}

\subsubsection{Normality}
Checking normality of the residuals at different levels. Keep in kind that the null hypothesis in the Shapiro-Wilk test is that the distribution of value is normal and that what is desired is not to reject this null hypothesis in any of the tests.

<<echo=TRUE, eval=TRUE, fig.height=5, fig.width=5,warning=FALSE>>=
# Level 1 residuals are directly obtained from the output:
# the difference between observed MoCA values and fitted values
par(mfrow=c(1,2))

res.Level1 <- as.numeric(residuals(Model.7))
                         
hist(res.Level1,prob=TRUE,xlab="Residuals")
curve(dnorm(x, mean=mean(res.Level1),sd=sd(res.Level1)),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")

shapiro.test(res.Level1)

# Level 2 residuals are obtained as the average of the residuals for each of the
# participants (there are as many residuals per person as moments of measurement)

res.Level2 <- rep(0,length(table(Id)))
for (i in 1:length(table(Id)))
{ 
  k <- i-1
  res.Level2[i] <- mean(res.Level1[(length(table(Moment))*k+1):
                                    (length(table(Moment))*k+length(table(Moment)))]) 
}

hist(res.Level2,prob=TRUE,xlab="Residuals")
curve(dnorm(x, mean=mean(res.Level2),sd=sd(res.Level2)),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")

shapiro.test(res.Level2)

@

\newpage
\subsubsection{Independence between residuals and predictors}

Assumption 1: the residuals at level 1 and the predictors at level 1 are not related

<<echo=TRUE, eval=TRUE, fig.height=4, fig.width=5,warning=FALSE>>=

# Create a dummy version (0,1) of the predictor, which is a factor (categorical)
resides.dummy <- rep(0,length(Id))
for (i in 1: length(Id))
  if (Resides[i] == "residence") resides.dummy[i] <- 1
# Point biserial correlation
cor(res.Level1,resides.dummy)

# Pearson's correlation between two numerical variables
cor(res.Level1,Mcenter)
cor(res.Level1,Activ)

# Graphical representation of the assumptions
par(mfrow=c(1,3))
plot(resides.dummy,res.Level1)
plot(Mcenter,res.Level1)
abline(lm(res.Level1~Mcenter))
plot(Activ,res.Level1)
abline(lm(res.Level1~Activ))
@

Assumption 2: the residuals at level 2 and the predictors at level 2 are not related

<<echo=TRUE, eval=TRUE, fig.height=4, fig.width=4,warning=FALSE>>=
# Create a dummy version (0,1) of the predictor, which is a factor (categorical)
occup.dummy <- rep(0,length(Id))
for (i in 1: length(Id))
  if (Prev_occup[i] == "physical") occup.dummy[i] <- 1


# It is necessary to compute a Level 2 version of the predictor, containing as many values
# as participants are in the sample
predictor.Level2 <- rep(0,length(table(Id)))
for (i in 1:length(table(Id)))
{ k <- i-1  
  predictor.Level2[i] <- occup.dummy[length(table(Mcenter))*k+1] 
}

# Point biserial correlation
cor(predictor.Level2,res.Level2)

# Graphical representation
plot(predictor.Level2,res.Level2)
@


Assumption 3: the residuals at level 1 and the predictors at level 2 are not related.\\[2ex] 
Assumption 4: the residuals at level 2 and the predictors at level 1 are not related

<<echo=TRUE, eval=TRUE, fig.height=7, fig.width=6,warning=FALSE>>=
par(mfrow=c(2,2))

cor(occup.dummy,res.Level1)
plot(res.Level1~Prev_occup)

#In order to be able to perform this check, it is necessary to create an extended version of the Level 2 residual 
# containing as many value as there are MoCA values; that is, repeating for each participant
# his/her average residual as many times as measurement occasions there are.

res.Level2.ext <- 1:length(Id)
for (i in 1:length(table(Id)))
{ 
  k <- i-1
  res.Level2.ext[( length(table(Mcenter))*k+1):
                 ( length(table(Mcenter))*k+length(table(Mcenter)))] <- res.Level2[i] 
}

cor(Mcenter,res.Level2.ext)
plot(Mcenter,res.Level2.ext)

cor(resides.dummy,res.Level2.ext)
plot(res.Level2.ext~Resides)

cor(Activ,res.Level2.ext)
plot(res.Level2.ext~Activ)
@

Assumption 6: lack of relation between the predictors (no multicollinearity)

<<echo=TRUE, eval=TRUE, fig.height=7, fig.width=6,warning=FALSE>>=
par(mfrow=c(2,3))

# Point biserial correlation
cor(Mcenter,occup.dummy)
# Plot for representing categories
plot(table(Mcenter,occup.dummy))

# Point biserial correlation
cor(Mcenter,resides.dummy)
# Plot for representing categories
plot(table(Mcenter,resides.dummy))

# Point biserial correlation
cor(Activ,occup.dummy)
# Boxplot
plot(Activ~Prev_occup)

# Correlation between quantitative variables
cor(Mcenter,Activ)
# Scatterplot
plot(Mcenter,Activ)

# Point biserial correlation
cor(Activ,resides.dummy)
# Boxplot
plot(Activ~Resides)

# Chis-square test
chisq.test(table(Prev_occup,Resides))

# Cramér's V
sqrt( chisq.test(table(Prev_occup,Resides))$statistic[[1]]/
       (length(Id)*(min(length(table(Prev_occup)),length(table(Resides))-1))) )

# Plot for representing categories
plot(table(Prev_occup,resides.dummy))
@

\newpage
\subsubsection{Independence of the residual}

Assumption: the residual is independent (i.e., not autocorrelated). For checking this assumption, we have included Model 8 in which autocorrelation is modelled. We advise the reader to see the point estimate of autocorrelation from Model 8 presented next, as well as the corresponding confidence interval, especially, whether it includes zero or not. Moreover, it is possible also to pay attention to the \emph{p} value of the test comparing Model 7 to Model 8, to see whether a better fit to the data is achieved by modelling autocorrelation.


<<echo=TRUE, eval=TRUE, warning=FALSE>>=
###################################################
#Model with with random intercept and time as a random factor
# Level 2: Between-cases (static) predictor: Previous occupation
# Level 1: Within-cases (dynamic) predictors: Weekly activities and place of residence
# Additional aspect modelled: autocorrelation 
Model.8 <- lme(MoCA~1+Mcenter+Prev_occup+Activ+Resides,
               random=~1+Mcenter|Id,
               correlation= corAR1(form=~1|Id),
               control=list(opt="optim"))

# Obtain summary information about the model 
summary(Model.8)

#Interval estimates for fixed effects and variances
intervals(Model.8)

#Variance and covariance components
VarCorr(Model.8)

# Comparison to Model 7
# Relative reduction of within-case (unexplained) variability
(Model.7$sigma^2 - Model.8$sigma^2)/Model.7$sigma^2

# Comparison to Model 7
# Relative reduction of intercept variance
(as.numeric(VarCorr(Model.7)[1,1])-as.numeric(VarCorr(Model.8)[1,1]))/
  as.numeric(VarCorr(Model.7)[1,1])

# Comparison to Model 7
# Main aim: Relative reduction of between-cases unexplained variability in trend lines
(as.numeric(VarCorr(Model.7)[2,1])-as.numeric(VarCorr(Model.8)[2,1]))/
  as.numeric(VarCorr(Model.7)[2,1])

# Verify statistically whether modelling the additional data aspects is necessary
anova(Model.7,Model.8)
@


\newpage
\section{Use of the General Linear Model with R}

\subsection{Simple logistic regression with a categorical predictor}
\label{sec:log.simple.cat}

In order to model the probability $\pi$ of an event taking place as a function of some predictor \emph{X}, the following expression is used:

\begin{equation*}
\pi(X)=\frac{e^{\beta_{0} + \beta_{1}X}}{1+e^{\beta_{0} + \beta_{1}X}}
\end{equation*}

In order to use a linear model, the \emph{logit} transformation is used, so that:

\begin{equation*}
g(X)=ln\bigg(\frac{\pi(X)}{1-\pi(X)}\bigg) = \beta_{0} + \beta_{1}X
\end{equation*}

According to this transformation, the parameter estimates obtain would refer to the \emph{logit}, that is to the natural logarithm of the odds, where the odds represents the ratio between the probability of the event of interest taking place and the probability of it not taking place. In that sense, the parameter estimates do not refer directly to probabilities, nor to the odds. \\[2ex]

A common way to interpret the estimate of the intercept is to first obtain its exponent: $e^{\hat{\beta_0}}$. This value represents the odds for suffering from alcohol addiction (i.e., the event of interest in the current example) for people for whom the predictor is equal to 0. In case the predictor used is the presence (1) vs. absence (0) of family history of addiction, $e^{\hat{\beta_0}}$ is equivalent to the number of people without family history that do present addiction divided by the number of peple  without family history that do not present addiction. A probability can be obtained from the odds, via $Pr(addiction)=\frac{odds(addiction)}{1+odds(addiction)}$. \\[2ex]

In order to interpret the value of the slope coefficient, it is also necessary to first obtain the exponent: $e^{\hat{\beta_1}}$. In simple logistic regression, this value represents the change in the odds when the predictor variable is increased in one unit. In other words, it represents an odds ratio. For the running example in which the family history is used as predictor, $e^{\hat{\beta_1}}$ would represent the increase in the odds of addiction for people with family history (variable equal to 1) vs. people without (variable to 0), that is, an odds ratio. \\[2ex]

The application of the whole regression equation in the form $e^{\hat{\beta_0 + \beta_1 \times 1}}$ would yield the odds for suffering from addiction for people whose score in the predictor variable is equal to 1 (i.e., people with family history). \\[2ex]

The statistical significance of the predictors (i.e., of the $\beta_i$ coefficients) can be assessed using Wald's test with one of the following statistics:

\begin{align*}
W=&\frac{\hat{\beta_1}}{SE_{\beta_1}} \sim \mathcal{N}(0,1) \quad {\tt when } \quad n\to\infty \\
W=&\frac{\hat{\beta_1}}{SE_{\beta_1}} \sim \chi_^{2}(\nu=1)
\end{align*}

\vspace{0.5cm}

In what follows we present an model with the previously mentioned categorical (actually, binary) predictor: presence vs. absence of family history of alcohol addition. We will use this model to illustrate how the terms of the logistic regression can be interpreted. We recommend that the reader pays attention to the
explanations provided as comments before each line of code.

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6, message=FALSE>>=
attach(Datos.Logistic)

# Model addiction as a function of having vs. not family history of addiction
reg.l1 <- glm(addiction ~ h.alcohol, family=binomial(logit))
# Obtain the summary information about the model
summary(reg.l1)

# Obtain the intercept: it represents an odds when X=0, that is,
#for people with no history: proportion with addiction divided by proportion with no addiction
exp(reg.l1$coefficients[[1]])

# Obtain the contingency table for the two categorical variables
table(addiction,h.alcohol)

# Obtain the same odds as before on the basis of the contingency table
odds.no.history <- as.numeric(table(addiction[h.alcohol=="no"]==1)["TRUE"])/
                      as.numeric(table(addiction[h.alcohol=="no"]==0)["TRUE"])
odds.no.history

# Obtain the probability of addiction when there is no family history out of the odds
prob.no.history <- odds.no.history / (1 + odds.no.history)
prob.no.history

# Obtain the odds addiction / no addiction for people with family history
odds.if.history <- as.numeric(table(addiction[h.alcohol=="yes"]==1)["TRUE"])/
                      as.numeric(table(addiction[h.alcohol=="yes"]==0)["TRUE"])
odds.if.history

# Obtain the probability of addiction when there is family history out of the odds
prob.if.history <- odds.if.history / (1 + odds.if.history)
prob.if.history 

# Obtain the slope coefficient: it represents an odds ratio:
# How many times more likely is it for a person with faimily history to
# present an addiction in comparison to people wiout family history
exp(reg.l1$coefficients[[2]])

# Obtain the same odds ratio from the contingeny table:
# it is the ratio (division) of the two previously obtained odds
OR.history <- odds.if.history/odds.no.history
OR.history
@

Comparing models can be achieved in several ways. One option is to use the \emph{G} statistic, which can be expressed via one of the following two ways:

\begin{align*}
G&=Deviance({\tt Model.0}) - Deviance({\tt Model.1}) \\
G&=-2ln\bigg(\frac{{\tt Model.0 \quad likelihood}}{{\tt Model.1 \quad likelihood}}\bigg)
\end{align*}

The \emph{G} statistic is distributed according to a $\chi^2$ distribution with 1 degree of freedom. \\[2ex]

Another quantification for comparing models is McFadden's pseudo-$R^2$, which represents the relative reduction of residual variability between a model with less predictors and a model with more predictors. The larger the pseudo-$R^2$, the better the fit of the model with more predictors. \\[2ex]

A third option to assess the fit of a model and to compare models is the \emph{V} index, which represents how well the model fits the data, by using the product of the probabilities estimated by the model for each individual presenting the characteristic of interest (e.g., addiction) and the product of the complementary of probabilities estimated by the model for each individual that does not present the characteristic of interest. Higher values represent better fit with 1 being the maximum, but the \emph{V} values tend to be very small; specifically, much smaller than the pseudo-$R^2$ values. \\[2ex]

With the following code we illustrate all three types of quantification and comparison between models. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
# Null model: No predictors included
reg.l0 <- glm(addiction ~ 1, family=binomial(logit))

# The intercept can be converted to the odds of addiction / no addiction in the sample
exp(reg.l0$coefficients[[1]])

# Obtaining the odds from the table of frequencies
table(addiction)
as.numeric(table(addiction==1)["TRUE"])/as.numeric(table(addiction==0)["TRUE"])

# Comparing  statistically Model 1 to the null model
anova(reg.l0,reg.l1, test="Chisq")

# McFadden's pseudo R-squared can be computed from the deviances of the two models
# Relative reduction of the unexplained variability
1 - (reg.l1$deviance/reg.l0$deviance)

# The square of the point biserial correlation between the binary target variable
# and the predicted probabilities can also be used as an indicator similar to R-squared
# Note that in "fitted" R provides the probabilities directly (not the logits)
(cor(addiction,reg.l1$fitted))^2

# The same value is obtained using the usual formula:
# 1 - (residual variability / total variability)
1 - ((sum((reg.l1$fitted-addiction)^2))/
       (var(addiction)*(length(addiction)-1)))

# V index of fit (the closer to 1, the better)
# Multiplying the fitted values (probabilities) for positive cases
#   by the complementary of the fitted values for negative cases
V.1 <- 1
for (i in 1:length(addiction))
{
  if (addiction[i]==0) V.1 <- V.1 * (1-reg.l1$fitted[[i]])
  if (addiction[i]==1) V.1 <- V.1 * reg.l1$fitted[[i]]
}
V.1

# Different intercept and slope coefficients would lead to 
# worse results for the current data, as shown
# First a dummy variable is created out of the factor
h.alcohol.dummy <- rep(0,length(h.alcohol))
for (i in 1:length(h.alcohol))
  if (h.alcohol[i]=="yes") h.alcohol.dummy[i] <- 1
# Second, the predicted lthe probabilities are obtained
reg.l1_other <- exp(-1 + 3*h.alcohol.dummy) / (1 + exp(-1 + 3*h.alcohol.dummy))
# Third, the value of V is obtained
V <- 1
for (i in 1:length(addiction))
{
  if (addiction[i]==0) V <- V * (1-reg.l1_other[[i]])
  if (addiction[i]==1) V <- V * reg.l1_other[[i]]
}
V
@

\newpage
\subsection{Simple logistic regression with a quantitative predictor}
\label{sec:log.simple.quanti}

\subsubsection{Main results}
In the present section we still deal with simple logistic regression, but in this case the predictor is quantitative, which means that there are several estimates of the probablity of presenting the event of interest - one for each value of the predictor. We first obtain the odds for suffering from addiction for two possible values of the quantitative predictor \enquote{income}. We than obtain the estimated probabilities out of the odds. Finally, we present the odds ration, indicating the increase in the odds for suffering from addiction for individuals with the lower of the two values of income. The increase in the odds (and the probability) of suffering from addiction takes place for lower values of income, given that the estimate of the regression coefficient associated with the \enquote{income} is negative. It is possible to perform analogous comparisons of the odds for suffering from addiction between other pairs of values of \enquote{income} that might be of interest. 


<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
reg.l2 <- glm(addiction ~ income, family=binomial(logit))
summary(reg.l2)

# Obtain the odds for addiction for income of 450 euros
odds.450 <- exp(reg.l2$coefficients[[1]]+reg.l2$coefficients[[2]]*450)
odds.450 

# Obtain the estimated probability for addiction for income of 450 euros
prob.450 <- odds.450 / (1+odds.450)
prob.450

# Obtain the odds for addiction for income of 950 euros
odds.950 <- exp(reg.l2$coefficients[[1]]+reg.l2$coefficients[[2]]*950)
odds.950 

# Obtain the estimated probability for addiction for income of 950 euros
prob.950 <- odds.950 / (1+odds.950)
prob.950

# Odds ratio for 450 vs. 950 euros income:
# How much does the risk to suffer from addiction increase
odds.450/odds.950
@

\subsubsection{Predictive (discriminative) capacity}

In order to quantify how good the model is in predicting (or classifying) the individuals into two groups - presenting or not addiction - it is possible to use the indices of sensitivity (labelling positive cases as positive, or avoiding false negatives) and specificity (labelling negative cases as negative, or avoiding false positives). The same quantification could have been opbtained for the simple logistic regression model with a categorical predictor, but in the current case, the fact that the predictor has more values leads to more different probabilities being estimated, which allows for analysis such as the ROC curve presented below. \\[2ex]

In order to assess sensitivity and specificity, it is necessary to choose a cut-off point in the estimated probability which distinguishes between individuals for whom addiction is predicted frm the invididuals for whom it is predicted not to take place. We here use the 0.5 probability as a cut-off, with values above it indicating that the model predicts presence of addiction. A good model would yield values as close to 1.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Using a probability of 0.5 as a cut-off
predict.addiction <- rep(0,length(addiction))
for (i in 1:length(addiction))
  if (reg.l2$fitted[i] > 0.5) predict.addiction[i] <- 1

# Contingency table
table(predict.addiction,addiction)

# Sensitivity to detect positive cases
sensitivity <- as.numeric(table(predict.addiction[addiction==1]==1)["TRUE"])/
  as.numeric(table(addiction==1)["TRUE"])
sensitivity

# Specificity: identify negative cases as such
specificity <- as.numeric(table(predict.addiction[addiction==0]==0)["TRUE"])/
  as.numeric(table(addiction==0)["TRUE"])
specificity
@

In order to avoid establishing a single cut-off point,it is possible to use the area under the ROC curve as a measure of the classification capacity of the model. The area under the curve is equivalent to the probability of superiority (Grissom, 1994; Parker and Vannest, 2009). The application of the probability of superiority in the current cases consists in comparing the estimated probability of addiction of each individual with addiction to the probability of addiction of each individual without addiction, quantifying the proportion of comparisons in which the individuals with addiction have a greater probability as estimated from the model. The following values have been suggested for assessing how well the model serves its purpose:  0.50 - 0.70: poor discrimination; 0.71 - 0.80: acceptable discrimination; 0.81 - 1.00: excellent discrimination.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Using the function for obtaining the probability of superiority (area under the ROC curve)

# Function
probsup <-function (x1,x2){
n1 <- sum(!is.na(x1))
n2 <- sum(!is.na(x2))
greater <- sum(unlist(lapply(x1,x2,FUN='>')),na.rm=TRUE)
equal <- 0.5*sum(unlist(lapply(x1,x2,FUN='==')),na.rm=TRUE)
prob_sup <- (greater+equal)/(n1*n2)
cat('Frequency >: ',greater,'\n')
cat('Frequency =: ',equal,'\n')
cat('Probability of superiority = Area under the curve: ', prob_sup,'\n')
}

# Enter the data
addiction.yes <- reg.l2$fitted[addiction==1]
addiction.no <- reg.l2$fitted[addiction==0]
# Call the function
probsup(addiction.yes,addiction.no)
@

The probability of superiority represents the area under the (ROC) curve and values closer to 1 suggest that the predictive capacity of the model is better in terms of predictive capacity. In the graphical representation sensitivity is represented, as well as the complementary of specificity, so that curves closer to the upper left extreme of the plot indicate that the model is better in terms of predictive capacity. We here represent the ROC curve as defined by using 10 different cut-off points (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, and 1) for the probability indicating presence of addiction. However, the line drawn is only an approximation to the actual area under the curve as quantified by the probability of superiority.

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
# Obtain the ROC curve: X (sensitivity) and Y (1 - specificity) 
# for 10 different cut-off probabilities: from 0 to 1 in steps of 0.1.
sens <- rep(0,10)
compl.spec <- rep(0,10)
for (i in 1:10)
{
  nombre <- rep(0,length(addiction))
  for (j in 1:length(addiction))
    if(reg.l2$fitted[j] <= (i/10)) nombre[j] <- "no" else nombre[j] <- "yes"
  
  if(sum(nombre=="yes")==0) 
    {
      sens[i] <- 0
      compl.spec[i] <-0
    }
  if(sum(nombre=="no")==0) 
    {
      sens[i] <- 1
      compl.spec[i] <-1
    }

  if( (sum(nombre=="no")!=0) && (sum(nombre=="yes")!=0))
  {
    sens[i] <- xtabs(~nombre+addiction)[2,2]/
      sum(xtabs(~nombre+addiction)[,2]);
    compl.spec[i] <- 1-xtabs(~nombre+addiction)[1,1]/
      sum(xtabs(~nombre+addiction)[,1])
  }
}

# Plot ROC curve: the closer to the upper left corner, the better
plot(compl.spec,sens,xlim=c(0,1),ylim=c(0,1),xlab="1-specificity",ylab="sensitivity")
abline(a=0,b=1)
lines(compl.spec[1:10],sens[1:10],col="red")
title(main="ROC Curve")
@

\newpage
\subsection{Multiple logistic regression with two categorical predictors}
\label{sec:log.multiple.cat}

\subsubsection{Main results}
In the current section we will illustrate the use of logistic regression when there are two categorical predictors. This will allows to show how models with a different number of predictors can be compared statistically in order to decide whether adding a predictor improves the model or not. Note that we define the reference categories in each categorical predictor - the categories denoted by 0, given that it is expected that the categories denoted by 1 are the ones associated with a greater probability for suffering from addiction. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=

# Ensure reference categories
h.alcohol.dummy <- rep(0,length(h.alcohol))
for (i in 1:length(h.alcohol))
  if(h.alcohol[i]=="yes") h.alcohol.dummy[i] <- 1

occup.status.dummy <- rep(0,length(occup.status))
for (i in 1:length(occup.status))
  if(occup.status[i]=="unemployed") occup.status.dummy[i] <- 1

# Predictors: family history of addiction and occupational status
reg.l3 <- glm(addiction ~ h.alcohol.dummy + occup.status.dummy, family=binomial(logit))
summary(reg.l3)

# Statistical comparison:
# Model 1 (familiy history) vs. Model 3 (family history + occupational status)
anova(reg.l1,reg.l3, test="Chisq")

# Relative reduction of residual variability: McFadden's R-squared
1 - (reg.l3$deviance/reg.l1$deviance)

# An increase in V (the fit of the model) is expected 
# if the predictor added (occupational status) is useful
V.2 <- 1
for (i in 1:length(addiction))
{
  if (addiction[i]==0) V.2 <- V.2 * (1-reg.l3$fitted[[i]])
  if (addiction[i]==1) V.2 <- V.2 * reg.l3$fitted[[i]]
}
V.2
# Increase in V
V.2 - V.1
@

We here get into more detail in the interpretation of the results from multiple logistic regression by considering a combination of risk factors. The final quantifications provided are odds ratios - increase in the risk of suffering from alcohol addiction in case of the \enquote{bad} vs. the \enquote{good} combination of factors. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# For people with no family history of addiction and working:
# Estimated odds for suffering from addiction vs. not
odds.nohist.working <- exp(reg.l3$coefficients[[1]]+
                             reg.l3$coefficients[[2]]*0+
                             reg.l3$coefficients[[3]]*0)
odds.nohist.working

# For people with family history of addiction and unemployed:
# Estimated odds for suffering from addiction vs. not
odds.hist.unemployed <- exp(reg.l3$coefficients[[1]]+
                             reg.l3$coefficients[[2]]*1+
                             reg.l3$coefficients[[3]]*1)
odds.hist.unemployed

# Odds ratio: increase in the risk of sufffering addiction:
# no family history of addiction & working vs. with family history & unemployed
odds.ratio <- odds.hist.unemployed/odds.nohist.working
odds.ratio

# For people with no family history of addiction and working:
# Estimated probability for suffering from addiction vs. not
odds.nohist.working / (1+odds.nohist.working)

# For people with family history of addiction and unemployed:
# Estimated odds for suffering from addiction vs. not
odds.hist.unemployed / (1+odds.hist.unemployed)
@

In the following, we considering the risk factors separately. We study the influence of occupational status for people with history of family addiction. The other combinations can be studied analogously.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# For people with family history of addiction and working:
# Estimated odds for suffering from addiction vs. not
odds.hist.working <- exp(reg.l3$coefficients[[1]]+
                             reg.l3$coefficients[[2]]*1+
                             reg.l3$coefficients[[3]]*0)
odds.hist.working

# For people with family history of addiction and unemployed:
# Estimated odds for suffering from addiction vs. not
odds.hist.unemployed <- exp(reg.l3$coefficients[[1]]+
                             reg.l3$coefficients[[2]]*1+
                             reg.l3$coefficients[[3]]*1)
odds.hist.unemployed

# Odds ratio: increase in the risk of sufffering addiction
# when there is family history of addiction 
# Working vs. unemployed
odds.ratio <- odds.hist.unemployed/odds.hist.working
odds.ratio

# For people with family history of addiction and working:
# Estimated probability for suffering from addiction vs. not
odds.hist.working / (1+odds.hist.working)

# For people with family history of addiction and unemployed:
# Estimated odds for suffering from addiction vs. not
odds.hist.unemployed / (1+odds.hist.unemployed)
@

\newpage
\subsubsection{Predictive (discriminative) capacity}

In the following we will assess how good Model 3 is at classifying people into the addiction vs. no addiction categories. First, we construct a contingency table with the actual addiction or not and the predicted addiction, using a probability of 0.5 as a cut-off point. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Using a probability of 0.5 as a cut-off
predict.addiction <- rep(0,length(addiction))
for (i in 1:length(addiction))
  if (reg.l3$fitted[i] > 0.5) predict.addiction[i] <- 1

# Contingency table
table(predict.addiction,addiction)
@

Second, we quantify the sensitivity and specificy of the model with the two categorical predictors. 
<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Sensitivity to detect positive cases
sensitivity <- as.numeric(table(predict.addiction[addiction==1]==1)["TRUE"])/
  as.numeric(table(addiction==1)["TRUE"])
sensitivity

# Specificity: identify negative cases as such
specificity <- as.numeric(table(predict.addiction[addiction==0]==0)["TRUE"])/
  as.numeric(table(addiction==0)["TRUE"])
specificity
@

Third, we compute the area under the curve (i.e., the probability of superiority) without the nueed of a cut-off point.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Enter the data
addiction.yes <- reg.l3$fitted[addiction==1]
addiction.no <- reg.l3$fitted[addiction==0]
# Call the function for computing the probability of superiority
probsup(addiction.yes,addiction.no)
@

Fourth, we represent the predicted probabilities of addiction, via histograms, separately for each category. A good model will present high probabilities for the individuals that actually suffer from addiction and low probabilities for the individuals that do not suffer from addiction.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=6, fig.width=5>>=
# Plot, expecting higher predicted probabilities for people with addiction
par(mfrow=c(2,1))
hist(reg.l3$fitted[addiction==1],xlim=c(0,1), breaks=seq(0,1,0.1),
     xlab="Predicted probabality", main="People with addiction")
axis(side=1,seq(0,1,0.1))
hist(reg.l3$fitted[addiction==0],xlim=c(0,1), breaks=seq(0,1,0.1),
     xlab="Predicted probabality", main="People without addiction")
axis(side=1,seq(0,1,0.1))
@

Fifth, we compute Tjur's discrimination coefficient, which compares the means of the two sets of predicted values according to whether the individuals actually present addiction or not. Larger differences indicate a better discrimination between the two categories.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
mean(reg.l3$fitted[addiction==1])-mean(reg.l1$fitted[addiction==0])
@

\newpage
\subsection{Multiple logistic regression with a categorical and a quantitative predictor}
\label{sec:log.multiple.catquanti}

In this example, just like in the previous one, there are two predictors. However, here one of them is categorical and the other one quantitative. In what follows we first specify a simple logistic regression model with the quantitative predictor only. Second, we add the categorical predictor and compare the two models in order to explore whether the model with two predictors is statistically signficantly better. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Numerical predictor only
reglog1 <- glm(addiction~income, family=binomial)
summary(reglog1)
# Adding the categorical predictor
reglog2 <- glm(addiction~income+h.alcohol, family=binomial)
summary(reglog2)
# Comparing the two models: statistical significance
anova(reglog1,reglog2,test="Chisq")
@

Third, we compute the relative reduction of unexplained variability, as compared to (a) the model including only the quantitative predictor and (b) the null model.

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
# Comparing the two models: 
# Reduction of residual variability, as compared to previous model 
# (i.e.., improvement in prediction by  adding the family history variable)
1 - (reglog2$deviance/reglog1$deviance)

# Comparison to a null model: Two ways of understanding McFadden's pseudo R-squared
# Proportion of variability that is not residual
(reglog2$null.deviance-reglog2$deviance)/reglog2$null.deviance
# Reduction of residual variability, as compared to null model
reglog0 <- glm(addiction~1, family=binomial(logit))
1 - (reglog2$deviance/reglog0$deviance)
@

Fourth, we represent graphically the probabilities for suffering from addiction for each observed value of the quantitative variable  \enquote{income}, separately for each of the two categories of the categorical variable \enquote{family history of addiction}. From the fitted regression (curved) lines, it can be seen that the estimates of the probability of addiction are greater for lower incomes and also for individuals with family history. In the plot we have also represented the maximal income value for the individuals without family history for which the probability of addiction is at least 0.5 and the maximal income value for the individuals with family history for which the probability of addiction is at least 0.5. These two values could be interpreted as critical for suffering from addiction: if the individual falls between this income, his/her probability of suffering from addiction would be greater than the probability of not suffering from addiction. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
#################
# Representing the results graphically
plot(income,predict(reglog2,type="response"),
     ylab="Estimated robability of addiction",ylim=c(0,1))

# Split quantitative predictor and fitted values 
# according to levels of the categorical predictor
incomes <- split(income,h.alcohol)
predicts <- split(predict(reglog2,type="response"),h.alcohol)
# Keep in mind the order of the values when plotting predicted
lines(sort(incomes[[1]]),predicts[[1]][order(incomes[[1]])],col="red")
lines(sort(incomes[[2]]),predicts[[2]][order(incomes[[2]])],col="blue")
# Draw horizontal line at probability of 0.5
abline(h=.5,lty=3)

# Represent a version of the "median lethal dose" (LD): 
# Minimal predictor values required for reaching a probability of addiction of 0.5
# Identify the income value for which people without family history 
# reach a probability of 0.5 for suffering from addition
ld1 <- (-(reglog2$coef[1]/reglog2$coef[2]))
# Identify the income value for which people with family history 
# reach a probability of 0.5 for suffering from addition
ld2 <- (-((reglog2$coef[1]+reglog2$coef[3])/reglog2$coef[2]))
# Plot these two income values
abline(v=ld1,lty=3)
abline(v=ld2,lty=3)
# Pring the corresponding text
paste("No history: 0.5 probability of addiction for income =", format(ld1,dig=2)) 
paste("If history: 0.5 probability of addiction for income =", format(ld2,dig=2))

# Plot the data: addiction vs. no addiction
points(income,addiction,pch=16)
text(mean(income),mean(predicts[[2]])+0.1,"If history",col="blue")
text(mean(income),mean(predicts[[1]])+0.1,"No history",col="red")
title(main="Prob addiction according to income & family history")
@

\newpage
\subsection{Multiple logistic regression, considering one of the variables as confounding factor}
\label{sec:log.confounding}

One way of conceptualizing the analysis between two variables, which are of main interest, and the influence of a third variable. In the current section we treat this third variable as a potential moderator of the effect of the main predictor on the dependent variable. Another way of looking at this relation between the three variables is that one of them is a confounding variable.  First, we focus on the two main variables (alcohol addiction as dependent variable and the presence vs. absence of other addictions as a predictor), computing the odds for addiction and the odds ratio. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print the contingency table for two main variables: 
# alcohol addiction as main DV and other addictions as main IV
table(addiction,other.addictions)

# For people with other addictions: Odds for alcohol addition vs. no addiction
odds.other <- as.numeric(table(addiction[other.addictions=="yes"]==1)["TRUE"]) /
  as.numeric(table(addiction[other.addictions=="yes"]==0)["TRUE"])
odds.other

# For people without other addictions: Odds for alcohol addition vs. no addiction
odds.no.other <- as.numeric(table(addiction[other.addictions=="no"]==1)["TRUE"]) /
  as.numeric(table(addiction[other.addictions=="no"]==0)["TRUE"])
odds.no.other

# Odds ratio: increase in risk of alcohol addiction 
# when having other addictions vs. not
odds.other/odds.no.other
@

Second, we explore how the previously obtained odds and odds ratios are (or are not) modified when considering a third variable: the family fistory of alcohol addiction. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
# Print the contingency table for three variables
# alcohol addiction as main DV and other addictions as main IV
# h.alcohol as potential confounder
table(addiction,other.addictions,h.alcohol)

# For people with family history
# For people with other addictions: Odds for alcohol addition vs. no addiction
odds.nohist.other <- 
  as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="yes"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="yes"]==0)["TRUE"])
odds.nohist.other

# For people with family history
# For people without other addictions: Odds for alcohol addition vs. no addiction
odds.nohist.no.other <- 
  as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="yes"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="yes"]==0)["TRUE"])
odds.nohist.no.other

# For people with no family history
# Odds ratio: increase in risk of alcohol addiction when having other addictions vs. not
odds.nohist.other/odds.nohist.no.other

# For people with no family history
# For people with other addictions: Odds for alcohol addition vs. no addiction
odds.nohist.other <- 
  as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="no"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="no"]==0)["TRUE"])
odds.nohist.other

# For people with no family history
# For people without other addictions: Odds for alcohol addition vs. no addiction
odds.nohist.no.other <- 
  as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="no"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="no"]==0)["TRUE"])
odds.nohist.no.other

# For people with no family history
# Odds ratio: increase in risk of alcohol addiction when having other addictions vs. not
odds.nohist.other/odds.nohist.no.other
@

Third, we compute Mantel Haenszel's odds ratio, which can be used to indicate how the odds ratio for addiction when comparing people with or without other addictions changes (or does not change) taking into account the third variable. It is possible to obtain the chi-square ($\chi^2$) value, with one degree of freedom, associated with Mantel Haenszel's odds ratio and the \emph{p} value.  

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Mantel Haenszel's Odds ratio
numerator <- table(addiction,other.addictions,h.alcohol)[1,1,1]*
               table(addiction,other.addictions,h.alcohol)[2,2,1]/
                  sum(table(addiction,other.addictions,h.alcohol)[,,1]) + 
             table(addiction,other.addictions,h.alcohol)[1,1,2]*
                table(addiction,other.addictions,h.alcohol)[2,2,2]/
                  sum(table(addiction,other.addictions,h.alcohol)[,,2])


denominator <- table(addiction,other.addictions,h.alcohol)[1,2,1]*
                 table(addiction,other.addictions,h.alcohol)[2,1,1]/
                   sum(table(addiction,other.addictions,h.alcohol)[,,1]) + 
               table(addiction,other.addictions,h.alcohol)[1,2,2]*
                 table(addiction,other.addictions,h.alcohol)[2,1,2]/
                  sum(table(addiction,other.addictions,h.alcohol)[,,2])

MH.OR <- numerator/denominator
MH.OR
@

The conclusion is that the family history of alcohol addiction is not a confounding factor, as we obtain exactly the same 
odds ratio considering it vs. not: we just get more detailed information with it (history vs. no history).\\[2ex]

In what follows we illustrate a different way of organizing the data and we obtain again the odds and odds ratio. The reason for this illustration is that in this case the effect of both predictors is identical, which is illustrated with the results of the multiple logistic regression analysis.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print the contingency table for the three variables, organized differently
table(addiction,h.alcohol,other.addictions)

# For people with other addictions
# For people with family history: Odds for alcohol addition vs. no addiction
odds.hist.other <- 
  as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="yes"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="yes"]==0)["TRUE"])
odds.hist.other

# For people with other addictions
# For people with no family history: Odds for alcohol addition vs. no addiction
odds.nohist.other <- 
  as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="no"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="yes" & h.alcohol=="no"]==0)["TRUE"])
odds.nohist.other

# For people with other addictions
# Odds ratio: increase in risk of alcohol addiction when having family history vs. not
odds.hist.other/odds.nohist.other

# For people with no other addictions
# For people with with family history: Odds for alcohol addition vs. no addiction
odds.hist.no.other <- 
  as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="yes"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="yes"]==0)["TRUE"])
odds.hist.no.other

# For people with no other addictions
# For people with no family history:: Odds for alcohol addition vs. no addiction
odds.nohist.no.other <- 
  as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="no"]==1)["TRUE"])/
    as.numeric(table(addiction[other.addictions=="no" & h.alcohol=="no"]==0)["TRUE"])
odds.nohist.no.other

# For people with no other addictions
# Odds ratio: increase in risk of alcohol addiction when having family history vs. not
odds.hist.no.other/odds.nohist.no.other

# For this specific example, the same odds ratios were obtained

#########################
# Results showing that the equality of odds ratio values is made evident in the same
# estimate for the corresponding regression coefficient with the same p value. 
reg.lconf <- glm(addiction ~ h.alcohol + other.addictions, family=binomial)
summary(reg.lconf)
@

\newpage
\subsection{Logistic regression for binary vs. proportion} 
\label{sec:log.proportion}

Logistic regression can be applied to both binary outcomes and to dependent variables that are expressed as a proportion. In the current section, we illustrate how the analysis can be carried out when the outcome is a proportion (keep in mind the way in which the dependent variable is specified) and we compare graphically the results of the two kinds of analysis.


<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=5, fig.width=6>>=
par(mfrow=c(1,2))

# Plot the data
plot(income,addiction,ylab="Estimated probability of addiction",xlab="Income")
# Plot the fitted regression line: logistic regression for a binary outcome
lines(sort(income),fitted(reg.l2)[order(income)])
title(main="Logistic reg [for binary]")

# Proportion data: compute the number of days per week with no visits to the bar
no.bar.per.week <- 7-bar.per.week
# Keep in the same object the number of visits and number of no visits with a max of 7
prop.bar.per.week <- cbind(bar.per.week,no.bar.per.week)
# Specify the model including both variables
reg.l4 <- glm(prop.bar.per.week~income,family=binomial(logit))
# Obtain the main results
summary(reg.l4)

# Plot the data
plot(income,bar.per.week/7,ylab="Estimated probability of going to the bar",xlab="Income")
# Plot the fitted regression line
lines(sort(income),fitted(reg.l4)[order(income)])
title(main="Logistic reg [for proportion]")
@

\newpage
\subsection{Poisson regression for counts}
\label{sec:Poisson}

In the current section we deal with another type of outcome which is not expected to be normally distributed. Previously, we illustrated the use of the general linear model with binary outcomes (sections \ref{sec:log.simple.cat}, \ref{sec:log.simple.quanti}, \ref{sec:log.multiple.cat}, and \ref{sec:log.multiple.catquanti}), that is, a nominal scale variable, and proportions (section \ref{sec:log.proportion}), which represent an outcome bounded between 0 and 1. Counts do not present strictly speaking an upper bound, but they form a discrete variable that cannot take negative values. Moreover, heteroskedasticity is expected, given that the variance is usually larger for larger mean values. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Make the code unversally applicable
X <- income
Y <- bar.per.week

# Specify the regression model
poireg <- glm(Y~X,family=poisson)
# Obtain the main results
summary(poireg)

# Plot the data
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
# Plot the fitted regression line
lines(sort(X),fitted(poireg)[order(X)])
title(main="Poisson reg [for frequencies]")
@


\newpage
\subsection{Polynomial regression as part of the GLM}
\label{sec:glm.polynomial}

Using the {\tt glm()} function in \textbf{R}, which we also used for logistic regression with binary outcome (sections \ref{sec:log.simple.cat}, \ref{sec:log.simple.quanti}, \ref{sec:log.multiple.cat}, and \ref{sec:log.multiple.catquanti}) and proportions (section \ref{sec:log.proportion}) and for Poisson regression with count data (section \ref{sec:Poisson}), it is possible to specify a polynomial regression model. In this case, it is not necessary to created other versions of the main predictor - elevated to the second, third, etc., power - as that is done automatically by the software.\\[2ex]

First, we use \enquote{age} as a predictor of \enquote{prospective memory failure}, fitting a quadratic model and then a cubic model. We compare the two models using the {\tt anova()} function in order to know whether modelling the relation as cubic (a line with two bends) is statistically significantly better than modelling it as quadratic (a line with a single bend). Technically, we are assessing whether the increase in R-squared is statistically significant. When the {\tt glm()} function is used the R-squared value is obtained from the deviances, as whowhn in the code, as it cannot be extracted directly using code like {\tt summary(glm.model)\$r.squared}. Note that the results are identical to the ones presented in section \ref{sec:polynomial}.     

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Make the code unversally applicable
X <- age
Y <- PM_failure

# Age as predictor: quadratic model
polyreg2 <- glm(Y~poly(X,2))
summary(polyreg2)
# Age as predictor: cubic model
polyreg3 <- glm(Y~poly(X,3))
summary(polyreg3)

# Change in R-squared
polyreg3.rsquared <- 1 - (summary(polyreg3)$deviance/summary(polyreg3)$null.deviance)
polyreg2.rsquared <- 1 - (summary(polyreg2)$deviance/summary(polyreg2)$null.deviance)
polyreg3.rsquared - polyreg2.rsquared

# Compare models for age as predictor
anova(polyreg2,polyreg3)
@

We now plot the data with the quadratic and cubic regression lines in separate graphs and in the same graph. Whenever we are comparing polynomials with different orders, the decisions regarding which is the best way to model the data should not be based exclusively on statistical criteria, but also on what kind of a relation is expected theoeretically and is conceptually meaningul.

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=3.5, fig.width=6>>=
# Define how the plots will be represented
par(mfrow=c(1,3))

# Plot quadratic model
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg2)[order(X)])
title(main="Polynomial quadratic regression",cex.main=0.8)
# Plot cubic model
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg3)[order(X)])
title(main="Polynomial cubic regression",cex.main=0.8)
# Plot the fit of both models jointly
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg2)[order(X)],col="blue")
lines(sort(X),fitted(polyreg3)[order(X)],col="red")
title(main="Comparing models")
@

We repeat the previously explained analyses and with \enquote{IQ} as predictor instead of \enquote{age}. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
X <- IQ
Y <- PM_failure

# IQ as predictor: Quadratic model
polyreg2 <- glm(Y~poly(X,2))
summary(polyreg2)

# IQ as predictor: Cubic model
polyreg3 <- glm(Y~poly(X,3))
summary(polyreg3)

# Change in R-squared
polyreg3.rsquared <- 1 - (summary(polyreg3)$deviance/summary(polyreg3)$null.deviance)
polyreg2.rsquared <- 1 - (summary(polyreg2)$deviance/summary(polyreg2)$null.deviance)
polyreg3.rsquared - polyreg2.rsquared

# Compare models for IQ as predictor
anova(polyreg2,polyreg3)
@

We repeat the previously explained graphical representations with \enquote{IQ} as predictor instead of \enquote{age}. 

<<echo=TRUE, eval=TRUE,warning=FALSE, fig.height=3.5, fig.width=6>>=
# Define how the plots will be represented
par(mfrow=c(1,3))

# Plot quadratic model
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg2)[order(X)])
title(main="Polynomial quadratic regression",cex.main=0.8)
# Plot cubic model
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg3)[order(X)])
title(main="Polynomial cubic regression",cex.main=0.8)
# Plot the fit of both models jointly
plot(X,Y,ylab="Dependent variable",xlab="Predictor")
lines(sort(X),fitted(polyreg2)[order(X)],col="blue")
lines(sort(X),fitted(polyreg3)[order(X)],col="red")
title(main="Comparing models")
@


\newpage
\section{Nonlinear regression}
\label{sec:nonlinear}

Whenever there is an interest in describing the relation between two variables by means of a regression model, it is advisable to first inspect the data visually. The data represented in a scatterplot can give an initial indication about whether a linear model is justified and how well is such a model expected to fit the data, according to the variability of the Y variable for each value of the X variable. When the visual inspections suggests that the relation between the variables is not linear, there are several possible ways in which the data analysts can proceed. In the following subsections we review some of these possibilities, starting from the simplest one and ending with the most complex one. 

\subsection{Modelling through polynomial regression}
\label{sec:nonlinear.poly}

One of the options to model a nonlinear relation is polynomial regression. It was already discussed in sections \ref{sec:polynomial} and \ref{sec:glm.polynomial} of the present document. We will here apply it to the data presented in section \ref{data.nonlinear}. We will fit a quadratic model to represent the main curve in the relation between the variavbles and a cubic model to try to achieve a better fit, taking into account the fact that there is actually more than one curve in the relation. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Load the data
attach(Datos.Nonlinear)
# Make the code universally applicable
X <- years_start
Y <- population

# Polynomial regression: quadratic model
X2 <- X^2
reg.non1 <- lm(Y ~ X + X2)
summary(reg.non1)

# Polynomial regression: cubic model
X3 <- X^3
reg.non2 <- lm(Y ~ X + X2 + X3)
summary(reg.non2)

# Compare the quadratic and the cubic models
anova(reg.non1,reg.non2)
@

It can be seen that the R-squared values are quite high and that the cubic model is statistically significantly better than the quasdratic model. With the following code we represent visually the fit of the two models. We also explore visually the residuals, identifying a clear patterns, which suggests that both models are misspecified. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Define how the plots will be represented
par(mfrow=c(2,2))

# Plot the quadratic model
plot(X,Y)
lines(sort(X),reg.non1$fitted[order(X)],col="red")
title(main="Fit of the quadratic model")
title(sub=list(paste("R-squared=", round(summary(reg.non1)$r.squared,2)),col="green"))
# Plot the quadratic model residuals
plot(reg.non1$fitted, reg.non1$residuals,xlab="Fitted",ylab="Residual")
lines(sort(reg.non1$fitted),
      loess(reg.non1$residuals~reg.non1$fitted, 
            span=0.8, degree=1)$fitted[order(reg.non1$fitted)],col="red")
title(main="Residuals for the quadratic model")

# Plot the cubic model
plot(X,Y)
lines(sort(X),reg.non2$fitted[order(X)],col="red")
title(main="Fit of the cubic model")
title(sub=list(paste("R-squared=", round(summary(reg.non2)$r.squared,2)),col="green"))

# Plot the cubic model residuals
plot(reg.non2$fitted, reg.non2$residuals,xlab="Fitted",ylab="Residual")
lines(sort(reg.non2$fitted),
      loess(reg.non2$residuals~reg.non2$fitted, 
            span=0.8, degree=1)$fitted[order(reg.non2$fitted)],col="red")
title(main="Residuals for the cubic model")
@

\newpage
Although the residuals already failed one of the requirements for considering the model valid, we can also check homoskedasticity and normality. It can be seen that all tests yield relatively low \emph{p} values, one of which is $\leq 0.05$ indicating that the assumption of homogeneity of variance is not met for the residuals of the quadratic model. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Check the homogeneity of the residuals
# Load the necessary package everytime the test is needed
require(lmtest)
bptest(reg.non1)
bptest(reg.non2)
# Check the normality of the residuals
shapiro.test(reg.non1$residuals)
shapiro.test(reg.non2$residuals)
@

\newpage
\subsection{Linearizable nonlinear regression}
\label{sec:nonlinear.linearize}

Given that the use of polynomial regression (section \ref{sec:nonlinear.poly}) was not appropriate for these data, another option is to try to transform the data in such a way that there relations becomes linear and ordinary least squares regression can still be used. The way in which the data are to be transformed can be guided by conceptual or mathematical knowledge, but it can also be aided by the Tukey-Mosteller bulging rule. In the following we adapted the \texbf{R} code from
\newline
\url{http://www.r-bloggers.com/tukey-and-mostellers-bulging-rule-and-ladder-of-powers/} in order to illustrate transformations that this rule suggests.


<<echo=FALSE, eval=TRUE,warning=FALSE>>=
fakedataMT=function(p=1,q=1,n=99,s=.1){
set.seed(1)
 X=seq(1/(n+1),1-1/(n+1),length=n)
 Y=(5+2*X^p+rnorm(n,sd=s))^(1/q)
 return(data.frame(x=X,y=Y))}
par(mfrow=c(2,2))
plot(fakedataMT(p=.5,q=2),xaxt="n", yaxt="n")
title(main="log(X) or sqrt(X) and/or Y^2 or Y^3", cex.main=0.8)
arrows(mean(fakedataMT(p=.5,q=2)$x),mean(fakedataMT(p=.5,q=2)$y),min(fakedataMT(p=.5,q=2)$x),max(fakedataMT(p=.5,q=2)$y),col="red")

plot(fakedataMT(p=3,q=-5),xaxt="n", yaxt="n")
title(main="X^2 or X^3 and/or Y^2 or Y^3", cex.main=0.8)
arrows(mean(fakedataMT(p=3,q=-5)$x),mean(fakedataMT(p=3,q=-5)$y),max(fakedataMT(p=3,q=-5)$x),max(fakedataMT(p=3,q=-5)$y),col="red")

plot(fakedataMT(p=.5,q=-1),xaxt="n", yaxt="n")
title(main="log(X) or sqrt(X) and/or log(Y) or sqrt(Y)", cex.main=0.8)
arrows(mean(fakedataMT(p=.5,q=-1)$x),mean(fakedataMT(p=.5,q=-1)$y),min(fakedataMT(p=.5,q=-1)$x),min(fakedataMT(p=.5,q=-1)$y),col="red")

plot(fakedataMT(p=3,q=5),xaxt="n", yaxt="n")
title(main="X^2 or X^3 and/or log(Y) or sqrt(Y)", cex.main=0.8)
arrows(mean(fakedataMT(p=3,q=5)$x),mean(fakedataMT(p=3,q=5)$y),max(fakedataMT(p=3,q=5)$x),min(fakedataMT(p=3,q=5)$y),col="red")
@

According to the bulging rule, the data resemble to a greater extent the pattern in the upper left quadrant. For that purpose we will perform the transformations suggested, to see which one fits best. Note that we will use the variable called \enquote{years} (since the first local inhabit started living on the island) instead of the variable called \enquote{years\_start} (since the first foreign expedition discovered the island), given that the latter contains negative numbers, which cannot be handled by a logarithm transformation.

\newpage
<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Transformation 1: using the logarithm of X
# Transform
log_X <- log(years)
# Carry out the regression analysis
reg.tm1 <- lm(Y ~ log_X)
# Represent visually the fit a of simple linear regression model to the transformed data
par(mfrow=c(2,2))
plot(log_X,Y)
lines(sort(log_X),reg.tm1$fitted[order(log_X)],col="red")
title(main="Transformation: log(X)")
title(sub=list(paste("R-squared=", round(summary(reg.tm1)$r.squared,2)),col="green"))

##############
# Transformation 2: taking the square root of X
# Transform
sqrt_X <- sqrt(years)
# Carry out the regression analysis
reg.tm2 <- lm(Y ~ sqrt_X)
# Represent visually the fit a of simple linear regression model to the transformed data
plot(sqrt_X,Y)
lines(sort(sqrt_X),reg.tm2$fitted[order(sqrt_X)],col="red")
title(main="Transformation: sqrt(X)")
title(sub=list(paste("R-squared=", round(summary(reg.tm2)$r.squared,2)),col="green"))

##############
# Transformation 3: elevating Y to the 3rd power
# Transform
Y3 <- population^3
X <- years
# Carry out the regression analysis
reg.tm3 <- lm(Y3 ~ X)
# Represent visually the fit a of simple linear regression model to the transformed data
plot(X,Y3)
lines(sort(X),reg.tm3$fitted[order(X)],col="red")
title(main="Transformation: Y^3")
title(sub=list(paste("R-squared=", round(summary(reg.tm3)$r.squared,2)),col="green"))

##############
# Transformation 4: taking the square root of X and elevating Y to the 3rd power
# Transform
Y3 <- population^3
sqrt_X <- sqrt(years)
# Carry out the regression analysis
reg.tm4 <- lm(Y3 ~ sqrt_X)
# Represent visually the fit a of simple linear regression model to the transformed data
plot(sqrt_X,Y3)
lines(sort(sqrt_X),reg.tm3$fitted[order(sqrt_X)],col="red")
title(main="Transformation: sqrt(X) and Y^3")
title(sub=list(paste("R-squared=", round(summary(reg.tm4)$r.squared,2)),col="green"))
@

\vspace{1cm}

With these data we illustrate that transforming the data is not always sufficient for making the relation between the variables linear and for using simple linear regression. Actually, the following conditions need to be met:
\begin{itemize}
\item All the values need to be positive to apply power transformations. \item The ratio largest/smallest value of the variable should be large
\item The relation should be monotonic and simple (1 curve) for linearizing to be possible
\end{itemize}
In that sense, not all nonlinear relations can be modelled with a linear model: not all are linearizable - some are intrisically nonlinear.

\newpage
\subsection{Intrinsically nonlinear regression}
\label{sec:nonlinear.intrinsic}

In the current section we will deal with a single type of nonlinear model, although there are multiple possible nonlinear models. Whether a nonlinear model is useful depends on the situation (variables studies) and whether it could be considered a good approximation of reality not only numerically, but also due to conceptual reasons. In all cases in which an intrisically nonlinear model is used, knowledge is required to guide the choice of the model and to specify the possible (initial) values of the parameters. In that sense, nonlinear regression cannot be purely data-driven. \\[2ex]

In the this example, we specify a nonlinear model on the basis of the knowledge that a Gompertz model (\url{https://en.wikipedia.org/wiki/Gompertz_function}) can be used to model the growth in a confined space (e.g., inhabitants in an island or cancer cells in the human body). To apply nonlinear regression it is necessary to have such theoretical and/or mathematical knowledge about a reasonable model and to be able to specify it with a formula. The Gompertz function is specified, in the curent example as: $Y(t)=a \times e^{b \times e^{c \times t}}$. The terms correspond to:
\begin{itemize}
\item \emph{Y} is the depedent variable, \enquote{population} in the current example
\item \emph{t} is the independent variable, \enquote{years\_start} in the current example
\item \emph{a} is the upper asymptote, that is, the level reached and not exceeded by Y (i.e., the level where population stabilizes). This value can be identified inspecting visually the scatterplot or identifying the maximal value of in the \enquote{population} variable
\item \emph{b} is a negative number indicating the desplacement in the abscissa: the smaller the number (i.e., higher negative), the later that the main growth occurs with respect to $t=0$. The choice of a seed can be guided by the indications provided in the page referenced about the shape of the data for different values of \emph{b}.
\item \emph{c} is a negative number indicating the growth rate: the smaller the number (i.e., higher negative), the steeper the slope. The choice of a seed can be guided by the indications provided in the page referenced about the shape of the data for different values of \emph{c}.
\end{itemize}

We here include the indications from \url{https://en.wikipedia.org/wiki/Gompertz_function} and the images created by a user called {\tt Georg-Johann}.

\begin{figure}[H] 
\includegraphics[keepaspectratio=TRUE,scale=0.63]{Gompertz}
\centering
\end{figure}

\newpage
\subsubsection{An illustration of an iterative methods for estimating parameters}
Imagine that we choose as initial values, the values $a=1000$, $b=-1$ and $c=-0.45$. In the following we illustrate the fact that nonlinear regression proceeds by checking these seeds and other similar (or not so similar) values that are both above and below the seeds. Note that we here predefine several possible parameter estimates to check the fit of the corresponding model to the data, but this is not exactly the way in which nonlinear regression takes place - see the text about the similarities and differences right after this illustration. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Make the code universally applicable
X <- years_start
Y <- population

# Function that will be called iteratively: for each set of parameter estimates
# Requires: the values of the parameters and the vectors containing the variables
# Computes the sum of squared errors and R-squared 
# Yields: the values of the parameters, sum of squared residuals, R-squared
as.if.nonlinear=function(a,b,c,X,Y){
fitted <- a*exp(b*exp(c*X))
totalSS <- sum((Y-mean(Y))^2)
residualSS <- sum((fitted-Y)^2)
R_sq <- 1 - residualSS/totalSS
tableline <- c(a,b,c,residualSS,R_sq)
return(tableline)}

# Predefine the parameter estimates that will be checked
# "a" term: upper asymptote: 990, 1000 (would have been the seed), 1010
avalues <- seq(from=990,to=1010,by=10)  
# "b" term: displacement in the abscissa: -2, -1.5, -1 (would have been the seed), -0.5
bvalues <- seq(from=-2,to=-0.5,by=0.5)  
# "c" term: growth rate: -1.25, -0.85, -0.45 (would have been the seed), -0.05
cvalues <- seq(from=-1.25,to=-0.05,by=0.4) 
# Number of iterations (combinations of estimates)
# For illustration purposes: there's no predefined number in nonlinear regression
iter <- length(avalues)*length(bvalues)*length(cvalues)

# Create a table to store the information about the iterations
tableiter <- rep(0,(6*iter))
dim(tableiter) <- c(iter,6)
tableiter[,1] <- 1:iter
colnames(tableiter) <- c("Iteration", "a", "b", "c", "Sum of squared errors", "R_squared")

# Start the iterations
count_iter <- 0
for (i in 1:length(avalues))
  for (j in 1:length(bvalues))
    for (k in 1:length(cvalues))
    {
      # Count the iterations perfomed
      count_iter <- count_iter + 1
      # Call the function and store the information about estimates and fit in the table
      tableiter[count_iter,2:6] <- as.if.nonlinear(a=avalues[i],
                                                   b=bvalues[j],
                                                   c=cvalues[k],
                                                   X=X,
                                                   Y=Y)
    }
@

\newpage

In this page we provide the summary table with the main information for the different iterations: the sum of squared residuals and the R-squared values for each combination of the parameter estimates for which the Gompertz model was fitted. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Print the table with the information of the iterations
tableiter <- as.data.frame(tableiter)
tableiter
@

\newpage
The following code is used for creating the graphical representation of the fit of the model to the data for two possible sets of parameter estimates. We focus on the best cand the worst ombinations of parameter estimates, out of the values included, defining \enquote{best} as the one with greatest R-squared and \enquote{worst} as the one with lowest R-squared.  

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=7, fig.width=6>>=
# Set the space for the graphical representations
par(mfrow=c(2,1))

# Identify parameter estimates for which the best fit was found
a.best <- tableiter$a[tableiter$R_squared==max(tableiter$R_squared)]
b.best <- tableiter$b[tableiter$R_squared==max(tableiter$R_squared)]
c.best <- tableiter$c[tableiter$R_squared==max(tableiter$R_squared)]
# (NOTE: It is neither an absolute minimum sum of squares, 
# as in ordinary least squares regression, nor a relative minimum,
# as in nonlinear regression, given that we have only illustrated how it works)

# Graphical representation of the best estimates among the ones checked
plot(X, Y, main=paste("Best fit for estimates: a=",
                      a.best,"; b=", b.best, "; c=", c.best, sep=""))
# Obtain the fitted values and represent the regression line
fitted <- a.best*exp(b.best*exp(c.best*years_start))
lines(years_start,fitted,col="red")
title(sub=list(paste("R-squared=", round(max(tableiter$R_squared),2)),col="green"))

# Identify parameter estimates for which the worst fit was found
a.worst <- tableiter$a[tableiter$R_squared==min(tableiter$R_squared)]
b.worst <- tableiter$b[tableiter$R_squared==min(tableiter$R_squared)]
c.worst <- tableiter$c[tableiter$R_squared==min(tableiter$R_squared)]

# Graphical representation of the worst estimates among the ones checked
plot(X, Y, main=paste("Worst fit for estimates: a=",
                      a.worst,"; b=", b.worst, "; c=", c.worst, sep=""))
# Obtain the fitted values and represent the regression line
fitted <- a.worst*exp(b.worst*exp(c.worst*years_start))
lines(years_start,fitted,col="red")
title(sub=list(paste("R-squared=", round(min(tableiter$R_squared),2)),col="green"))
@

\newpage
With the following code we comparing the pattern and size of the residuals across several models, namely, the cubic model from polynomial regression (upper left panel), the simple linear regression using the square root of X as a main predictor (upper right panel), and the nonlinear model with the best combination of parameter estimates out of the 48 combinations attempted (bottom left panel). We also compare the size of these residuals with the boxplots represented in the bottom right panel. 

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=7, fig.width=6>>=
# Set the space for the graphical representations
par(mfrow=c(2,2))

# Represent the residuals for the cubic polynomial model
plot(reg.non2$fitted, reg.non2$residuals,xlab="Fitted",ylab="Residual")
lines(sort(reg.non2$fitted),
      loess(reg.non2$residuals~reg.non2$fitted, 
            span=0.8, degree=1)$fitted[order(reg.non2$fitted)],col="red")
title(main="Residuals for the cubic model", cex.main=0.8)

# Represent the residuals for the simple linear regression model with log(X)
plot(reg.tm2$fitted, reg.tm2$residuals,xlab="Fitted",ylab="Residual")
lines(sort(reg.tm2$fitted),
      loess(reg.tm2$residuals~reg.tm2$fitted, 
            span=0.8, degree=1)$fitted[order(reg.tm2$fitted)],col="red")
title(main="Residuals for model with sqrt(X)", cex.main=0.8)

# Represent the residuals for the nonlinear model with the best predictors
nonlin.residuals <- a.best*exp(b.best*exp(c.best*X))-Y
nonlin.fitted <- a.best*exp(b.best*exp(c.best*X))
plot(nonlin.fitted, nonlin.residuals,xlab="Fitted",ylab="Residual")
lines(sort(nonlin.fitted),
      loess(nonlin.residuals~nonlin.fitted, 
            span=0.8, degree=1)$fitted[order(nonlin.fitted)],col="red")
title(main="Residuals for nonlinear model", cex.main=0.8)

# Represent all three sets of residuals together in boxplots
boxplot(reg.non2$residuals,reg.tm2$residuals, nonlin.residuals,
        names=c("Cubic", "Sqrt(X)", "Nonlinear"),
        main="Comparing residuals across models", cex.main=0.8)
@

It can be seen that the only residual not showing a clear pattern is the one for the nonlinear regression model. Moreover, it is also the residual smallest in size. This information, taken together with the R-squared value presented above suggests that this model is the appropriate one for representing the data. Moreover, it is conceptually meaningful for the situation it describes. 

\newpage
\subsubsection{Beyond the illustration: How nonlinear regression actually works}

\textbf{Similarities} between the approach followed here for illustrating the iterative check of parameter estimates and the way in which nonlinear regression actually takes place:
\begin{itemize}
\item The aim is to identify the parameter estimates that lead to relative minimal sum of squared errors (i.e., larger R-squared values)
\item Both procedures are iterative: several steps are necessary in order to identify parameter estimates that lead to relative minimal sum of squared errors
\item Initial values (seeds) need to be specified for the parameter estimates in order to start the iterative procedure of looking for better estimates leading to smaller error
\end{itemize}

\vspace{1cm}

\textbf{Diferences} between the approach followed here for illustrating the iterative check of parameter estimates and the way in which nonlinear regression actually takes place:
\begin{itemize}
\item The algorithms for nonlinear regression do not have a pre-specified number of interations, given that the number of interations actually depends on reaching a pre-specified minimal meaningful reduction of the sum of squared errors.
\item The last iteration in nonlinear regression is the one for which the relative minimal sum of squared errors is achieved. Therefore, the relative best parameter estimates are the ones contained in the last iteration, unlike the table presented previously.
\item The iterations do not necessarily follow a systematic variation of the parameters (i.e., always increasing their values), given that the increase or decrease of the values depends on whether the error increases or decreases (that is, whether it is reasonable to continiue in the same direction, say, increasing, or it is necessary to change the direction in which the parameter estimates are varied, say, decreasing)
\item Not all parameter estimates have to be varied always in the same direction, some migh increase and other decrease according to the sum of squared errors.
\item Smaller variations (than the ones presented here) in the parameters estimates are lilely when fine-tuning.
\end{itemize}

Intrinsically nonlinear regression can be performed in \textbf{R} using the {\tt nls()} function illustrated in the following section, but here we preferred to illustrate (in a simplified way and considering the Similarities \& Differences made explicit above) how underlying process of nonlinear regression can be understood.  

\newpage
\subsubsection{Carrying out intrinsically nonlinear regression in R}

In present subsection, we use the \textbf{R} function called {\tt nls()} which implements intrinsically nonlinear regression, the characteristics of which were summarised in the previous page. In order to perform a nonlinear regression it is necessary, both in \textbf{R} and in \textbf{IBM SPSS}\textsuperscript{\textregistered}, to provide the initial values and to specify the mathematical expression of the model that is expected to represent well the relation between the variables. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Specify the initial values
a.seed <- 1000 
b.seed <- -2
c.seed <- -0.1
# Created a named vector with the initial values
seeds <- c(a.seed,b.seed,c.seed)
names(seeds) = c("a", "b", "c")
# Create an object of the class "formula" representing the Gompertz model
fo <- Y ~ a*exp(b*exp(c*X))
# Carry out the nonlinear regression analysis
nonlin <- nls(formula=fo,start=seeds)
summary(nonlin)
@

Note that, the number of iterations required wkith these initial values for the parameters was only 5. As shown in the following table, the results obtained are very similar to the best estimates identified previously, with the procedure that only illustrates in a simplified way how nonlinear regression works. However, this is so only because the values pre-determined in the illustration were reasonable. 

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Create the comparison table
table.compare <- rep(0,(length(seeds)*2))
dim(table.compare) <- c(length(seeds),2)
# Specify the names of rows and columns
rownames(table.compare) <- names(seeds)
colnames(table.compare) <- c("Illustration", "Actual")
# Deactivates scientific notation
options(scipen=999)
# Enter the values from the actual nonlinear regression
table.compare[,2] <- round(coefficients(nonlin),3)
# Enter the values from the illustration
table.compare[1,1] <- a.best
table.compare[2,1] <- b.best
table.compare[3,1] <- c.best
table.compare
@

In order to obtain the R-squared value of the model, which is not part of the output of the {\tt summary()} function as applied to an object of class {\tt nls}, we first need to identify the residual sum of squares. Second, the total sum of squares is computed as the sum of the squared deviations of each value of Y (\enquote{population}) from its mean. Third, R-squared (the proportion of variability explained) is obtained as the complementary of the proportion of residual variability.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Get the residual sum of squares from the deviance() function
residualSS <- deviance(nonlin)
# Compute the total sum of squares
totalSS <- sum((Y-mean(Y))^2)
# Compute and print R-squared
r.squared <- 1-(residualSS/totalSS)
r.squared
@

We now represent the residuals graphically in order to inspect whether there is any pattern. We also compare the size of teh residuals as obtained with the {\tt nls()} function to the best estimates identified with the illustrative set of iterations.

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4, fig.width=6>>=
# Set the space for the graphical representations
par(mfrow=c(1,2))

# Represent the fitted values vs. residuals
plot(fitted(nonlin),residuals(nonlin))
lines(sort(fitted(nonlin)),
      loess(residuals(nonlin)~fitted(nonlin), 
            span=0.8, degree=1)$fitted[order(fitted(nonlin))],col="red")

# Represent all three sets of residuals together in boxplots
boxplot(nonlin.residuals, residuals(nonlin),
        names=c("Illustration", "Actual"),
        main="Residuals across nonlinear models", cex.main=0.8)
@

\newpage
With the following graph and statistical test, the normality of the residual can be checked, although it is not necessary, given that ordinary least squares regression was not used to obtain the parameter estimates or their associated \emph{p} values.  

<<echo=TRUE, eval=TRUE,warning=FALSE,fig.height=4, fig.width=4>>=
# Histogram with superimposed normal curve
hist(residuals(nonlin),prob=TRUE,xlab="Residuals")
curve(dnorm(x, mean=mean(residuals(nonlin)),sd=sd(residuals(nonlin))),
            col="darkblue", lwd=2, add=TRUE, yaxt="n")

# Shapiro-Wilk test
shapiro.test(residuals(nonlin))
@

\newpage
Given that with nonlinear regression only a local (or relative) minimal sum of squared errors is achieved, but not an absolute one, the analyst can never be sure that the parameter estimates are the best possible ones. What can increase the confidence of the analyst about the appropriateness of these estimates to repeat the analysis with different seeds. We followed this advice here and obtained identical results, after 10 iterations.

<<echo=TRUE, eval=TRUE,warning=FALSE>>=
# Specify the initial values
a.seed <- max(Y)
b.seed <- -4
c.seed <- -0.01
# Created a named vector with the initial values
seeds <- c(a.seed,b.seed,c.seed)
names(seeds) = c("a", "b", "c")
# Create an object of the class "formula" representing the Gompertz model
class(fo <- Y ~ a*exp(b*exp(c*X)))
# Carry out the nonlinear regression analysis
nls(formula=fo,start=seeds)
@


\newpage
\section{References}

\justify Allison, D. B., \& Gorman, B. S. (1993). Calculating effect sizes for meta-analysis: The case of the single case. \emph{Behaviour Research and Therapy}, \emph{31}, 621-631

\justify Barlow, D. H., Nock, M. K., \& Hersen, M. (2009). \emph{Single case experimental designs: Strategies for studying behavior change} (3rd Ed.). Boston, MA: Pearson.

\justify Baron, R. M., \& Kenny, D. A. (1986). The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. \emph{Journal of Personality and Social Psychology}, \emph{51}, 1173-1182.

\justify Biesanz, J. C., Deeb-Sossa, N., Papadakis, A. A., Bollen, K. A., \& Curran, P. J. (2004). The role of coding time in estimating and interpreting growth curve models. \emph{Psychological Methods}, \emph{9}, 30-52. 

\justify Bliese, P. (2013). \emph{Multilevel modeling in R (2.5): A brief introduction to R, the multilevel package and the nlme package}. Retrieved on June 29, 2015 from \url{http://cran.r-project.org/doc/contrib/Bliese_Multilevel.pdf} 

\justify Center, B. A., Skiba, R. J., \& Casey, A. (1985-1986). A methodology for the quantitative synthesis of intra-subject design research. \emph{The Journal of Special Education}, \emph{19}, 387-400.

\justify Dolezal, D. N., Weber, K. P., Evavold, J. J., Wylie, J., \& McLaughlin, T. F. (2007). The effects of a reinforcement package for on-task and reading behavior with at-risk and middle school students with disabilities. \emph{Child and Family Therapy}, \emph{29}, 9-25.

\justify Efron, B., \& Tibshirani, R. (1997). Improvements on cross-validation: the 632+ bootstrap method. \emph{Journal of the American Statistical Association}, \emph{92}, 548-560.

\justify Ferron, J. M., Bell, B. A., Hess, M. R., Rendina-Gobioff, G., \& Hibbard, S. T. (2009). Making treatment effect inferences from multiple-baseline data: The utility of multilevel modeling approaches. \emph{Behavior Research Methods}, \emph{41}, 372-384.

\justify Finkel, D., Reynolds, C. A., McArdle, J. J., Gatz, M., \& Pedersen, N. L. (2003). Latent growth curve analyses of accelerating decline in cognitive abilities in late adulthood. \emph{Developmental Psychology}, \emph{39}, 535-550.

\justify Gorsuch, R. L. (1983). Three methods for analyzing limited time-series (N of 1) data. \emph{Behavioral Assessment}, \emph{5}, 141-154. 

\justify Grissom, R. J. (1994). Probability of the superior outcome of one treatment over another. \emph{Journal of Applied Psychology}, \emph{79}, 314-316. 

\justify Hedecker, D., & Gibbons, R. D. (2006). \emph{Longitudinal data analysis}. Hoboken, NJ: John Wiley & Sons. 

\justify Heffernan, T. M., O'Neill, T. S., \& Moss, M. (2012). Smoking-related prospective memory deficits in a real-world task. \emph{Drug and Alcohol Dependence}, \emph{120}, 1-6. 

\justify Huitema, B. E., \& McKean, J. W. (2000). Design specification issues in time-series intervention models. \emph{Educational and Psychological Measurement}, \emph{60}, 38-58.  

\justify Hox, J. (2002). emph{Multilevel analysis: Techniques and applications}. London, UK: Lawrence Erlbaum. 

\justify Jacoby, W. G. (2000). Loess: A nonparametric, graphical tool for depicting relationships between variables. \emph{Electoral Studies}, \emph{19}, 577-613.

\justify Kotrlik, J. W., Williams, H. A., \& Jabor, M. K. (2011). Reporting and interpreting effect size in quantitative agricultural education research. \emph{Journal of Agricultural Education}, \emph{52}, 132-142.

\justify MacKinnon, D. P., Lockwood, C. M., Hoffman, J. M., West, S. G., \& Sheets, V. (2002). A comparison of methods to test mediation and other intervening variable effects. \emph{Psychological Methods}, \emph{7}, 83-104. 

\justify McCoach, D. B., O'Connell, A. A., Reis, S. M., \& Levitt, H. A. (2006). Growing readers: A hierarchical linear model of children's reading growth during the first 2 years. \emph{Journal of Educational Psychology}, \emph{98}, 14-28.

\justify Moeyaert, M., Ugille, M., Ferron, J., Beretvas, S. N., \& Van Den Noortgate, W. (2014). The influence of the design matrix on treatment effect estimates in the quantitative analyses of single-case experimental designs research. \emph{Behavior Modification}, \emph{38}, 665-704. 

\justify Newsom, J. T. (2012). Basic longitudinal analysis approaches for continuous and categorical variables. In J. T. Newsom, R. N. Jones, & S. M. Hofer, \emph{Longitudinal data analysis: A practical guide for researchers in aging, health, and social sciences} (pp. 143-180). Hove, East Sussex, UK: Routledge.

\justify Parker, R. I., \& Vannest, K. J. (2009). An improved effect size for single-case research: Nonoverlap of all pairs. \emph{Behavior Therapy}, \emph{40}, 357-367.

\justify Phillips, J., Ajrouch, K., \& Hillcoat-Nallétamby, S. (2010). \emph{Key concepts in social gerontology}. London, UK: Sage.

\justify Rovine, M. J., \& Liu, S. (2012). Latent growth curve models. In J. T. Newsom, R. N. Jones, & S. M. Hofer, \emph{Longitudinal data analysis: A practical guide for researchers in aging, health, and social sciences} (pp. 271-290). Hove, East Sussex, UK: Routledge.

\justify Shaw, B. A., \& Liang, J. (2012). Growth models with multilevel regression. In J. T. Newsom, R. N. Jones, & S. M. Hofer, \emph{Longitudinal data analysis: A practical guide for researchers in aging, health, and social sciences} (pp. 217-242). Hove, East Sussex, UK: Routledge.

\justify Solanas, A., \& Puyuelo, M. (1998). Funciones de desarrollo aplicadas al estudio evolutivo del lenguaje. En J. Renom (Ed.), \emph{Tratamiento informatizado de datos} (pp. 127-158). Barcelona: Masson.

\justify Stickley, A., Koyanagi, A., Roberts, B., Murphy, A., Kizilova, K., \& McKee, M. (2015). Male solitary drinking and hazardous alcohol use in nine countries of the former Soviet Union. \emph{Drug and Alcohol Dependence}, \emph{150}, 105-111.

\justify Swaminathan, H., Rogers, H. J., Horner, R., Sugai, G., \& Smolkowski, K. (2014). Regression models for the analysis of single case designs. \emph{Neuropsychological Rehabilitation}, \emph{24}, 554-571. 

\justify Tryon, W. W. (2001). Evaluating statistical difference, equivalence, and indeterminacy using inferential confidence intervals: An integrated alternative method of conducting null hypothesis statistical tests. \emph{Psychological Methods}, \emph{6}, 371-386.

\justify Van Den Noortgate, W., \& Onghena, P. (2003). Hierarchical linear models for the quantitative integration of effect sizes in single-case research. \emph{Behavior Research Methods, Instruments, & Computers}, \emph{35}, 1-10.

\justify Wilcox, R. (2012). \emph{Introduction to robust estimation and hypothesis testing (3rd ed.)} London, UK: Elsevier.

\justify Zahodne, L. B., Marsiske, M., Okun, M. S., Rodriguez, R. L., Malaty, I., \& Bowers, D. (2012). Mood and motor trajectories in Parkinson's disease: Multivariate latent growth curve modeling. Neuropsychology, \emph{26}, 71-80. 

\newpage
\section{Suggested readings}
\label{sec:read}
\subsection{Readings in English}

\justify Cohen, J. \& Cohen, P. (1975). \emph{Applied multiple regression/correlation analysis for the behavioral sciences}. Hillsdale, NJ: Lawrence Erlbaum.

\justify Chaterjee, S., \& Hadi, A. S. (2006). \emph{Regression analysis by example} (4et ed.) London, UK: John Wiley \& Sons.

\justify Hedecker, D., \& Gibbons, R. D. (2006). \emph{Longitudinal data analysis}. Hoboken, NJ: John Wiley \& Sons.

\justify Hosmer, D. W., Lemeshow, S., \& Sturdivant, R. X. (2013). \emph{Applied logistic regression} (3rd Ed.). Hoboken, NJ: John Wiley & Sons.

\justify Keppel., G. (1991). \emph{Design and analysis: A researcher's handbook} (3rd ed.). Englewood Cliffs Prentice Hall.  

\justify Maas, C. J. M., \& Hox, J. J. (2005). Sufficient simple size for multilevel modeling. \emph{Methodology}, \emph{1}, 86-92.

\justify Nasreddine, Z. S., Phillips, N. A., Bédirian, V., Charbonneau, S., Whitehead, V., Collin, I., et al. (2005). The Montreal Cognitive Assessment, MoCA: A brief screening tool for mild cognitive impairment. \emph{Journal of the American Geriatrics Society}, \emph{53}, 695-699.

\justify Rovine, M. J., \& Liu, S. (2012). Latent growth curve models. In J. T. Newsom, R. N. Jones, \& S. M. Hofer, \emph{Longitudinal data analysis: A practical guide for researchers in aging, health, and social sciences} (pp. 271-290). Hove, East Sussex, UK: Routledge.

\justify Shaw, B. A., \& Liang, J. (2012). Growth models with multilevel regression. In J. T. Newsom, R. N. Jones, \& S. M. Hofer, \emph{Longitudinal data analysis: A practical guide for researchers in aging, health, and social sciences} (pp. 217-242). Hove, East Sussex, UK: Routledge.

\justify Tabachnik, B. G., \& Fidell, L. S. (2013). \emph{Multivariate statistics} (6th Ed.). Madrid: Pearson. 

\justify Wilcox, R. (2012). \emph{Introduction to robust estimation and hypothesis testing (3rd ed.)} London, UK: Elsevier. (Chapter 10).

\justify Wright, D. B., \& London, K. (2009). \emph{Modern regression techniques using R: A practical guide for students and researchers}. London, UK: Sage. 

\vspace{1cm}

\subsection{Readings in Spanish}

\justify Ato, M. y Vallejo, G. (2007). \emph{Diseños experimentales en Psicología}. Madrid: Pirámide. (Cap. 6).

\justify Guàrdia, J., Freixa, M., Peró, M. y Turnaby, J. (2008). \emph{Análisis de datos en psicología} (2a ed.). Madird: Delta. (Cap. 11). 

\justify Peró, M., Leiva, D., Guàrdia, J. y Solanas, A. (Eds.) (2012). \emph{Estadística aplicada a las ciencias sociales mediante R y R-Commander}. Madrid: Garceta. (Cap. 9).

\justify Silva Ayçaguer, L. C., \& Barroso Utra, I. M. (2004). \emph{Regresión logística}. Madrid: La Muralla. 

\justify Solanas, A., Salafranca, Ll., Fauquet, J. y Núñez, M. I. (2005). \emph{Estadística descriptiva en ciencias del comportamiento}. Madrid: Thomson. (Cap. 12).

\end{document}